{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e8719881-b067-4348-b595-a5353298c294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca748c2b-4249-41c3-8efa-a86ca9872139",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ac45631-459d-4fa3-9f8b-211e80ca1226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b32cbd-59d6-4dc6-9f60-b550e4de4eab",
   "metadata": {},
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f7459a0f-4a62-48c6-9539-27276524679d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImpurityDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataframe, fixed_features, labels, feature_scaler=None, label_scaler=None, device=None):\n",
    "        assert len(labels) % 2 == 0\n",
    "        \n",
    "        self.fixed_features = fixed_features\n",
    "        self.labels = labels\n",
    "        self.n_samples = len(dataframe)\n",
    "        \n",
    "        self.output_length = len(labels)\n",
    "        self.input_length = len(fixed_features)\n",
    "        self.sequence_length = 1\n",
    "        \n",
    "        df_features = dataframe[fixed_features]\n",
    "        df_labels = dataframe[labels]\n",
    "\n",
    "        if feature_scaler is not None and label_scaler is not None:\n",
    "            xs = feature_scaler.transform(df_features.values)\n",
    "            ys = label_scaler.transform(df_labels.values)\n",
    "        else:\n",
    "            xs = df_features.values\n",
    "            ys = df_labels.values\n",
    "\n",
    "        feature_data = np.zeros((self.n_samples, self.sequence_length, self.input_length))\n",
    "        label_data = np.zeros((self.n_samples, self.output_length))\n",
    "        \n",
    "        for i in range(self.n_samples):\n",
    "            xi = xs[i]\n",
    "            yi = ys[i]\n",
    "            \n",
    "            feature_data[i, :, :] = xi\n",
    "            label_data[i, :] = yi\n",
    "\n",
    "        self.feature_data = torch.tensor(feature_data, dtype=torch.float).to(device)\n",
    "        self.label_data = torch.tensor(label_data, dtype=torch.float).to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.feature_data[idx], self.label_data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e4745dbe-e866-40ee-8014-320fb6de09fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scalers(dataframe, fixed_features, labels, test_size, random_state=None):\n",
    "    train_df, _ = train_test_split(dataframe, test_size=test_size, random_state=random_state)\n",
    "    df_features = train_df[fixed_features]\n",
    "    df_labels = train_df[labels]\n",
    "    \n",
    "    feature_scaler = StandardScaler()\n",
    "    feature_scaler.fit(df_features.values)\n",
    "    \n",
    "    label_scaler = StandardScaler()\n",
    "    label_scaler.fit(df_labels.values)\n",
    "\n",
    "    return feature_scaler, label_scaler      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d1bb7fbd-7c0a-495d-bc24-772b235a90e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../data/20230825_144318_10k_EVDoubExp-TExp-wmax5-sparse-hyb_with_perturbation.csv'\n",
    "\n",
    "#fixed_features = ['beta', 'U', 'Eimp', 'E1', 'E2', 'E3', 'V1', 'V2', 'V3']\n",
    "\n",
    "# original feature set\n",
    "#fixed_features = ['beta', 'E1', 'E2', 'E3', 'V1', 'V2', 'V3']\n",
    "\n",
    "fixed_features = ['beta', 'E1', 'E2', 'E3', 'V1', 'V2', 'V3', 'ReFso1', 'ImFso1', 'ReFso3', 'ImFso3', 'ReFso5', 'ImFso5', 'ReFso7', 'ImFso7', 'ReFso9', 'ImFso9', 'ReFso11', 'ImFso11', 'ReFso13', 'ImFso13', 'ReFso15', 'ImFso15', 'ReFso17', 'ImFso17', 'ReFso19', 'ImFso19', 'ReFso21', 'ImFso21', 'ReFso23', 'ImFso23', 'ReFso25', 'ImFso25', 'ReFso29', 'ImFso29', 'ReFso33', 'ImFso33', 'ReFso37', 'ImFso37', 'ReFso43', 'ImFso43', 'ReFso49', 'ImFso49', 'ReFso57', 'ImFso57', 'ReFso69', 'ImFso69', 'ReFso83', 'ImFso83', 'ReFso101', 'ImFso101', 'ReFso127', 'ImFso127', 'ReFso165', 'ImFso165', 'ReFso237', 'ImFso237', 'ReFso399', 'ImFso399', 'ReFso1207', 'ImFso1207']\n",
    "\n",
    "labels = ['ReSf1', 'ImSf1', 'ReSf3', 'ImSf3', 'ReSf5', 'ImSf5', 'ReSf7', 'ImSf7', 'ReSf9', 'ImSf9', 'ReSf11', 'ImSf11', 'ReSf13', 'ImSf13', 'ReSf15', 'ImSf15', 'ReSf17', 'ImSf17', 'ReSf19', 'ImSf19', 'ReSf21', 'ImSf21', 'ReSf23', 'ImSf23', 'ReSf25', 'ImSf25', 'ReSf29', 'ImSf29', 'ReSf33', 'ImSf33', 'ReSf37', 'ImSf37', 'ReSf43', 'ImSf43', 'ReSf49', 'ImSf49', 'ReSf57', 'ImSf57', 'ReSf69', 'ImSf69', 'ReSf83', 'ImSf83', 'ReSf101', 'ImSf101', 'ReSf127', 'ImSf127', 'ReSf165', 'ImSf165', 'ReSf237', 'ImSf237', 'ReSf399', 'ImSf399', 'ReSf1207', 'ImSf1207']\n",
    "\n",
    "df = pd.read_csv(file_path, skiprows=4) # we skip the first four lines, because they are just metadata\n",
    "df = df[fixed_features + labels]\n",
    "\n",
    "# remove one special row, looks very weird; ReSf1 = 2.377167465976437e-06\n",
    "df = df[df['ReSf1'] >= 1e-05]\n",
    "\n",
    "validation_size = 0.1 # 90% training, 10% for validation\n",
    "\n",
    "use_scaling = True\n",
    "\n",
    "if use_scaling:\n",
    "    feature_scaler, label_scaler = compute_scalers(df, fixed_features, labels, validation_size, seed) # make sure we use the same seed, otherwise the two splits differ!\n",
    "    dataset = ImpurityDataset(df, fixed_features, labels, feature_scaler, label_scaler, device=device)\n",
    "else:\n",
    "    dataset = ImpurityDataset(df, fixed_features, labels, device=device)\n",
    "\n",
    "indices = list(range(len(dataset)))\n",
    "train_indices, val_indices = train_test_split(indices, test_size=validation_size, random_state=seed)  # make sure we use the same seed, otherwise the two splits differ!\n",
    "\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "98f8b9d2-454a-41dd-bd6e-232135263d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.__getitem__(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "71ac3b91-cf54-40e2-b18b-682d9df0ee08",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(df['ReSf1'].abs())[:10];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23694fa-f39d-48b1-9a50-d815762d86ea",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "132329aa-fb68-482e-8e19-4c8163b908fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncodingL(nn.Module):\n",
    "    def __init__(self, T, C, dropout):\n",
    "        super(PositionalEncodingL, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.positional_embedding = nn.Parameter(torch.zeros(T, C))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        position_encoded = self.positional_embedding[:T, :].unsqueeze(0).expand(B, -1, -1)\n",
    "        x = x + position_encoded\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c648bc1c-a7b4-4b82-bf63-e06dd0967f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    input_dim: int\n",
    "    output_dim: int\n",
    "    sequence_length: int\n",
    "    \n",
    "    d_model: int\n",
    "    nhead: int\n",
    "    num_layers: int\n",
    "    dim_feedforward: int\n",
    "    \n",
    "    dropout: float\n",
    "    activation: str\n",
    "    bias: bool\n",
    "\n",
    "class AutoregressiveTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, config, device):\n",
    "        super(AutoregressiveTransformer, self).__init__()\n",
    "\n",
    "        self.config = config\n",
    "        self.input_projection = nn.Linear(config.input_dim, config.d_model)\n",
    "        self.positional_encoding = PositionalEncodingL(config.input_dim, config.d_model, config.dropout)\n",
    "        self.attention_pooling = nn.Linear(config.d_model, 1)\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(config.d_model)\n",
    "        self.ln2 = nn.LayerNorm(config.d_model)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=config.d_model, \n",
    "            nhead=config.nhead, \n",
    "            dim_feedforward=config.dim_feedforward, \n",
    "            dropout=config.dropout,\n",
    "            activation=config.activation, \n",
    "            batch_first=True, \n",
    "            norm_first=True, \n",
    "            bias=config.bias\n",
    "        )\n",
    "        \n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=config.num_layers)\n",
    "        self.output_layer = nn.Linear(config.d_model, config.output_dim)\n",
    "        \n",
    "    def forward(self, x):        \n",
    "        x = self.input_projection(x)\n",
    "        \n",
    "        x = x.transpose(1, 2)  # [BS, 1, d_model] -> [BS, d_model, 1]\n",
    "        x = x.expand(-1, -1, self.config.input_dim)  # [BS, d_model, 1] -> [BS, d_model, 61]\n",
    "        x = x.transpose(1, 2)  # [BS, d_model, 61] -> [BS, 61, d_model]\n",
    "\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.ln1(x)\n",
    "        \n",
    "        x = self.transformer_encoder(x)\n",
    "        \n",
    "        weights = torch.softmax(self.attention_pooling(x), dim=1)\n",
    "        x = (x * weights).sum(dim=1)\n",
    "        x = self.ln2(x)\n",
    "        \n",
    "        output = self.output_layer(x)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b0842f-41f6-420c-b896-4b19f67805c3",
   "metadata": {},
   "source": [
    "### Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5edc082b-3d45-4afb-95b6-4d2edaa703e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "816.439 k parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "config = ModelConfig(\n",
    "    input_dim = len(fixed_features),\n",
    "    output_dim = len(labels),\n",
    "    sequence_length = 1,\n",
    "    \n",
    "    d_model = 128,\n",
    "    nhead = 1,\n",
    "    num_layers = 4,\n",
    "    dim_feedforward = 128 * 4,\n",
    "    \n",
    "    dropout = 0.2,\n",
    "    activation = 'gelu',\n",
    "    bias = True\n",
    ")\n",
    "\n",
    "model = AutoregressiveTransformer(config, device).to(device)\n",
    "print(sum(p.numel() for p in model.parameters())/1e3, 'k parameters')\n",
    "criterion = nn.MSELoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "#scheduler = lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "dec32597-9f94-42fc-a936-be7eb75d331e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    \n",
    "    for inputs, targets in loader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "\n",
    "    return np.average(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "7d324c9b-2a4f-4b65-abcd-7ead8d07e4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for inputs, targets in loader:\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "    return np.average(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f72c8c95-6a90-4792-a74e-302534c87e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_mape(model, loader, device, use_scaling=use_scaling, epsilon=1e-8):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for inputs, targets in loader:\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            if use_scaling:\n",
    "                outputs = torch.tensor(reverse_tranform_output(outputs, label_scaler))\n",
    "                targets = torch.tensor(reverse_tranform_output(targets, label_scaler))\n",
    "            \n",
    "            ape = torch.abs((targets - outputs) / (targets + epsilon))\n",
    "            mape = torch.mean(ape) * 100\n",
    "            \n",
    "            losses.append(mape.item())\n",
    "\n",
    "    return np.average(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "aa4537b0-7c15-4d74-85bf-65d053516262",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_tranform_output(data, scaler):\n",
    "    B, C = data.shape\n",
    "    data = data.view(B, C)\n",
    "    data = data.cpu().numpy()\n",
    "    return scaler.inverse_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "711ddc3c-3eac-4ae7-9007-46b69f5c6ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102.1504100561142"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_output = len(fixed_features) - 1\n",
    "\n",
    "validate_mape(model, val_loader, device, use_scaling=use_scaling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec0284b-7904-4dbc-b42d-69b6d367b8d6",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b9067e6b-ea16-42e9-989b-ef6896481deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1: Train Loss: 0.117917, Train MAPE: 20.050%, Val Loss: 0.126711, Val MAPE: 22.991%\n",
      "Epoch   2: Train Loss: 0.076941, Train MAPE: 13.155%, Val Loss: 0.088881, Val MAPE: 16.608%\n",
      "Epoch   3: Train Loss: 0.052309, Train MAPE: 13.215%, Val Loss: 0.061127, Val MAPE: 16.868%\n",
      "Epoch   4: Train Loss: 0.072542, Train MAPE: 14.060%, Val Loss: 0.083208, Val MAPE: 17.516%\n",
      "Epoch   5: Train Loss: 0.059263, Train MAPE: 9.983%, Val Loss: 0.063015, Val MAPE: 10.860%\n",
      "Epoch   6: Train Loss: 0.037063, Train MAPE: 8.873%, Val Loss: 0.040486, Val MAPE: 10.795%\n",
      "Epoch   7: Train Loss: 0.033450, Train MAPE: 8.605%, Val Loss: 0.035491, Val MAPE: 9.935%\n",
      "Epoch   8: Train Loss: 0.029672, Train MAPE: 8.134%, Val Loss: 0.031849, Val MAPE: 8.974%\n",
      "Epoch   9: Train Loss: 0.055593, Train MAPE: 11.577%, Val Loss: 0.057361, Val MAPE: 11.880%\n",
      "Epoch  10: Train Loss: 0.032338, Train MAPE: 8.454%, Val Loss: 0.030841, Val MAPE: 8.660%\n",
      "Epoch  11: Train Loss: 0.023482, Train MAPE: 6.122%, Val Loss: 0.024081, Val MAPE: 7.159%\n",
      "Epoch  12: Train Loss: 0.021446, Train MAPE: 7.215%, Val Loss: 0.024232, Val MAPE: 8.084%\n",
      "Epoch  13: Train Loss: 0.017695, Train MAPE: 5.472%, Val Loss: 0.020646, Val MAPE: 6.310%\n",
      "Epoch  14: Train Loss: 0.018784, Train MAPE: 5.411%, Val Loss: 0.022678, Val MAPE: 6.533%\n",
      "Epoch  15: Train Loss: 0.030532, Train MAPE: 7.049%, Val Loss: 0.030697, Val MAPE: 7.740%\n",
      "Epoch  16: Train Loss: 0.023668, Train MAPE: 7.496%, Val Loss: 0.026428, Val MAPE: 8.443%\n",
      "Epoch  17: Train Loss: 0.016513, Train MAPE: 5.484%, Val Loss: 0.019198, Val MAPE: 6.312%\n",
      "Epoch  18: Train Loss: 0.020034, Train MAPE: 6.082%, Val Loss: 0.022484, Val MAPE: 7.254%\n",
      "Epoch  19: Train Loss: 0.016258, Train MAPE: 6.045%, Val Loss: 0.018218, Val MAPE: 7.001%\n",
      "Epoch  20: Train Loss: 0.015760, Train MAPE: 5.312%, Val Loss: 0.017338, Val MAPE: 6.348%\n",
      "Epoch  21: Train Loss: 0.024445, Train MAPE: 7.068%, Val Loss: 0.027217, Val MAPE: 8.444%\n",
      "Epoch  22: Train Loss: 0.014653, Train MAPE: 6.165%, Val Loss: 0.019629, Val MAPE: 7.281%\n",
      "Epoch  23: Train Loss: 0.034283, Train MAPE: 6.935%, Val Loss: 0.036092, Val MAPE: 8.306%\n",
      "Epoch  24: Train Loss: 0.011820, Train MAPE: 4.866%, Val Loss: 0.013752, Val MAPE: 5.991%\n",
      "Epoch  25: Train Loss: 0.013975, Train MAPE: 5.725%, Val Loss: 0.016354, Val MAPE: 7.337%\n",
      "Epoch  26: Train Loss: 0.011408, Train MAPE: 5.151%, Val Loss: 0.015156, Val MAPE: 6.007%\n",
      "Epoch  27: Train Loss: 0.013514, Train MAPE: 5.200%, Val Loss: 0.016984, Val MAPE: 6.458%\n",
      "Epoch  28: Train Loss: 0.017811, Train MAPE: 7.469%, Val Loss: 0.019512, Val MAPE: 9.177%\n",
      "Epoch  29: Train Loss: 0.014864, Train MAPE: 6.018%, Val Loss: 0.015908, Val MAPE: 6.971%\n",
      "Epoch  30: Train Loss: 0.017041, Train MAPE: 5.935%, Val Loss: 0.020696, Val MAPE: 7.233%\n",
      "Epoch  31: Train Loss: 0.011170, Train MAPE: 5.367%, Val Loss: 0.014831, Val MAPE: 6.603%\n",
      "Epoch  32: Train Loss: 0.013660, Train MAPE: 5.068%, Val Loss: 0.016068, Val MAPE: 6.029%\n",
      "Epoch  33: Train Loss: 0.012607, Train MAPE: 5.384%, Val Loss: 0.014435, Val MAPE: 6.203%\n",
      "Epoch  34: Train Loss: 0.012497, Train MAPE: 6.308%, Val Loss: 0.015215, Val MAPE: 7.512%\n",
      "Epoch  35: Train Loss: 0.014408, Train MAPE: 6.136%, Val Loss: 0.017465, Val MAPE: 7.089%\n",
      "Epoch  36: Train Loss: 0.011003, Train MAPE: 4.855%, Val Loss: 0.015421, Val MAPE: 5.632%\n",
      "Epoch  37: Train Loss: 0.032845, Train MAPE: 11.083%, Val Loss: 0.037291, Val MAPE: 12.072%\n",
      "Epoch  38: Train Loss: 0.011730, Train MAPE: 5.125%, Val Loss: 0.014179, Val MAPE: 6.079%\n",
      "Epoch  39: Train Loss: 0.047027, Train MAPE: 8.668%, Val Loss: 0.044179, Val MAPE: 10.019%\n",
      "Epoch  40: Train Loss: 0.011159, Train MAPE: 4.809%, Val Loss: 0.012994, Val MAPE: 5.769%\n",
      "Epoch  41: Train Loss: 0.010062, Train MAPE: 4.628%, Val Loss: 0.014018, Val MAPE: 5.584%\n",
      "Epoch  42: Train Loss: 0.009275, Train MAPE: 5.303%, Val Loss: 0.013193, Val MAPE: 6.171%\n",
      "Epoch  43: Train Loss: 0.009201, Train MAPE: 4.242%, Val Loss: 0.012712, Val MAPE: 5.223%\n",
      "Epoch  44: Train Loss: 0.010504, Train MAPE: 5.280%, Val Loss: 0.012663, Val MAPE: 6.212%\n",
      "Epoch  45: Train Loss: 0.009088, Train MAPE: 4.845%, Val Loss: 0.011767, Val MAPE: 5.757%\n",
      "Epoch  46: Train Loss: 0.008839, Train MAPE: 4.783%, Val Loss: 0.012316, Val MAPE: 5.470%\n",
      "Epoch  47: Train Loss: 0.054556, Train MAPE: 17.878%, Val Loss: 0.050266, Val MAPE: 19.648%\n",
      "Epoch  48: Train Loss: 0.010229, Train MAPE: 5.799%, Val Loss: 0.013109, Val MAPE: 6.440%\n",
      "Epoch  49: Train Loss: 0.012151, Train MAPE: 7.137%, Val Loss: 0.021911, Val MAPE: 8.369%\n",
      "Epoch  50: Train Loss: 0.008009, Train MAPE: 4.177%, Val Loss: 0.011559, Val MAPE: 4.972%\n",
      "Epoch  51: Train Loss: 0.045335, Train MAPE: 12.715%, Val Loss: 0.042952, Val MAPE: 13.844%\n",
      "Epoch  52: Train Loss: 0.010883, Train MAPE: 4.998%, Val Loss: 0.014866, Val MAPE: 5.704%\n",
      "Epoch  53: Train Loss: 0.013181, Train MAPE: 6.316%, Val Loss: 0.015609, Val MAPE: 7.419%\n",
      "Epoch  54: Train Loss: 0.009238, Train MAPE: 5.153%, Val Loss: 0.013687, Val MAPE: 5.886%\n",
      "Epoch  55: Train Loss: 0.008571, Train MAPE: 4.503%, Val Loss: 0.012800, Val MAPE: 5.017%\n",
      "Epoch  56: Train Loss: 0.009512, Train MAPE: 6.302%, Val Loss: 0.014141, Val MAPE: 7.173%\n",
      "Epoch  57: Train Loss: 0.008222, Train MAPE: 4.501%, Val Loss: 0.011564, Val MAPE: 5.175%\n",
      "Epoch  58: Train Loss: 0.011618, Train MAPE: 6.176%, Val Loss: 0.016434, Val MAPE: 7.330%\n",
      "Epoch  59: Train Loss: 0.008276, Train MAPE: 5.366%, Val Loss: 0.012149, Val MAPE: 6.384%\n",
      "Epoch  60: Train Loss: 0.007635, Train MAPE: 5.226%, Val Loss: 0.011555, Val MAPE: 6.059%\n",
      "Epoch  61: Train Loss: 0.006909, Train MAPE: 5.427%, Val Loss: 0.010219, Val MAPE: 5.902%\n",
      "Epoch  62: Train Loss: 0.007515, Train MAPE: 5.467%, Val Loss: 0.010111, Val MAPE: 6.106%\n",
      "Epoch  63: Train Loss: 0.008070, Train MAPE: 4.722%, Val Loss: 0.011827, Val MAPE: 5.545%\n",
      "Epoch  64: Train Loss: 0.019290, Train MAPE: 8.664%, Val Loss: 0.026279, Val MAPE: 9.996%\n",
      "Epoch  65: Train Loss: 0.006940, Train MAPE: 5.581%, Val Loss: 0.011830, Val MAPE: 6.529%\n",
      "Epoch  66: Train Loss: 0.007479, Train MAPE: 5.613%, Val Loss: 0.011479, Val MAPE: 6.683%\n",
      "Epoch  67: Train Loss: 0.006974, Train MAPE: 4.367%, Val Loss: 0.010534, Val MAPE: 5.126%\n",
      "Epoch  68: Train Loss: 0.015779, Train MAPE: 7.395%, Val Loss: 0.021861, Val MAPE: 8.350%\n",
      "Epoch  69: Train Loss: 0.008104, Train MAPE: 4.733%, Val Loss: 0.013537, Val MAPE: 6.400%\n",
      "Epoch  70: Train Loss: 0.007564, Train MAPE: 4.240%, Val Loss: 0.012934, Val MAPE: 5.042%\n",
      "Epoch  71: Train Loss: 0.007098, Train MAPE: 4.988%, Val Loss: 0.012366, Val MAPE: 6.262%\n",
      "Epoch  72: Train Loss: 0.007207, Train MAPE: 5.320%, Val Loss: 0.011358, Val MAPE: 6.000%\n",
      "Epoch  73: Train Loss: 0.017375, Train MAPE: 7.482%, Val Loss: 0.021748, Val MAPE: 8.272%\n",
      "Epoch  74: Train Loss: 0.007640, Train MAPE: 4.622%, Val Loss: 0.010625, Val MAPE: 5.392%\n",
      "Epoch  75: Train Loss: 0.006094, Train MAPE: 4.633%, Val Loss: 0.011389, Val MAPE: 5.684%\n",
      "Epoch  76: Train Loss: 0.007263, Train MAPE: 5.358%, Val Loss: 0.010019, Val MAPE: 6.089%\n",
      "Epoch  77: Train Loss: 0.005925, Train MAPE: 5.422%, Val Loss: 0.010719, Val MAPE: 6.216%\n",
      "Epoch  78: Train Loss: 0.007942, Train MAPE: 6.242%, Val Loss: 0.012663, Val MAPE: 7.292%\n",
      "Epoch  79: Train Loss: 0.008766, Train MAPE: 5.159%, Val Loss: 0.013261, Val MAPE: 6.080%\n",
      "Epoch  80: Train Loss: 0.006133, Train MAPE: 4.725%, Val Loss: 0.010889, Val MAPE: 5.451%\n",
      "Epoch  81: Train Loss: 0.007895, Train MAPE: 5.939%, Val Loss: 0.013177, Val MAPE: 6.885%\n",
      "Epoch  82: Train Loss: 0.007284, Train MAPE: 5.044%, Val Loss: 0.012227, Val MAPE: 5.867%\n",
      "Epoch  83: Train Loss: 0.005391, Train MAPE: 4.446%, Val Loss: 0.009951, Val MAPE: 5.413%\n",
      "Epoch  84: Train Loss: 0.006462, Train MAPE: 4.569%, Val Loss: 0.012331, Val MAPE: 5.426%\n",
      "Epoch  85: Train Loss: 0.009982, Train MAPE: 5.644%, Val Loss: 0.013619, Val MAPE: 6.508%\n",
      "Epoch  86: Train Loss: 0.004969, Train MAPE: 3.699%, Val Loss: 0.009233, Val MAPE: 4.729%\n",
      "Epoch  87: Train Loss: 0.006171, Train MAPE: 4.248%, Val Loss: 0.011416, Val MAPE: 5.845%\n",
      "Epoch  88: Train Loss: 0.006333, Train MAPE: 4.373%, Val Loss: 0.011934, Val MAPE: 5.450%\n",
      "Epoch  89: Train Loss: 0.004719, Train MAPE: 4.450%, Val Loss: 0.009948, Val MAPE: 5.386%\n",
      "Epoch  90: Train Loss: 0.006359, Train MAPE: 4.627%, Val Loss: 0.009374, Val MAPE: 5.523%\n",
      "Epoch  91: Train Loss: 0.005437, Train MAPE: 3.993%, Val Loss: 0.010949, Val MAPE: 4.703%\n",
      "Epoch  92: Train Loss: 0.004754, Train MAPE: 3.738%, Val Loss: 0.008627, Val MAPE: 4.584%\n",
      "Epoch  93: Train Loss: 0.004838, Train MAPE: 4.774%, Val Loss: 0.008434, Val MAPE: 5.572%\n",
      "Epoch  94: Train Loss: 0.006920, Train MAPE: 3.982%, Val Loss: 0.011800, Val MAPE: 4.623%\n",
      "Epoch  95: Train Loss: 0.015232, Train MAPE: 6.107%, Val Loss: 0.018753, Val MAPE: 6.809%\n",
      "Epoch  96: Train Loss: 0.006167, Train MAPE: 4.522%, Val Loss: 0.012772, Val MAPE: 5.733%\n",
      "Epoch  97: Train Loss: 0.005240, Train MAPE: 4.549%, Val Loss: 0.009998, Val MAPE: 5.360%\n",
      "Epoch  98: Train Loss: 0.006535, Train MAPE: 4.893%, Val Loss: 0.011903, Val MAPE: 5.789%\n",
      "Epoch  99: Train Loss: 0.005420, Train MAPE: 3.730%, Val Loss: 0.010209, Val MAPE: 4.642%\n",
      "Epoch 100: Train Loss: 0.006951, Train MAPE: 4.779%, Val Loss: 0.010189, Val MAPE: 5.593%\n",
      "Epoch 101: Train Loss: 0.007493, Train MAPE: 5.231%, Val Loss: 0.012456, Val MAPE: 6.254%\n",
      "Epoch 102: Train Loss: 0.004640, Train MAPE: 4.591%, Val Loss: 0.009375, Val MAPE: 5.401%\n",
      "Epoch 103: Train Loss: 0.004734, Train MAPE: 3.842%, Val Loss: 0.008774, Val MAPE: 4.607%\n",
      "Epoch 104: Train Loss: 0.006807, Train MAPE: 5.335%, Val Loss: 0.010820, Val MAPE: 6.454%\n",
      "Epoch 105: Train Loss: 0.005347, Train MAPE: 3.796%, Val Loss: 0.010074, Val MAPE: 4.749%\n",
      "Epoch 106: Train Loss: 0.004718, Train MAPE: 4.401%, Val Loss: 0.008607, Val MAPE: 5.067%\n",
      "Epoch 107: Train Loss: 0.007150, Train MAPE: 6.519%, Val Loss: 0.010092, Val MAPE: 7.901%\n",
      "Epoch 108: Train Loss: 0.006155, Train MAPE: 5.012%, Val Loss: 0.011882, Val MAPE: 5.779%\n",
      "Epoch 109: Train Loss: 0.005651, Train MAPE: 4.655%, Val Loss: 0.010291, Val MAPE: 5.591%\n",
      "Epoch 110: Train Loss: 0.004944, Train MAPE: 4.752%, Val Loss: 0.010706, Val MAPE: 5.492%\n",
      "Epoch 111: Train Loss: 0.007284, Train MAPE: 4.585%, Val Loss: 0.010825, Val MAPE: 5.290%\n",
      "Epoch 112: Train Loss: 0.006864, Train MAPE: 4.992%, Val Loss: 0.011170, Val MAPE: 6.068%\n",
      "Epoch 113: Train Loss: 0.004668, Train MAPE: 4.001%, Val Loss: 0.009731, Val MAPE: 4.706%\n",
      "Epoch 114: Train Loss: 0.003961, Train MAPE: 4.154%, Val Loss: 0.009243, Val MAPE: 5.307%\n",
      "Epoch 115: Train Loss: 0.005145, Train MAPE: 4.028%, Val Loss: 0.010468, Val MAPE: 4.713%\n",
      "Epoch 116: Train Loss: 0.004723, Train MAPE: 4.610%, Val Loss: 0.010067, Val MAPE: 5.503%\n",
      "Epoch 117: Train Loss: 0.004864, Train MAPE: 4.570%, Val Loss: 0.009149, Val MAPE: 5.304%\n",
      "Epoch 118: Train Loss: 0.004272, Train MAPE: 5.325%, Val Loss: 0.009558, Val MAPE: 6.261%\n",
      "Epoch 119: Train Loss: 0.005844, Train MAPE: 5.791%, Val Loss: 0.011192, Val MAPE: 6.623%\n",
      "Epoch 120: Train Loss: 0.011997, Train MAPE: 5.281%, Val Loss: 0.014368, Val MAPE: 6.047%\n",
      "Epoch 121: Train Loss: 0.026311, Train MAPE: 7.676%, Val Loss: 0.037988, Val MAPE: 8.730%\n",
      "Epoch 122: Train Loss: 0.004054, Train MAPE: 3.384%, Val Loss: 0.009703, Val MAPE: 4.335%\n",
      "Epoch 123: Train Loss: 0.007198, Train MAPE: 6.006%, Val Loss: 0.011263, Val MAPE: 7.507%\n",
      "Epoch 124: Train Loss: 0.008317, Train MAPE: 5.062%, Val Loss: 0.014380, Val MAPE: 6.101%\n",
      "Epoch 125: Train Loss: 0.005879, Train MAPE: 3.815%, Val Loss: 0.010560, Val MAPE: 4.487%\n",
      "Epoch 126: Train Loss: 0.003882, Train MAPE: 4.522%, Val Loss: 0.008652, Val MAPE: 5.377%\n",
      "Epoch 127: Train Loss: 0.004075, Train MAPE: 3.893%, Val Loss: 0.010736, Val MAPE: 4.608%\n",
      "Epoch 128: Train Loss: 0.004785, Train MAPE: 5.149%, Val Loss: 0.009526, Val MAPE: 6.417%\n",
      "Epoch 129: Train Loss: 0.004683, Train MAPE: 4.439%, Val Loss: 0.008256, Val MAPE: 5.306%\n",
      "Epoch 130: Train Loss: 0.003754, Train MAPE: 4.535%, Val Loss: 0.007528, Val MAPE: 5.506%\n",
      "Epoch 131: Train Loss: 0.005501, Train MAPE: 7.258%, Val Loss: 0.009730, Val MAPE: 8.262%\n",
      "Epoch 132: Train Loss: 0.004140, Train MAPE: 5.369%, Val Loss: 0.008557, Val MAPE: 6.270%\n",
      "Epoch 133: Train Loss: 0.004059, Train MAPE: 3.838%, Val Loss: 0.008622, Val MAPE: 4.673%\n",
      "Epoch 134: Train Loss: 0.005563, Train MAPE: 3.966%, Val Loss: 0.011222, Val MAPE: 4.887%\n",
      "Epoch 135: Train Loss: 0.004866, Train MAPE: 4.425%, Val Loss: 0.009252, Val MAPE: 5.382%\n",
      "Epoch 136: Train Loss: 0.003857, Train MAPE: 4.966%, Val Loss: 0.008960, Val MAPE: 5.961%\n",
      "Epoch 137: Train Loss: 0.005386, Train MAPE: 3.319%, Val Loss: 0.011221, Val MAPE: 4.289%\n",
      "Epoch 138: Train Loss: 0.013003, Train MAPE: 7.529%, Val Loss: 0.020131, Val MAPE: 9.034%\n",
      "Epoch 139: Train Loss: 0.005974, Train MAPE: 5.057%, Val Loss: 0.015462, Val MAPE: 5.872%\n",
      "Epoch 140: Train Loss: 0.005188, Train MAPE: 5.061%, Val Loss: 0.011016, Val MAPE: 7.067%\n",
      "Epoch 141: Train Loss: 0.003398, Train MAPE: 4.364%, Val Loss: 0.008925, Val MAPE: 5.141%\n",
      "Epoch 142: Train Loss: 0.005680, Train MAPE: 6.151%, Val Loss: 0.012325, Val MAPE: 7.392%\n",
      "Epoch 143: Train Loss: 0.004037, Train MAPE: 4.303%, Val Loss: 0.008057, Val MAPE: 5.260%\n",
      "Epoch 144: Train Loss: 0.003629, Train MAPE: 3.355%, Val Loss: 0.008557, Val MAPE: 4.034%\n",
      "Epoch 145: Train Loss: 0.003524, Train MAPE: 5.097%, Val Loss: 0.009262, Val MAPE: 6.453%\n",
      "Epoch 146: Train Loss: 0.011762, Train MAPE: 5.580%, Val Loss: 0.014708, Val MAPE: 6.131%\n",
      "Epoch 147: Train Loss: 0.003355, Train MAPE: 3.756%, Val Loss: 0.008163, Val MAPE: 4.234%\n",
      "Epoch 148: Train Loss: 0.002587, Train MAPE: 3.187%, Val Loss: 0.007177, Val MAPE: 3.737%\n",
      "Epoch 149: Train Loss: 0.005035, Train MAPE: 4.312%, Val Loss: 0.009361, Val MAPE: 4.975%\n",
      "Epoch 150: Train Loss: 0.003511, Train MAPE: 5.622%, Val Loss: 0.007817, Val MAPE: 6.317%\n",
      "Epoch 151: Train Loss: 0.002782, Train MAPE: 4.253%, Val Loss: 0.008231, Val MAPE: 4.959%\n",
      "Epoch 152: Train Loss: 0.003189, Train MAPE: 3.825%, Val Loss: 0.008142, Val MAPE: 4.438%\n",
      "Epoch 153: Train Loss: 0.003867, Train MAPE: 4.810%, Val Loss: 0.008894, Val MAPE: 5.604%\n",
      "Epoch 154: Train Loss: 0.003102, Train MAPE: 4.958%, Val Loss: 0.007742, Val MAPE: 6.750%\n",
      "Epoch 155: Train Loss: 0.003819, Train MAPE: 3.620%, Val Loss: 0.008756, Val MAPE: 4.328%\n",
      "Epoch 156: Train Loss: 0.006704, Train MAPE: 6.481%, Val Loss: 0.009422, Val MAPE: 7.237%\n",
      "Epoch 157: Train Loss: 0.004107, Train MAPE: 3.613%, Val Loss: 0.009238, Val MAPE: 4.206%\n",
      "Epoch 158: Train Loss: 0.003160, Train MAPE: 3.963%, Val Loss: 0.009334, Val MAPE: 4.608%\n",
      "Epoch 159: Train Loss: 0.002749, Train MAPE: 3.761%, Val Loss: 0.007521, Val MAPE: 4.454%\n",
      "Epoch 160: Train Loss: 0.003664, Train MAPE: 4.349%, Val Loss: 0.008477, Val MAPE: 5.075%\n",
      "Epoch 161: Train Loss: 0.005040, Train MAPE: 6.040%, Val Loss: 0.009538, Val MAPE: 6.722%\n",
      "Epoch 162: Train Loss: 0.005406, Train MAPE: 5.025%, Val Loss: 0.010425, Val MAPE: 5.904%\n",
      "Epoch 163: Train Loss: 0.003350, Train MAPE: 3.397%, Val Loss: 0.008236, Val MAPE: 4.255%\n",
      "Epoch 164: Train Loss: 0.004288, Train MAPE: 5.273%, Val Loss: 0.009117, Val MAPE: 5.860%\n",
      "Epoch 165: Train Loss: 0.002405, Train MAPE: 3.582%, Val Loss: 0.008043, Val MAPE: 4.317%\n",
      "Epoch 166: Train Loss: 0.003925, Train MAPE: 4.690%, Val Loss: 0.008124, Val MAPE: 6.200%\n",
      "Epoch 167: Train Loss: 0.004992, Train MAPE: 3.730%, Val Loss: 0.009921, Val MAPE: 4.329%\n",
      "Epoch 168: Train Loss: 0.002615, Train MAPE: 3.636%, Val Loss: 0.007973, Val MAPE: 4.403%\n",
      "Epoch 169: Train Loss: 0.003245, Train MAPE: 4.704%, Val Loss: 0.008189, Val MAPE: 5.578%\n",
      "Epoch 170: Train Loss: 0.002859, Train MAPE: 4.582%, Val Loss: 0.008187, Val MAPE: 5.503%\n",
      "Epoch 171: Train Loss: 0.002675, Train MAPE: 3.816%, Val Loss: 0.008700, Val MAPE: 5.726%\n",
      "Epoch 172: Train Loss: 0.002620, Train MAPE: 3.430%, Val Loss: 0.007641, Val MAPE: 4.020%\n",
      "Epoch 173: Train Loss: 0.002561, Train MAPE: 4.130%, Val Loss: 0.008351, Val MAPE: 4.799%\n",
      "Epoch 174: Train Loss: 0.009864, Train MAPE: 4.281%, Val Loss: 0.013937, Val MAPE: 5.571%\n",
      "Epoch 175: Train Loss: 0.003121, Train MAPE: 3.330%, Val Loss: 0.008297, Val MAPE: 3.859%\n",
      "Epoch 176: Train Loss: 0.003395, Train MAPE: 3.803%, Val Loss: 0.008411, Val MAPE: 4.828%\n",
      "Epoch 177: Train Loss: 0.005692, Train MAPE: 4.880%, Val Loss: 0.010789, Val MAPE: 5.959%\n",
      "Epoch 178: Train Loss: 0.003269, Train MAPE: 4.551%, Val Loss: 0.009763, Val MAPE: 5.157%\n",
      "Epoch 179: Train Loss: 0.002677, Train MAPE: 4.804%, Val Loss: 0.007295, Val MAPE: 6.377%\n",
      "Epoch 180: Train Loss: 0.002326, Train MAPE: 3.547%, Val Loss: 0.008134, Val MAPE: 4.509%\n",
      "Epoch 181: Train Loss: 0.003093, Train MAPE: 4.166%, Val Loss: 0.010662, Val MAPE: 7.528%\n",
      "Epoch 182: Train Loss: 0.002199, Train MAPE: 4.226%, Val Loss: 0.007780, Val MAPE: 4.967%\n",
      "Epoch 183: Train Loss: 0.003193, Train MAPE: 6.010%, Val Loss: 0.009569, Val MAPE: 7.148%\n",
      "Epoch 184: Train Loss: 0.002366, Train MAPE: 4.543%, Val Loss: 0.008774, Val MAPE: 5.771%\n",
      "Epoch 185: Train Loss: 0.003367, Train MAPE: 5.564%, Val Loss: 0.009493, Val MAPE: 6.944%\n",
      "Epoch 186: Train Loss: 0.005108, Train MAPE: 3.672%, Val Loss: 0.011054, Val MAPE: 4.528%\n",
      "Epoch 187: Train Loss: 0.003315, Train MAPE: 4.925%, Val Loss: 0.009262, Val MAPE: 5.668%\n",
      "Epoch 188: Train Loss: 0.004900, Train MAPE: 4.195%, Val Loss: 0.011103, Val MAPE: 4.861%\n",
      "Epoch 189: Train Loss: 0.005001, Train MAPE: 6.570%, Val Loss: 0.010435, Val MAPE: 7.616%\n",
      "Epoch 190: Train Loss: 0.003949, Train MAPE: 3.973%, Val Loss: 0.011803, Val MAPE: 5.172%\n",
      "Epoch 191: Train Loss: 0.007918, Train MAPE: 6.888%, Val Loss: 0.011367, Val MAPE: 8.230%\n",
      "Epoch 192: Train Loss: 0.003632, Train MAPE: 5.696%, Val Loss: 0.009551, Val MAPE: 6.672%\n",
      "Epoch 193: Train Loss: 0.004126, Train MAPE: 3.827%, Val Loss: 0.010404, Val MAPE: 4.721%\n",
      "Epoch 194: Train Loss: 0.002124, Train MAPE: 3.457%, Val Loss: 0.007332, Val MAPE: 4.363%\n",
      "Epoch 195: Train Loss: 0.002506, Train MAPE: 4.087%, Val Loss: 0.008653, Val MAPE: 4.776%\n",
      "Epoch 196: Train Loss: 0.010531, Train MAPE: 5.616%, Val Loss: 0.014193, Val MAPE: 6.547%\n",
      "Epoch 197: Train Loss: 0.002583, Train MAPE: 4.200%, Val Loss: 0.008238, Val MAPE: 5.067%\n",
      "Epoch 198: Train Loss: 0.002199, Train MAPE: 2.651%, Val Loss: 0.007829, Val MAPE: 3.178%\n",
      "Epoch 199: Train Loss: 0.005579, Train MAPE: 6.351%, Val Loss: 0.007917, Val MAPE: 6.740%\n",
      "Epoch 200: Train Loss: 0.003328, Train MAPE: 4.026%, Val Loss: 0.007435, Val MAPE: 4.657%\n",
      "Epoch 201: Train Loss: 0.003479, Train MAPE: 5.191%, Val Loss: 0.009538, Val MAPE: 6.055%\n",
      "Epoch 202: Train Loss: 0.002056, Train MAPE: 3.226%, Val Loss: 0.007585, Val MAPE: 3.878%\n",
      "Epoch 203: Train Loss: 0.002475, Train MAPE: 4.114%, Val Loss: 0.008507, Val MAPE: 5.131%\n",
      "Epoch 204: Train Loss: 0.002758, Train MAPE: 4.303%, Val Loss: 0.008112, Val MAPE: 5.688%\n",
      "Epoch 205: Train Loss: 0.002556, Train MAPE: 3.126%, Val Loss: 0.007332, Val MAPE: 3.743%\n",
      "Epoch 206: Train Loss: 0.002425, Train MAPE: 3.963%, Val Loss: 0.008684, Val MAPE: 4.689%\n",
      "Epoch 207: Train Loss: 0.002660, Train MAPE: 4.368%, Val Loss: 0.008944, Val MAPE: 6.279%\n",
      "Epoch 208: Train Loss: 0.002831, Train MAPE: 3.587%, Val Loss: 0.009471, Val MAPE: 4.336%\n",
      "Epoch 209: Train Loss: 0.002156, Train MAPE: 3.609%, Val Loss: 0.007969, Val MAPE: 4.249%\n",
      "Epoch 210: Train Loss: 0.001760, Train MAPE: 3.111%, Val Loss: 0.008048, Val MAPE: 3.594%\n",
      "Epoch 211: Train Loss: 0.002403, Train MAPE: 4.366%, Val Loss: 0.008852, Val MAPE: 4.874%\n",
      "Epoch 212: Train Loss: 0.002086, Train MAPE: 2.693%, Val Loss: 0.008441, Val MAPE: 3.460%\n",
      "Epoch 213: Train Loss: 0.002224, Train MAPE: 3.611%, Val Loss: 0.008134, Val MAPE: 4.303%\n",
      "Epoch 214: Train Loss: 0.003395, Train MAPE: 3.948%, Val Loss: 0.010083, Val MAPE: 4.755%\n",
      "Epoch 215: Train Loss: 0.003670, Train MAPE: 3.724%, Val Loss: 0.009270, Val MAPE: 4.439%\n",
      "Epoch 216: Train Loss: 0.003895, Train MAPE: 5.028%, Val Loss: 0.008754, Val MAPE: 6.365%\n",
      "Epoch 217: Train Loss: 0.001973, Train MAPE: 3.866%, Val Loss: 0.007431, Val MAPE: 4.679%\n",
      "Epoch 218: Train Loss: 0.003303, Train MAPE: 5.892%, Val Loss: 0.009011, Val MAPE: 7.088%\n",
      "Epoch 219: Train Loss: 0.002615, Train MAPE: 3.591%, Val Loss: 0.008135, Val MAPE: 4.214%\n",
      "Epoch 220: Train Loss: 0.002020, Train MAPE: 2.666%, Val Loss: 0.007771, Val MAPE: 3.159%\n",
      "Epoch 221: Train Loss: 0.002397, Train MAPE: 2.939%, Val Loss: 0.007653, Val MAPE: 3.394%\n",
      "Epoch 222: Train Loss: 0.002685, Train MAPE: 3.915%, Val Loss: 0.008750, Val MAPE: 4.615%\n",
      "Epoch 223: Train Loss: 0.002192, Train MAPE: 3.110%, Val Loss: 0.007614, Val MAPE: 3.566%\n",
      "Epoch 224: Train Loss: 0.001908, Train MAPE: 3.870%, Val Loss: 0.007304, Val MAPE: 4.341%\n",
      "Epoch 225: Train Loss: 0.003741, Train MAPE: 5.882%, Val Loss: 0.010227, Val MAPE: 7.620%\n",
      "Epoch 226: Train Loss: 0.002114, Train MAPE: 3.670%, Val Loss: 0.011311, Val MAPE: 4.378%\n",
      "Epoch 227: Train Loss: 0.002027, Train MAPE: 3.146%, Val Loss: 0.007982, Val MAPE: 3.605%\n",
      "Epoch 228: Train Loss: 0.002332, Train MAPE: 3.379%, Val Loss: 0.008441, Val MAPE: 4.023%\n",
      "Epoch 229: Train Loss: 0.002470, Train MAPE: 3.601%, Val Loss: 0.008958, Val MAPE: 4.285%\n",
      "Epoch 230: Train Loss: 0.002136, Train MAPE: 3.909%, Val Loss: 0.008409, Val MAPE: 4.528%\n",
      "Epoch 231: Train Loss: 0.003524, Train MAPE: 4.973%, Val Loss: 0.009633, Val MAPE: 5.714%\n",
      "Epoch 232: Train Loss: 0.002046, Train MAPE: 4.451%, Val Loss: 0.009431, Val MAPE: 5.315%\n",
      "Epoch 233: Train Loss: 0.002625, Train MAPE: 3.140%, Val Loss: 0.009855, Val MAPE: 3.866%\n",
      "Epoch 234: Train Loss: 0.006533, Train MAPE: 5.874%, Val Loss: 0.017496, Val MAPE: 6.745%\n",
      "Epoch 235: Train Loss: 0.001984, Train MAPE: 3.146%, Val Loss: 0.007892, Val MAPE: 3.615%\n",
      "Epoch 236: Train Loss: 0.002203, Train MAPE: 5.339%, Val Loss: 0.008122, Val MAPE: 6.150%\n",
      "Epoch 237: Train Loss: 0.003344, Train MAPE: 4.389%, Val Loss: 0.009856, Val MAPE: 4.920%\n",
      "Epoch 238: Train Loss: 0.002220, Train MAPE: 3.928%, Val Loss: 0.006842, Val MAPE: 4.419%\n",
      "Epoch 239: Train Loss: 0.003243, Train MAPE: 6.309%, Val Loss: 0.009257, Val MAPE: 7.239%\n",
      "Epoch 240: Train Loss: 0.001998, Train MAPE: 4.067%, Val Loss: 0.007692, Val MAPE: 4.544%\n",
      "Epoch 241: Train Loss: 0.008771, Train MAPE: 5.868%, Val Loss: 0.017040, Val MAPE: 6.784%\n",
      "Epoch 242: Train Loss: 0.007092, Train MAPE: 7.623%, Val Loss: 0.011942, Val MAPE: 8.459%\n",
      "Epoch 243: Train Loss: 0.019286, Train MAPE: 9.734%, Val Loss: 0.031233, Val MAPE: 12.227%\n",
      "Epoch 244: Train Loss: 0.002677, Train MAPE: 4.564%, Val Loss: 0.009455, Val MAPE: 5.579%\n",
      "Epoch 245: Train Loss: 0.001941, Train MAPE: 4.470%, Val Loss: 0.007459, Val MAPE: 5.229%\n",
      "Epoch 246: Train Loss: 0.002050, Train MAPE: 3.727%, Val Loss: 0.008320, Val MAPE: 4.240%\n",
      "Epoch 247: Train Loss: 0.002713, Train MAPE: 4.064%, Val Loss: 0.009171, Val MAPE: 4.659%\n",
      "Epoch 248: Train Loss: 0.002136, Train MAPE: 4.192%, Val Loss: 0.007845, Val MAPE: 4.628%\n",
      "Epoch 249: Train Loss: 0.002242, Train MAPE: 3.937%, Val Loss: 0.008031, Val MAPE: 4.391%\n",
      "Epoch 250: Train Loss: 0.001472, Train MAPE: 3.350%, Val Loss: 0.007730, Val MAPE: 3.946%\n",
      "Epoch 251: Train Loss: 0.001820, Train MAPE: 4.872%, Val Loss: 0.007823, Val MAPE: 5.498%\n",
      "Epoch 252: Train Loss: 0.001725, Train MAPE: 3.202%, Val Loss: 0.007582, Val MAPE: 3.667%\n",
      "Epoch 253: Train Loss: 0.001504, Train MAPE: 3.525%, Val Loss: 0.007295, Val MAPE: 4.033%\n",
      "Epoch 254: Train Loss: 0.001627, Train MAPE: 4.085%, Val Loss: 0.008555, Val MAPE: 4.767%\n",
      "Epoch 255: Train Loss: 0.003060, Train MAPE: 4.990%, Val Loss: 0.009648, Val MAPE: 5.569%\n",
      "Epoch 256: Train Loss: 0.002047, Train MAPE: 4.798%, Val Loss: 0.007134, Val MAPE: 5.611%\n",
      "Epoch 257: Train Loss: 0.013815, Train MAPE: 5.269%, Val Loss: 0.019256, Val MAPE: 6.066%\n",
      "Epoch 258: Train Loss: 0.008062, Train MAPE: 8.055%, Val Loss: 0.014337, Val MAPE: 10.319%\n",
      "Epoch 259: Train Loss: 0.002668, Train MAPE: 4.340%, Val Loss: 0.008799, Val MAPE: 4.946%\n",
      "Epoch 260: Train Loss: 0.002019, Train MAPE: 3.292%, Val Loss: 0.008387, Val MAPE: 3.931%\n",
      "Epoch 261: Train Loss: 0.001805, Train MAPE: 3.334%, Val Loss: 0.007357, Val MAPE: 3.874%\n",
      "Epoch 262: Train Loss: 0.001614, Train MAPE: 2.948%, Val Loss: 0.006848, Val MAPE: 3.518%\n",
      "Epoch 263: Train Loss: 0.001688, Train MAPE: 3.755%, Val Loss: 0.007085, Val MAPE: 4.499%\n",
      "Epoch 264: Train Loss: 0.001285, Train MAPE: 2.903%, Val Loss: 0.007371, Val MAPE: 3.500%\n",
      "Epoch 265: Train Loss: 0.001662, Train MAPE: 3.007%, Val Loss: 0.008493, Val MAPE: 3.525%\n",
      "Epoch 266: Train Loss: 0.002339, Train MAPE: 5.244%, Val Loss: 0.009766, Val MAPE: 6.069%\n",
      "Epoch 267: Train Loss: 0.002510, Train MAPE: 4.270%, Val Loss: 0.008358, Val MAPE: 4.949%\n",
      "Epoch 268: Train Loss: 0.003773, Train MAPE: 6.455%, Val Loss: 0.010554, Val MAPE: 7.706%\n",
      "Epoch 269: Train Loss: 0.002388, Train MAPE: 4.236%, Val Loss: 0.008585, Val MAPE: 4.971%\n",
      "Epoch 270: Train Loss: 0.003112, Train MAPE: 4.406%, Val Loss: 0.007931, Val MAPE: 5.219%\n",
      "Epoch 271: Train Loss: 0.001989, Train MAPE: 3.361%, Val Loss: 0.007450, Val MAPE: 4.239%\n",
      "Epoch 272: Train Loss: 0.001594, Train MAPE: 3.871%, Val Loss: 0.007271, Val MAPE: 4.492%\n",
      "Epoch 273: Train Loss: 0.002307, Train MAPE: 3.862%, Val Loss: 0.008718, Val MAPE: 4.508%\n",
      "Epoch 274: Train Loss: 0.002186, Train MAPE: 2.890%, Val Loss: 0.007801, Val MAPE: 3.492%\n",
      "Epoch 275: Train Loss: 0.001558, Train MAPE: 3.406%, Val Loss: 0.007897, Val MAPE: 4.063%\n",
      "Epoch 276: Train Loss: 0.002277, Train MAPE: 4.482%, Val Loss: 0.008524, Val MAPE: 5.427%\n",
      "Epoch 277: Train Loss: 0.001984, Train MAPE: 3.559%, Val Loss: 0.007551, Val MAPE: 4.275%\n",
      "Epoch 278: Train Loss: 0.001910, Train MAPE: 4.365%, Val Loss: 0.007970, Val MAPE: 5.126%\n",
      "Epoch 279: Train Loss: 0.002061, Train MAPE: 4.267%, Val Loss: 0.008373, Val MAPE: 4.975%\n",
      "Epoch 280: Train Loss: 0.027209, Train MAPE: 4.934%, Val Loss: 0.021870, Val MAPE: 5.527%\n",
      "Epoch 281: Train Loss: 0.001907, Train MAPE: 3.183%, Val Loss: 0.008487, Val MAPE: 3.743%\n",
      "Epoch 282: Train Loss: 0.001413, Train MAPE: 2.532%, Val Loss: 0.008337, Val MAPE: 3.159%\n",
      "Epoch 283: Train Loss: 0.001478, Train MAPE: 2.945%, Val Loss: 0.008169, Val MAPE: 3.699%\n",
      "Epoch 284: Train Loss: 0.002016, Train MAPE: 2.776%, Val Loss: 0.008231, Val MAPE: 3.446%\n",
      "Epoch 285: Train Loss: 0.002013, Train MAPE: 3.319%, Val Loss: 0.009462, Val MAPE: 3.906%\n",
      "Epoch 286: Train Loss: 0.001404, Train MAPE: 2.984%, Val Loss: 0.007626, Val MAPE: 3.648%\n",
      "Epoch 287: Train Loss: 0.003121, Train MAPE: 4.886%, Val Loss: 0.010006, Val MAPE: 5.384%\n",
      "Epoch 288: Train Loss: 0.002162, Train MAPE: 3.781%, Val Loss: 0.008571, Val MAPE: 4.397%\n",
      "Epoch 289: Train Loss: 0.001723, Train MAPE: 3.100%, Val Loss: 0.007681, Val MAPE: 3.549%\n",
      "Epoch 290: Train Loss: 0.002077, Train MAPE: 3.599%, Val Loss: 0.008642, Val MAPE: 4.167%\n",
      "Epoch 291: Train Loss: 0.002018, Train MAPE: 2.957%, Val Loss: 0.008781, Val MAPE: 3.664%\n",
      "Epoch 292: Train Loss: 0.001909, Train MAPE: 4.290%, Val Loss: 0.008206, Val MAPE: 4.910%\n",
      "Epoch 293: Train Loss: 0.002056, Train MAPE: 3.226%, Val Loss: 0.008420, Val MAPE: 3.808%\n",
      "Epoch 294: Train Loss: 0.001893, Train MAPE: 3.932%, Val Loss: 0.008788, Val MAPE: 4.573%\n",
      "Epoch 295: Train Loss: 0.003562, Train MAPE: 5.877%, Val Loss: 0.009475, Val MAPE: 6.899%\n",
      "Epoch 296: Train Loss: 0.001637, Train MAPE: 3.881%, Val Loss: 0.011033, Val MAPE: 4.572%\n",
      "Epoch 297: Train Loss: 0.001760, Train MAPE: 4.499%, Val Loss: 0.008087, Val MAPE: 5.261%\n",
      "Epoch 298: Train Loss: 0.002109, Train MAPE: 4.010%, Val Loss: 0.009941, Val MAPE: 4.807%\n",
      "Epoch 299: Train Loss: 0.002024, Train MAPE: 4.337%, Val Loss: 0.008101, Val MAPE: 4.832%\n",
      "Epoch 300: Train Loss: 0.001481, Train MAPE: 4.181%, Val Loss: 0.008011, Val MAPE: 4.839%\n",
      "Epoch 301: Train Loss: 0.001589, Train MAPE: 3.930%, Val Loss: 0.008186, Val MAPE: 4.506%\n",
      "Epoch 302: Train Loss: 0.002117, Train MAPE: 5.156%, Val Loss: 0.008180, Val MAPE: 5.815%\n",
      "Epoch 303: Train Loss: 0.001716, Train MAPE: 5.736%, Val Loss: 0.008475, Val MAPE: 6.626%\n",
      "Epoch 304: Train Loss: 0.001646, Train MAPE: 3.626%, Val Loss: 0.007697, Val MAPE: 4.162%\n",
      "Epoch 305: Train Loss: 0.001805, Train MAPE: 3.523%, Val Loss: 0.008104, Val MAPE: 4.126%\n",
      "Epoch 306: Train Loss: 0.002264, Train MAPE: 3.837%, Val Loss: 0.008337, Val MAPE: 4.523%\n",
      "Epoch 307: Train Loss: 0.002260, Train MAPE: 4.368%, Val Loss: 0.008745, Val MAPE: 5.045%\n",
      "Epoch 308: Train Loss: 0.002086, Train MAPE: 4.731%, Val Loss: 0.008245, Val MAPE: 5.459%\n",
      "Epoch 309: Train Loss: 0.003017, Train MAPE: 5.298%, Val Loss: 0.008923, Val MAPE: 6.205%\n",
      "Epoch 310: Train Loss: 0.001755, Train MAPE: 4.212%, Val Loss: 0.009436, Val MAPE: 5.140%\n",
      "Epoch 311: Train Loss: 0.001391, Train MAPE: 3.237%, Val Loss: 0.007676, Val MAPE: 3.846%\n",
      "Epoch 312: Train Loss: 0.002268, Train MAPE: 4.448%, Val Loss: 0.008340, Val MAPE: 4.973%\n",
      "Epoch 313: Train Loss: 0.003727, Train MAPE: 4.166%, Val Loss: 0.009051, Val MAPE: 4.810%\n",
      "Epoch 314: Train Loss: 0.001984, Train MAPE: 5.482%, Val Loss: 0.008013, Val MAPE: 6.423%\n",
      "Epoch 315: Train Loss: 0.001674, Train MAPE: 4.685%, Val Loss: 0.009213, Val MAPE: 5.556%\n",
      "Epoch 316: Train Loss: 0.001931, Train MAPE: 4.129%, Val Loss: 0.008408, Val MAPE: 4.604%\n",
      "Epoch 317: Train Loss: 0.001519, Train MAPE: 3.271%, Val Loss: 0.008189, Val MAPE: 3.771%\n",
      "Epoch 318: Train Loss: 0.001759, Train MAPE: 4.201%, Val Loss: 0.008534, Val MAPE: 5.025%\n",
      "Epoch 319: Train Loss: 0.004464, Train MAPE: 3.682%, Val Loss: 0.011517, Val MAPE: 4.190%\n",
      "Epoch 320: Train Loss: 0.005453, Train MAPE: 5.825%, Val Loss: 0.011478, Val MAPE: 6.551%\n",
      "Epoch 321: Train Loss: 0.001234, Train MAPE: 3.045%, Val Loss: 0.007402, Val MAPE: 3.626%\n",
      "Epoch 322: Train Loss: 0.001733, Train MAPE: 3.385%, Val Loss: 0.009108, Val MAPE: 3.950%\n",
      "Epoch 323: Train Loss: 0.001693, Train MAPE: 5.262%, Val Loss: 0.008245, Val MAPE: 6.089%\n",
      "Epoch 324: Train Loss: 0.001288, Train MAPE: 2.776%, Val Loss: 0.007505, Val MAPE: 3.283%\n",
      "Epoch 325: Train Loss: 0.001422, Train MAPE: 3.420%, Val Loss: 0.007698, Val MAPE: 4.061%\n",
      "Epoch 326: Train Loss: 0.001552, Train MAPE: 3.501%, Val Loss: 0.007706, Val MAPE: 3.956%\n",
      "Epoch 327: Train Loss: 0.001354, Train MAPE: 3.462%, Val Loss: 0.007449, Val MAPE: 4.116%\n",
      "Epoch 328: Train Loss: 0.001446, Train MAPE: 3.170%, Val Loss: 0.030303, Val MAPE: 3.764%\n",
      "Epoch 329: Train Loss: 0.001853, Train MAPE: 3.242%, Val Loss: 0.008479, Val MAPE: 4.020%\n",
      "Epoch 330: Train Loss: 0.001869, Train MAPE: 4.058%, Val Loss: 0.008103, Val MAPE: 4.712%\n",
      "Epoch 331: Train Loss: 0.001834, Train MAPE: 2.680%, Val Loss: 0.008752, Val MAPE: 3.398%\n",
      "Epoch 332: Train Loss: 0.001845, Train MAPE: 4.395%, Val Loss: 0.008402, Val MAPE: 5.210%\n",
      "Epoch 333: Train Loss: 0.001737, Train MAPE: 4.826%, Val Loss: 0.008664, Val MAPE: 5.532%\n",
      "Epoch 334: Train Loss: 0.001394, Train MAPE: 3.781%, Val Loss: 0.008058, Val MAPE: 4.542%\n",
      "Epoch 335: Train Loss: 0.002115, Train MAPE: 3.290%, Val Loss: 0.008580, Val MAPE: 3.898%\n",
      "Epoch 336: Train Loss: 0.002731, Train MAPE: 3.476%, Val Loss: 0.009723, Val MAPE: 4.053%\n",
      "Epoch 337: Train Loss: 0.001864, Train MAPE: 4.519%, Val Loss: 0.009025, Val MAPE: 5.293%\n",
      "Epoch 338: Train Loss: 0.001258, Train MAPE: 2.898%, Val Loss: 0.007444, Val MAPE: 3.395%\n",
      "Epoch 339: Train Loss: 0.006054, Train MAPE: 6.064%, Val Loss: 0.013732, Val MAPE: 6.645%\n",
      "Epoch 340: Train Loss: 0.002693, Train MAPE: 5.163%, Val Loss: 0.008635, Val MAPE: 6.081%\n",
      "Epoch 341: Train Loss: 0.001909, Train MAPE: 3.804%, Val Loss: 0.008432, Val MAPE: 5.133%\n",
      "Epoch 342: Train Loss: 0.001821, Train MAPE: 3.787%, Val Loss: 0.008485, Val MAPE: 4.406%\n",
      "Epoch 343: Train Loss: 0.001105, Train MAPE: 2.953%, Val Loss: 0.007984, Val MAPE: 3.455%\n",
      "Epoch 344: Train Loss: 0.001264, Train MAPE: 4.157%, Val Loss: 0.007747, Val MAPE: 4.804%\n",
      "Epoch 345: Train Loss: 0.001657, Train MAPE: 3.928%, Val Loss: 0.007844, Val MAPE: 4.574%\n",
      "Epoch 346: Train Loss: 0.001440, Train MAPE: 3.868%, Val Loss: 0.008188, Val MAPE: 4.562%\n",
      "Epoch 347: Train Loss: 0.001549, Train MAPE: 3.447%, Val Loss: 0.008558, Val MAPE: 4.071%\n",
      "Epoch 348: Train Loss: 0.001507, Train MAPE: 2.878%, Val Loss: 0.008466, Val MAPE: 3.734%\n",
      "Epoch 349: Train Loss: 0.002088, Train MAPE: 4.664%, Val Loss: 0.009052, Val MAPE: 5.314%\n",
      "Epoch 350: Train Loss: 0.001923, Train MAPE: 5.003%, Val Loss: 0.008440, Val MAPE: 6.395%\n",
      "Epoch 351: Train Loss: 0.001448, Train MAPE: 3.501%, Val Loss: 0.008240, Val MAPE: 4.027%\n",
      "Epoch 352: Train Loss: 0.001423, Train MAPE: 4.177%, Val Loss: 0.008124, Val MAPE: 4.965%\n",
      "Epoch 353: Train Loss: 0.001692, Train MAPE: 4.381%, Val Loss: 0.008582, Val MAPE: 5.076%\n",
      "Epoch 354: Train Loss: 0.002196, Train MAPE: 3.973%, Val Loss: 0.009875, Val MAPE: 4.619%\n",
      "Epoch 355: Train Loss: 0.002040, Train MAPE: 4.470%, Val Loss: 0.009257, Val MAPE: 5.124%\n",
      "Epoch 356: Train Loss: 0.001785, Train MAPE: 4.511%, Val Loss: 0.008822, Val MAPE: 5.172%\n",
      "Epoch 357: Train Loss: 0.001528, Train MAPE: 3.517%, Val Loss: 0.008902, Val MAPE: 4.127%\n",
      "Epoch 358: Train Loss: 0.002755, Train MAPE: 4.834%, Val Loss: 0.010120, Val MAPE: 5.536%\n",
      "Epoch 359: Train Loss: 0.002504, Train MAPE: 4.880%, Val Loss: 0.008773, Val MAPE: 5.589%\n",
      "Epoch 360: Train Loss: 0.001793, Train MAPE: 3.147%, Val Loss: 0.008573, Val MAPE: 3.702%\n",
      "Epoch 361: Train Loss: 0.002134, Train MAPE: 4.165%, Val Loss: 0.009355, Val MAPE: 4.920%\n",
      "Epoch 362: Train Loss: 0.001716, Train MAPE: 3.244%, Val Loss: 0.008514, Val MAPE: 3.808%\n",
      "Epoch 363: Train Loss: 0.001372, Train MAPE: 3.554%, Val Loss: 0.008037, Val MAPE: 4.173%\n",
      "Epoch 364: Train Loss: 0.001367, Train MAPE: 3.476%, Val Loss: 0.007935, Val MAPE: 4.100%\n",
      "Epoch 365: Train Loss: 0.001510, Train MAPE: 3.409%, Val Loss: 0.008043, Val MAPE: 4.177%\n",
      "Epoch 366: Train Loss: 0.001473, Train MAPE: 4.543%, Val Loss: 0.007995, Val MAPE: 5.183%\n",
      "Epoch 367: Train Loss: 0.001833, Train MAPE: 4.433%, Val Loss: 0.007857, Val MAPE: 5.095%\n",
      "Epoch 368: Train Loss: 0.002210, Train MAPE: 3.703%, Val Loss: 0.008439, Val MAPE: 4.219%\n",
      "Epoch 369: Train Loss: 0.001730, Train MAPE: 3.533%, Val Loss: 0.008293, Val MAPE: 4.072%\n",
      "Epoch 370: Train Loss: 0.001577, Train MAPE: 3.615%, Val Loss: 0.008387, Val MAPE: 4.267%\n",
      "Epoch 371: Train Loss: 0.001685, Train MAPE: 4.630%, Val Loss: 0.007159, Val MAPE: 5.390%\n",
      "Epoch 372: Train Loss: 0.009941, Train MAPE: 5.996%, Val Loss: 0.017453, Val MAPE: 6.567%\n",
      "Epoch 373: Train Loss: 0.004264, Train MAPE: 3.702%, Val Loss: 0.011673, Val MAPE: 4.326%\n",
      "Epoch 374: Train Loss: 0.002282, Train MAPE: 3.780%, Val Loss: 0.008002, Val MAPE: 4.355%\n",
      "Epoch 375: Train Loss: 0.001322, Train MAPE: 3.872%, Val Loss: 0.008363, Val MAPE: 4.460%\n",
      "Epoch 376: Train Loss: 0.001540, Train MAPE: 3.539%, Val Loss: 0.008143, Val MAPE: 4.100%\n",
      "Epoch 377: Train Loss: 0.001232, Train MAPE: 3.199%, Val Loss: 0.007773, Val MAPE: 3.586%\n",
      "Epoch 378: Train Loss: 0.001337, Train MAPE: 2.718%, Val Loss: 0.008700, Val MAPE: 3.324%\n",
      "Epoch 379: Train Loss: 0.001505, Train MAPE: 4.329%, Val Loss: 0.007972, Val MAPE: 5.065%\n",
      "Epoch 380: Train Loss: 0.001573, Train MAPE: 3.916%, Val Loss: 0.008610, Val MAPE: 4.659%\n",
      "Epoch 381: Train Loss: 0.001635, Train MAPE: 4.829%, Val Loss: 0.008626, Val MAPE: 5.922%\n",
      "Epoch 382: Train Loss: 0.001097, Train MAPE: 3.858%, Val Loss: 0.007876, Val MAPE: 5.158%\n",
      "Epoch 383: Train Loss: 0.001390, Train MAPE: 3.080%, Val Loss: 0.009304, Val MAPE: 3.595%\n",
      "Epoch 384: Train Loss: 0.002468, Train MAPE: 4.636%, Val Loss: 0.009537, Val MAPE: 6.151%\n",
      "Epoch 385: Train Loss: 0.001142, Train MAPE: 2.991%, Val Loss: 0.008118, Val MAPE: 3.454%\n",
      "Epoch 386: Train Loss: 0.001565, Train MAPE: 4.056%, Val Loss: 0.008717, Val MAPE: 4.999%\n",
      "Epoch 387: Train Loss: 0.002142, Train MAPE: 3.747%, Val Loss: 0.008882, Val MAPE: 4.387%\n",
      "Epoch 388: Train Loss: 0.001447, Train MAPE: 2.791%, Val Loss: 0.008718, Val MAPE: 3.494%\n",
      "Epoch 389: Train Loss: 0.002298, Train MAPE: 3.485%, Val Loss: 0.008197, Val MAPE: 4.203%\n",
      "Epoch 390: Train Loss: 0.001829, Train MAPE: 3.925%, Val Loss: 0.007934, Val MAPE: 4.494%\n",
      "Epoch 391: Train Loss: 0.001471, Train MAPE: 3.851%, Val Loss: 0.008746, Val MAPE: 4.656%\n",
      "Epoch 392: Train Loss: 0.001829, Train MAPE: 4.355%, Val Loss: 0.008211, Val MAPE: 4.956%\n",
      "Epoch 393: Train Loss: 0.007588, Train MAPE: 6.550%, Val Loss: 0.014334, Val MAPE: 7.609%\n",
      "Epoch 394: Train Loss: 0.011409, Train MAPE: 9.395%, Val Loss: 0.016059, Val MAPE: 10.650%\n",
      "Epoch 395: Train Loss: 0.003389, Train MAPE: 4.600%, Val Loss: 0.009060, Val MAPE: 5.199%\n",
      "Epoch 396: Train Loss: 0.001288, Train MAPE: 3.431%, Val Loss: 0.007184, Val MAPE: 3.989%\n",
      "Epoch 397: Train Loss: 0.001435, Train MAPE: 4.236%, Val Loss: 0.007303, Val MAPE: 4.850%\n",
      "Epoch 398: Train Loss: 0.001448, Train MAPE: 3.139%, Val Loss: 0.007987, Val MAPE: 3.856%\n",
      "Epoch 399: Train Loss: 0.001602, Train MAPE: 4.464%, Val Loss: 0.007302, Val MAPE: 5.380%\n",
      "Epoch 400: Train Loss: 0.001499, Train MAPE: 3.760%, Val Loss: 0.007802, Val MAPE: 4.337%\n",
      "Epoch 401: Train Loss: 0.001306, Train MAPE: 3.345%, Val Loss: 0.007887, Val MAPE: 3.980%\n",
      "Epoch 402: Train Loss: 0.001337, Train MAPE: 4.232%, Val Loss: 0.007730, Val MAPE: 5.201%\n",
      "Epoch 403: Train Loss: 0.001308, Train MAPE: 3.278%, Val Loss: 0.008042, Val MAPE: 3.970%\n",
      "Epoch 404: Train Loss: 0.001438, Train MAPE: 3.475%, Val Loss: 0.008444, Val MAPE: 4.097%\n",
      "Epoch 405: Train Loss: 0.001193, Train MAPE: 4.081%, Val Loss: 0.008200, Val MAPE: 4.836%\n",
      "Epoch 406: Train Loss: 0.003459, Train MAPE: 5.654%, Val Loss: 0.012133, Val MAPE: 6.395%\n",
      "Epoch 407: Train Loss: 0.001676, Train MAPE: 4.034%, Val Loss: 0.008655, Val MAPE: 4.783%\n",
      "Epoch 408: Train Loss: 0.003486, Train MAPE: 5.657%, Val Loss: 0.010001, Val MAPE: 6.713%\n",
      "Epoch 409: Train Loss: 0.001664, Train MAPE: 3.129%, Val Loss: 0.008051, Val MAPE: 3.608%\n",
      "Epoch 410: Train Loss: 0.001823, Train MAPE: 3.921%, Val Loss: 0.009361, Val MAPE: 4.700%\n",
      "Epoch 411: Train Loss: 0.002542, Train MAPE: 3.683%, Val Loss: 0.009280, Val MAPE: 4.311%\n",
      "Epoch 412: Train Loss: 0.001831, Train MAPE: 3.952%, Val Loss: 0.009567, Val MAPE: 4.562%\n",
      "Epoch 413: Train Loss: 0.001647, Train MAPE: 4.182%, Val Loss: 0.008375, Val MAPE: 4.783%\n",
      "Epoch 414: Train Loss: 0.001821, Train MAPE: 4.273%, Val Loss: 0.008300, Val MAPE: 5.047%\n",
      "Epoch 415: Train Loss: 0.002046, Train MAPE: 3.519%, Val Loss: 0.009304, Val MAPE: 4.205%\n",
      "Epoch 416: Train Loss: 0.001292, Train MAPE: 3.341%, Val Loss: 0.008683, Val MAPE: 3.905%\n",
      "Epoch 417: Train Loss: 0.002386, Train MAPE: 4.659%, Val Loss: 0.009722, Val MAPE: 5.457%\n",
      "Epoch 418: Train Loss: 0.001748, Train MAPE: 2.776%, Val Loss: 0.009015, Val MAPE: 3.344%\n",
      "Epoch 419: Train Loss: 0.001836, Train MAPE: 3.302%, Val Loss: 0.009421, Val MAPE: 4.032%\n",
      "Epoch 420: Train Loss: 0.002905, Train MAPE: 4.079%, Val Loss: 0.011619, Val MAPE: 4.717%\n",
      "Epoch 421: Train Loss: 0.001862, Train MAPE: 3.289%, Val Loss: 0.007590, Val MAPE: 3.851%\n",
      "Epoch 422: Train Loss: 0.001439, Train MAPE: 3.593%, Val Loss: 0.008100, Val MAPE: 4.158%\n",
      "Epoch 423: Train Loss: 0.001533, Train MAPE: 2.836%, Val Loss: 0.007561, Val MAPE: 3.371%\n",
      "Epoch 424: Train Loss: 0.001919, Train MAPE: 5.381%, Val Loss: 0.008141, Val MAPE: 6.241%\n",
      "Epoch 425: Train Loss: 0.002207, Train MAPE: 3.812%, Val Loss: 0.009497, Val MAPE: 4.778%\n",
      "Epoch 426: Train Loss: 0.001935, Train MAPE: 5.267%, Val Loss: 0.008352, Val MAPE: 5.918%\n",
      "Epoch 427: Train Loss: 0.002697, Train MAPE: 4.505%, Val Loss: 0.009488, Val MAPE: 5.591%\n",
      "Epoch 428: Train Loss: 0.002012, Train MAPE: 4.379%, Val Loss: 0.008554, Val MAPE: 4.913%\n",
      "Epoch 429: Train Loss: 0.003131, Train MAPE: 4.523%, Val Loss: 0.011243, Val MAPE: 5.151%\n",
      "Epoch 430: Train Loss: 0.001950, Train MAPE: 5.464%, Val Loss: 0.009744, Val MAPE: 6.383%\n",
      "Epoch 431: Train Loss: 0.001889, Train MAPE: 4.249%, Val Loss: 0.009119, Val MAPE: 4.809%\n",
      "Epoch 432: Train Loss: 0.002094, Train MAPE: 3.768%, Val Loss: 0.007509, Val MAPE: 4.346%\n",
      "Epoch 433: Train Loss: 0.001280, Train MAPE: 2.958%, Val Loss: 0.007926, Val MAPE: 3.468%\n",
      "Epoch 434: Train Loss: 0.001788, Train MAPE: 3.966%, Val Loss: 0.009578, Val MAPE: 4.616%\n",
      "Epoch 435: Train Loss: 0.001684, Train MAPE: 4.641%, Val Loss: 0.008166, Val MAPE: 6.581%\n",
      "Epoch 436: Train Loss: 0.001685, Train MAPE: 3.113%, Val Loss: 0.010059, Val MAPE: 3.739%\n",
      "Epoch 437: Train Loss: 0.001615, Train MAPE: 2.719%, Val Loss: 0.009130, Val MAPE: 3.372%\n",
      "Epoch 438: Train Loss: 0.017999, Train MAPE: 4.715%, Val Loss: 0.015596, Val MAPE: 5.179%\n",
      "Epoch 439: Train Loss: 0.002960, Train MAPE: 4.992%, Val Loss: 0.010064, Val MAPE: 6.062%\n",
      "Epoch 440: Train Loss: 0.002257, Train MAPE: 5.987%, Val Loss: 0.008991, Val MAPE: 6.720%\n",
      "Epoch 441: Train Loss: 0.001135, Train MAPE: 3.083%, Val Loss: 0.009588, Val MAPE: 3.721%\n",
      "Epoch 442: Train Loss: 0.001291, Train MAPE: 3.248%, Val Loss: 0.008795, Val MAPE: 3.811%\n",
      "Epoch 443: Train Loss: 0.001594, Train MAPE: 3.815%, Val Loss: 0.008564, Val MAPE: 4.502%\n",
      "Epoch 444: Train Loss: 0.001221, Train MAPE: 3.460%, Val Loss: 0.008725, Val MAPE: 4.056%\n",
      "Epoch 445: Train Loss: 0.001606, Train MAPE: 3.862%, Val Loss: 0.008832, Val MAPE: 4.634%\n",
      "Epoch 446: Train Loss: 0.001629, Train MAPE: 3.303%, Val Loss: 0.008920, Val MAPE: 3.856%\n",
      "Epoch 447: Train Loss: 0.001562, Train MAPE: 4.553%, Val Loss: 0.009079, Val MAPE: 5.171%\n",
      "Epoch 448: Train Loss: 0.002136, Train MAPE: 4.679%, Val Loss: 0.010275, Val MAPE: 5.548%\n",
      "Epoch 449: Train Loss: 0.001671, Train MAPE: 3.358%, Val Loss: 0.008783, Val MAPE: 4.092%\n",
      "Epoch 450: Train Loss: 0.001474, Train MAPE: 3.239%, Val Loss: 0.008879, Val MAPE: 3.983%\n",
      "Epoch 451: Train Loss: 0.002074, Train MAPE: 4.766%, Val Loss: 0.010526, Val MAPE: 5.818%\n",
      "Epoch 452: Train Loss: 0.001428, Train MAPE: 3.402%, Val Loss: 0.007634, Val MAPE: 4.031%\n",
      "Epoch 453: Train Loss: 0.001720, Train MAPE: 3.664%, Val Loss: 0.008909, Val MAPE: 4.275%\n",
      "Epoch 454: Train Loss: 0.002263, Train MAPE: 4.267%, Val Loss: 0.011419, Val MAPE: 5.062%\n",
      "Epoch 455: Train Loss: 0.001848, Train MAPE: 5.767%, Val Loss: 0.009716, Val MAPE: 6.634%\n",
      "Epoch 456: Train Loss: 0.001797, Train MAPE: 3.649%, Val Loss: 0.008631, Val MAPE: 4.043%\n",
      "Epoch 457: Train Loss: 0.001424, Train MAPE: 3.790%, Val Loss: 0.008280, Val MAPE: 6.258%\n",
      "Epoch 458: Train Loss: 0.001354, Train MAPE: 3.989%, Val Loss: 0.008131, Val MAPE: 5.067%\n",
      "Epoch 459: Train Loss: 0.001368, Train MAPE: 4.115%, Val Loss: 0.008763, Val MAPE: 5.287%\n",
      "Epoch 460: Train Loss: 0.001503, Train MAPE: 3.202%, Val Loss: 0.008087, Val MAPE: 3.734%\n",
      "Epoch 461: Train Loss: 0.002380, Train MAPE: 4.677%, Val Loss: 0.009943, Val MAPE: 5.314%\n",
      "Epoch 462: Train Loss: 0.001470, Train MAPE: 2.225%, Val Loss: 0.009275, Val MAPE: 2.697%\n",
      "Epoch 463: Train Loss: 0.001732, Train MAPE: 3.519%, Val Loss: 0.010716, Val MAPE: 4.209%\n",
      "Epoch 464: Train Loss: 0.003613, Train MAPE: 5.289%, Val Loss: 0.024333, Val MAPE: 6.170%\n",
      "Epoch 465: Train Loss: 0.001143, Train MAPE: 3.901%, Val Loss: 0.008313, Val MAPE: 4.731%\n",
      "Epoch 466: Train Loss: 0.001464, Train MAPE: 3.255%, Val Loss: 0.009097, Val MAPE: 3.980%\n",
      "Epoch 467: Train Loss: 0.001572, Train MAPE: 4.712%, Val Loss: 0.008727, Val MAPE: 5.454%\n",
      "Epoch 468: Train Loss: 0.001499, Train MAPE: 3.021%, Val Loss: 0.009224, Val MAPE: 3.557%\n",
      "Epoch 469: Train Loss: 0.001981, Train MAPE: 2.526%, Val Loss: 0.010130, Val MAPE: 2.987%\n",
      "Epoch 470: Train Loss: 0.002025, Train MAPE: 4.446%, Val Loss: 0.010598, Val MAPE: 5.285%\n",
      "Epoch 471: Train Loss: 0.001557, Train MAPE: 4.659%, Val Loss: 0.009097, Val MAPE: 5.517%\n",
      "Epoch 472: Train Loss: 0.003912, Train MAPE: 4.436%, Val Loss: 0.013594, Val MAPE: 4.998%\n",
      "Epoch 473: Train Loss: 0.002927, Train MAPE: 5.227%, Val Loss: 0.011131, Val MAPE: 6.319%\n",
      "Epoch 474: Train Loss: 0.001658, Train MAPE: 4.577%, Val Loss: 0.008811, Val MAPE: 5.309%\n",
      "Epoch 475: Train Loss: 0.001966, Train MAPE: 5.197%, Val Loss: 0.009664, Val MAPE: 6.211%\n",
      "Epoch 476: Train Loss: 0.001439, Train MAPE: 2.969%, Val Loss: 0.008754, Val MAPE: 3.511%\n",
      "Epoch 477: Train Loss: 0.001767, Train MAPE: 4.621%, Val Loss: 0.009699, Val MAPE: 5.313%\n",
      "Epoch 478: Train Loss: 0.002103, Train MAPE: 5.109%, Val Loss: 0.010582, Val MAPE: 5.886%\n",
      "Epoch 479: Train Loss: 0.001425, Train MAPE: 4.190%, Val Loss: 0.009600, Val MAPE: 4.937%\n",
      "Epoch 480: Train Loss: 0.001184, Train MAPE: 3.740%, Val Loss: 0.009047, Val MAPE: 4.665%\n",
      "Epoch 481: Train Loss: 0.001511, Train MAPE: 4.235%, Val Loss: 0.010114, Val MAPE: 5.414%\n",
      "Epoch 482: Train Loss: 0.001720, Train MAPE: 4.427%, Val Loss: 0.008812, Val MAPE: 5.050%\n",
      "Epoch 483: Train Loss: 0.001422, Train MAPE: 3.998%, Val Loss: 0.010037, Val MAPE: 4.744%\n",
      "Epoch 484: Train Loss: 0.001296, Train MAPE: 3.369%, Val Loss: 0.008096, Val MAPE: 3.882%\n",
      "Epoch 485: Train Loss: 0.001250, Train MAPE: 3.590%, Val Loss: 0.007858, Val MAPE: 4.253%\n",
      "Epoch 486: Train Loss: 0.001886, Train MAPE: 4.510%, Val Loss: 0.010469, Val MAPE: 5.371%\n",
      "Epoch 487: Train Loss: 0.001952, Train MAPE: 5.330%, Val Loss: 0.009620, Val MAPE: 6.191%\n",
      "Epoch 488: Train Loss: 0.002129, Train MAPE: 3.961%, Val Loss: 0.009832, Val MAPE: 4.773%\n",
      "Epoch 489: Train Loss: 0.001721, Train MAPE: 2.757%, Val Loss: 0.009883, Val MAPE: 3.330%\n",
      "Epoch 490: Train Loss: 0.001762, Train MAPE: 3.646%, Val Loss: 0.009695, Val MAPE: 4.453%\n",
      "Epoch 491: Train Loss: 0.001541, Train MAPE: 4.526%, Val Loss: 0.008765, Val MAPE: 5.264%\n",
      "Epoch 492: Train Loss: 0.001285, Train MAPE: 2.930%, Val Loss: 0.008147, Val MAPE: 3.569%\n",
      "Epoch 493: Train Loss: 0.001614, Train MAPE: 3.199%, Val Loss: 0.009288, Val MAPE: 4.017%\n",
      "Epoch 494: Train Loss: 0.001285, Train MAPE: 2.821%, Val Loss: 0.008700, Val MAPE: 3.481%\n",
      "Epoch 495: Train Loss: 0.001889, Train MAPE: 3.172%, Val Loss: 0.009310, Val MAPE: 3.777%\n",
      "Epoch 496: Train Loss: 0.001611, Train MAPE: 3.753%, Val Loss: 0.009305, Val MAPE: 4.208%\n",
      "Epoch 497: Train Loss: 0.001526, Train MAPE: 4.760%, Val Loss: 0.009094, Val MAPE: 5.831%\n",
      "Epoch 498: Train Loss: 0.001942, Train MAPE: 5.057%, Val Loss: 0.009659, Val MAPE: 5.665%\n",
      "Epoch 499: Train Loss: 0.001317, Train MAPE: 3.280%, Val Loss: 0.008637, Val MAPE: 3.974%\n",
      "Epoch 500: Train Loss: 0.001416, Train MAPE: 3.723%, Val Loss: 0.008926, Val MAPE: 4.349%\n",
      "Epoch 501: Train Loss: 0.002182, Train MAPE: 5.352%, Val Loss: 0.013201, Val MAPE: 6.425%\n",
      "Epoch 502: Train Loss: 0.001859, Train MAPE: 4.540%, Val Loss: 0.009411, Val MAPE: 5.409%\n",
      "Epoch 503: Train Loss: 0.001333, Train MAPE: 3.025%, Val Loss: 0.008836, Val MAPE: 3.847%\n",
      "Epoch 504: Train Loss: 0.001791, Train MAPE: 4.668%, Val Loss: 0.008411, Val MAPE: 5.504%\n",
      "Epoch 505: Train Loss: 0.006312, Train MAPE: 7.323%, Val Loss: 0.014542, Val MAPE: 8.299%\n",
      "Epoch 506: Train Loss: 0.013451, Train MAPE: 8.412%, Val Loss: 0.021403, Val MAPE: 9.307%\n",
      "Epoch 507: Train Loss: 0.001842, Train MAPE: 4.453%, Val Loss: 0.008613, Val MAPE: 5.071%\n",
      "Epoch 508: Train Loss: 0.001815, Train MAPE: 4.793%, Val Loss: 0.008124, Val MAPE: 5.895%\n",
      "Epoch 509: Train Loss: 0.001529, Train MAPE: 4.519%, Val Loss: 0.008746, Val MAPE: 5.155%\n",
      "Epoch 510: Train Loss: 0.001392, Train MAPE: 4.652%, Val Loss: 0.008439, Val MAPE: 5.361%\n",
      "Epoch 511: Train Loss: 0.001501, Train MAPE: 4.281%, Val Loss: 0.008289, Val MAPE: 4.868%\n",
      "Epoch 512: Train Loss: 0.001311, Train MAPE: 4.104%, Val Loss: 0.008471, Val MAPE: 4.627%\n",
      "Epoch 513: Train Loss: 0.001157, Train MAPE: 4.243%, Val Loss: 0.008226, Val MAPE: 5.178%\n",
      "Epoch 514: Train Loss: 0.001320, Train MAPE: 4.349%, Val Loss: 0.008137, Val MAPE: 5.102%\n",
      "Epoch 515: Train Loss: 0.001052, Train MAPE: 3.093%, Val Loss: 0.008342, Val MAPE: 3.630%\n",
      "Epoch 516: Train Loss: 0.001418, Train MAPE: 4.677%, Val Loss: 0.008386, Val MAPE: 5.359%\n",
      "Epoch 517: Train Loss: 0.001623, Train MAPE: 3.742%, Val Loss: 0.008649, Val MAPE: 4.306%\n",
      "Epoch 518: Train Loss: 0.001176, Train MAPE: 4.572%, Val Loss: 0.008192, Val MAPE: 5.387%\n",
      "Epoch 519: Train Loss: 0.001695, Train MAPE: 3.734%, Val Loss: 0.008940, Val MAPE: 4.466%\n",
      "Epoch 520: Train Loss: 0.001136, Train MAPE: 3.996%, Val Loss: 0.008184, Val MAPE: 4.759%\n",
      "Epoch 521: Train Loss: 0.001785, Train MAPE: 4.657%, Val Loss: 0.009570, Val MAPE: 5.207%\n",
      "Epoch 522: Train Loss: 0.001176, Train MAPE: 4.056%, Val Loss: 0.007926, Val MAPE: 5.335%\n",
      "Epoch 523: Train Loss: 0.001518, Train MAPE: 3.732%, Val Loss: 0.009397, Val MAPE: 4.283%\n",
      "Epoch 524: Train Loss: 0.002007, Train MAPE: 3.311%, Val Loss: 0.009227, Val MAPE: 3.818%\n",
      "Epoch 525: Train Loss: 0.002181, Train MAPE: 5.000%, Val Loss: 0.009931, Val MAPE: 5.883%\n",
      "Epoch 526: Train Loss: 0.001456, Train MAPE: 3.789%, Val Loss: 0.009130, Val MAPE: 4.406%\n",
      "Epoch 527: Train Loss: 0.001399, Train MAPE: 3.184%, Val Loss: 0.008347, Val MAPE: 3.781%\n",
      "Epoch 528: Train Loss: 0.001681, Train MAPE: 3.599%, Val Loss: 0.009369, Val MAPE: 4.134%\n",
      "Epoch 529: Train Loss: 0.001938, Train MAPE: 4.265%, Val Loss: 0.008975, Val MAPE: 4.986%\n",
      "Epoch 530: Train Loss: 0.001425, Train MAPE: 3.787%, Val Loss: 0.008545, Val MAPE: 4.403%\n",
      "Epoch 531: Train Loss: 0.001334, Train MAPE: 3.539%, Val Loss: 0.008436, Val MAPE: 4.117%\n",
      "Epoch 532: Train Loss: 0.001356, Train MAPE: 3.323%, Val Loss: 0.007939, Val MAPE: 4.133%\n",
      "Epoch 533: Train Loss: 0.001629, Train MAPE: 4.049%, Val Loss: 0.008969, Val MAPE: 4.738%\n",
      "Epoch 534: Train Loss: 0.001365, Train MAPE: 4.057%, Val Loss: 0.009577, Val MAPE: 5.039%\n",
      "Epoch 535: Train Loss: 0.001752, Train MAPE: 4.182%, Val Loss: 0.008535, Val MAPE: 5.050%\n",
      "Epoch 536: Train Loss: 0.010064, Train MAPE: 4.478%, Val Loss: 0.014319, Val MAPE: 5.586%\n",
      "Epoch 537: Train Loss: 0.003155, Train MAPE: 4.789%, Val Loss: 0.009177, Val MAPE: 5.877%\n",
      "Epoch 538: Train Loss: 0.001965, Train MAPE: 4.099%, Val Loss: 0.008606, Val MAPE: 5.264%\n",
      "Epoch 539: Train Loss: 0.001830, Train MAPE: 4.119%, Val Loss: 0.008644, Val MAPE: 5.141%\n",
      "Epoch 540: Train Loss: 0.001227, Train MAPE: 3.084%, Val Loss: 0.008972, Val MAPE: 3.889%\n",
      "Epoch 541: Train Loss: 0.001467, Train MAPE: 4.480%, Val Loss: 0.008692, Val MAPE: 5.268%\n",
      "Epoch 542: Train Loss: 0.001440, Train MAPE: 3.845%, Val Loss: 0.008790, Val MAPE: 4.538%\n",
      "Epoch 543: Train Loss: 0.001377, Train MAPE: 3.898%, Val Loss: 0.008857, Val MAPE: 4.451%\n",
      "Epoch 544: Train Loss: 0.001530, Train MAPE: 4.478%, Val Loss: 0.008872, Val MAPE: 5.353%\n",
      "Epoch 545: Train Loss: 0.001333, Train MAPE: 4.223%, Val Loss: 0.009179, Val MAPE: 5.083%\n",
      "Epoch 546: Train Loss: 0.001095, Train MAPE: 3.894%, Val Loss: 0.008600, Val MAPE: 4.545%\n",
      "Epoch 547: Train Loss: 0.000938, Train MAPE: 3.241%, Val Loss: 0.008483, Val MAPE: 3.997%\n",
      "Epoch 548: Train Loss: 0.001891, Train MAPE: 4.974%, Val Loss: 0.009275, Val MAPE: 5.732%\n",
      "Epoch 549: Train Loss: 0.001367, Train MAPE: 3.728%, Val Loss: 0.009038, Val MAPE: 4.561%\n",
      "Epoch 550: Train Loss: 0.002311, Train MAPE: 5.056%, Val Loss: 0.010972, Val MAPE: 5.843%\n",
      "Epoch 551: Train Loss: 0.001658, Train MAPE: 5.217%, Val Loss: 0.008842, Val MAPE: 6.037%\n",
      "Epoch 552: Train Loss: 0.001262, Train MAPE: 3.773%, Val Loss: 0.010923, Val MAPE: 4.391%\n",
      "Epoch 553: Train Loss: 0.001870, Train MAPE: 4.370%, Val Loss: 0.008999, Val MAPE: 5.183%\n",
      "Epoch 554: Train Loss: 0.001292, Train MAPE: 4.757%, Val Loss: 0.008546, Val MAPE: 5.720%\n",
      "Epoch 555: Train Loss: 0.001221, Train MAPE: 4.281%, Val Loss: 0.008165, Val MAPE: 4.917%\n",
      "Epoch 556: Train Loss: 0.001437, Train MAPE: 3.894%, Val Loss: 0.008459, Val MAPE: 4.688%\n",
      "Epoch 557: Train Loss: 0.001443, Train MAPE: 3.851%, Val Loss: 0.008594, Val MAPE: 4.484%\n",
      "Epoch 558: Train Loss: 0.002766, Train MAPE: 6.754%, Val Loss: 0.009685, Val MAPE: 7.880%\n",
      "Epoch 559: Train Loss: 0.002657, Train MAPE: 5.055%, Val Loss: 0.010985, Val MAPE: 5.818%\n",
      "Epoch 560: Train Loss: 0.002059, Train MAPE: 4.945%, Val Loss: 0.009607, Val MAPE: 5.740%\n",
      "Epoch 561: Train Loss: 0.001327, Train MAPE: 3.762%, Val Loss: 0.008562, Val MAPE: 4.450%\n",
      "Epoch 562: Train Loss: 0.002044, Train MAPE: 5.502%, Val Loss: 0.009489, Val MAPE: 6.392%\n",
      "Epoch 563: Train Loss: 0.001476, Train MAPE: 4.313%, Val Loss: 0.009436, Val MAPE: 4.981%\n",
      "Epoch 564: Train Loss: 0.002015, Train MAPE: 4.647%, Val Loss: 0.009710, Val MAPE: 5.470%\n",
      "Epoch 565: Train Loss: 0.002672, Train MAPE: 5.818%, Val Loss: 0.010154, Val MAPE: 6.516%\n",
      "Epoch 566: Train Loss: 0.002332, Train MAPE: 4.174%, Val Loss: 0.010284, Val MAPE: 4.937%\n",
      "Epoch 567: Train Loss: 0.005027, Train MAPE: 6.217%, Val Loss: 0.013064, Val MAPE: 7.262%\n",
      "Epoch 568: Train Loss: 0.009332, Train MAPE: 6.519%, Val Loss: 0.016241, Val MAPE: 6.980%\n",
      "Epoch 569: Train Loss: 0.001493, Train MAPE: 3.898%, Val Loss: 0.008486, Val MAPE: 4.708%\n",
      "Epoch 570: Train Loss: 0.001342, Train MAPE: 3.558%, Val Loss: 0.008993, Val MAPE: 4.156%\n",
      "Epoch 571: Train Loss: 0.001552, Train MAPE: 4.706%, Val Loss: 0.009468, Val MAPE: 5.653%\n",
      "Epoch 572: Train Loss: 0.000924, Train MAPE: 3.290%, Val Loss: 0.008321, Val MAPE: 4.026%\n",
      "Epoch 573: Train Loss: 0.001251, Train MAPE: 2.826%, Val Loss: 0.008869, Val MAPE: 3.473%\n",
      "Epoch 574: Train Loss: 0.001568, Train MAPE: 4.389%, Val Loss: 0.008888, Val MAPE: 4.991%\n",
      "Epoch 575: Train Loss: 0.001614, Train MAPE: 4.719%, Val Loss: 0.009229, Val MAPE: 5.420%\n",
      "Epoch 576: Train Loss: 0.001190, Train MAPE: 3.562%, Val Loss: 0.008446, Val MAPE: 4.330%\n",
      "Epoch 577: Train Loss: 0.001258, Train MAPE: 3.402%, Val Loss: 0.010210, Val MAPE: 4.114%\n",
      "Epoch 578: Train Loss: 0.001710, Train MAPE: 4.289%, Val Loss: 0.009368, Val MAPE: 5.075%\n",
      "Epoch 579: Train Loss: 0.001285, Train MAPE: 4.266%, Val Loss: 0.008239, Val MAPE: 5.094%\n",
      "Epoch 580: Train Loss: 0.001766, Train MAPE: 4.981%, Val Loss: 0.009386, Val MAPE: 5.744%\n",
      "Epoch 581: Train Loss: 0.002395, Train MAPE: 5.865%, Val Loss: 0.009674, Val MAPE: 6.859%\n",
      "Epoch 582: Train Loss: 0.001491, Train MAPE: 3.796%, Val Loss: 0.009446, Val MAPE: 4.429%\n",
      "Epoch 583: Train Loss: 0.001206, Train MAPE: 3.836%, Val Loss: 0.008197, Val MAPE: 4.593%\n",
      "Epoch 584: Train Loss: 0.002322, Train MAPE: 5.356%, Val Loss: 0.009239, Val MAPE: 6.239%\n",
      "Epoch 585: Train Loss: 0.001756, Train MAPE: 3.361%, Val Loss: 0.010096, Val MAPE: 3.935%\n",
      "Epoch 586: Train Loss: 0.001568, Train MAPE: 4.489%, Val Loss: 0.008616, Val MAPE: 5.383%\n",
      "Epoch 587: Train Loss: 0.002043, Train MAPE: 4.240%, Val Loss: 0.009567, Val MAPE: 5.008%\n",
      "Epoch 588: Train Loss: 0.001308, Train MAPE: 3.834%, Val Loss: 0.009336, Val MAPE: 4.503%\n",
      "Epoch 589: Train Loss: 0.001094, Train MAPE: 3.097%, Val Loss: 0.014927, Val MAPE: 4.057%\n",
      "Epoch 590: Train Loss: 0.002959, Train MAPE: 4.051%, Val Loss: 0.009682, Val MAPE: 4.953%\n",
      "Epoch 591: Train Loss: 0.003397, Train MAPE: 4.136%, Val Loss: 0.012020, Val MAPE: 4.889%\n",
      "Epoch 592: Train Loss: 0.002200, Train MAPE: 4.127%, Val Loss: 0.011429, Val MAPE: 4.895%\n",
      "Epoch 593: Train Loss: 0.001294, Train MAPE: 4.133%, Val Loss: 0.007924, Val MAPE: 4.749%\n",
      "Epoch 594: Train Loss: 0.001461, Train MAPE: 4.845%, Val Loss: 0.008733, Val MAPE: 6.012%\n",
      "Epoch 595: Train Loss: 0.001338, Train MAPE: 4.328%, Val Loss: 0.012735, Val MAPE: 5.072%\n",
      "Epoch 596: Train Loss: 0.001208, Train MAPE: 4.271%, Val Loss: 0.008336, Val MAPE: 5.121%\n",
      "Epoch 597: Train Loss: 0.001520, Train MAPE: 4.350%, Val Loss: 0.008623, Val MAPE: 5.050%\n",
      "Epoch 598: Train Loss: 0.001385, Train MAPE: 4.338%, Val Loss: 0.008495, Val MAPE: 5.054%\n",
      "Epoch 599: Train Loss: 0.001535, Train MAPE: 4.489%, Val Loss: 0.008856, Val MAPE: 5.287%\n",
      "Epoch 600: Train Loss: 0.001095, Train MAPE: 3.393%, Val Loss: 0.008352, Val MAPE: 4.265%\n",
      "Epoch 601: Train Loss: 0.001122, Train MAPE: 3.128%, Val Loss: 0.008410, Val MAPE: 3.767%\n",
      "Epoch 602: Train Loss: 0.001252, Train MAPE: 3.217%, Val Loss: 0.008332, Val MAPE: 4.125%\n",
      "Epoch 603: Train Loss: 0.001347, Train MAPE: 3.354%, Val Loss: 0.008738, Val MAPE: 4.225%\n",
      "Epoch 604: Train Loss: 0.001426, Train MAPE: 4.906%, Val Loss: 0.009814, Val MAPE: 5.832%\n",
      "Epoch 605: Train Loss: 0.001354, Train MAPE: 3.861%, Val Loss: 0.008966, Val MAPE: 4.687%\n",
      "Epoch 606: Train Loss: 0.001370, Train MAPE: 4.821%, Val Loss: 0.008902, Val MAPE: 6.199%\n",
      "Epoch 607: Train Loss: 0.001810, Train MAPE: 4.205%, Val Loss: 0.010299, Val MAPE: 5.062%\n",
      "Epoch 608: Train Loss: 0.001725, Train MAPE: 4.445%, Val Loss: 0.010342, Val MAPE: 5.184%\n",
      "Epoch 609: Train Loss: 0.001315, Train MAPE: 4.193%, Val Loss: 0.009372, Val MAPE: 5.010%\n",
      "Epoch 610: Train Loss: 0.001641, Train MAPE: 4.773%, Val Loss: 0.009548, Val MAPE: 5.647%\n",
      "Epoch 611: Train Loss: 0.001241, Train MAPE: 3.912%, Val Loss: 0.008752, Val MAPE: 4.775%\n",
      "Epoch 612: Train Loss: 0.001740, Train MAPE: 4.442%, Val Loss: 0.010489, Val MAPE: 5.339%\n",
      "Epoch 613: Train Loss: 0.002196, Train MAPE: 5.986%, Val Loss: 0.010396, Val MAPE: 6.793%\n",
      "Epoch 614: Train Loss: 0.001352, Train MAPE: 4.908%, Val Loss: 0.009324, Val MAPE: 5.682%\n",
      "Epoch 615: Train Loss: 0.002314, Train MAPE: 2.703%, Val Loss: 0.009632, Val MAPE: 3.220%\n",
      "Epoch 616: Train Loss: 0.001396, Train MAPE: 4.475%, Val Loss: 0.008955, Val MAPE: 5.613%\n",
      "Epoch 617: Train Loss: 0.001798, Train MAPE: 4.475%, Val Loss: 0.009595, Val MAPE: 5.298%\n",
      "Epoch 618: Train Loss: 0.001542, Train MAPE: 3.861%, Val Loss: 0.008996, Val MAPE: 4.661%\n",
      "Epoch 619: Train Loss: 0.001391, Train MAPE: 3.710%, Val Loss: 0.010816, Val MAPE: 4.520%\n",
      "Epoch 620: Train Loss: 0.001558, Train MAPE: 4.695%, Val Loss: 0.009070, Val MAPE: 5.624%\n",
      "Epoch 621: Train Loss: 0.001819, Train MAPE: 3.941%, Val Loss: 0.010379, Val MAPE: 4.836%\n",
      "Epoch 622: Train Loss: 0.001588, Train MAPE: 4.625%, Val Loss: 0.008469, Val MAPE: 5.612%\n",
      "Epoch 623: Train Loss: 0.001341, Train MAPE: 3.827%, Val Loss: 0.008914, Val MAPE: 4.460%\n",
      "Epoch 624: Train Loss: 0.002505, Train MAPE: 4.812%, Val Loss: 0.011153, Val MAPE: 6.239%\n",
      "Epoch 625: Train Loss: 0.001704, Train MAPE: 5.116%, Val Loss: 0.008851, Val MAPE: 6.244%\n",
      "Epoch 626: Train Loss: 0.001579, Train MAPE: 4.090%, Val Loss: 0.008809, Val MAPE: 4.713%\n",
      "Epoch 627: Train Loss: 0.001876, Train MAPE: 3.254%, Val Loss: 0.013431, Val MAPE: 4.003%\n",
      "Epoch 628: Train Loss: 0.001825, Train MAPE: 3.393%, Val Loss: 0.009773, Val MAPE: 3.868%\n",
      "Epoch 629: Train Loss: 0.001647, Train MAPE: 4.440%, Val Loss: 0.008949, Val MAPE: 5.203%\n",
      "Epoch 630: Train Loss: 0.001733, Train MAPE: 3.982%, Val Loss: 0.007463, Val MAPE: 4.562%\n",
      "Epoch 631: Train Loss: 0.001198, Train MAPE: 3.366%, Val Loss: 0.008783, Val MAPE: 4.145%\n",
      "Epoch 632: Train Loss: 0.001940, Train MAPE: 4.450%, Val Loss: 0.009620, Val MAPE: 5.270%\n",
      "Epoch 633: Train Loss: 0.001568, Train MAPE: 4.737%, Val Loss: 0.009004, Val MAPE: 5.799%\n",
      "Epoch 634: Train Loss: 0.001370, Train MAPE: 4.905%, Val Loss: 0.009782, Val MAPE: 6.426%\n",
      "Epoch 635: Train Loss: 0.001269, Train MAPE: 3.768%, Val Loss: 0.008099, Val MAPE: 4.906%\n",
      "Epoch 636: Train Loss: 0.001582, Train MAPE: 4.315%, Val Loss: 0.008952, Val MAPE: 5.205%\n",
      "Epoch 637: Train Loss: 0.002903, Train MAPE: 5.198%, Val Loss: 0.011926, Val MAPE: 6.688%\n",
      "Epoch 638: Train Loss: 0.003045, Train MAPE: 4.142%, Val Loss: 0.010252, Val MAPE: 5.184%\n",
      "Epoch 639: Train Loss: 0.002010, Train MAPE: 4.852%, Val Loss: 0.009236, Val MAPE: 6.080%\n",
      "Epoch 640: Train Loss: 0.001511, Train MAPE: 4.289%, Val Loss: 0.009017, Val MAPE: 5.082%\n",
      "Epoch 641: Train Loss: 0.001417, Train MAPE: 4.601%, Val Loss: 0.008839, Val MAPE: 5.683%\n",
      "Epoch 642: Train Loss: 0.001650, Train MAPE: 4.992%, Val Loss: 0.009057, Val MAPE: 5.826%\n",
      "Epoch 643: Train Loss: 0.001734, Train MAPE: 4.250%, Val Loss: 0.009239, Val MAPE: 5.311%\n",
      "Epoch 644: Train Loss: 0.001189, Train MAPE: 3.378%, Val Loss: 0.008248, Val MAPE: 4.162%\n",
      "Epoch 645: Train Loss: 0.001824, Train MAPE: 4.902%, Val Loss: 0.009469, Val MAPE: 5.789%\n",
      "Epoch 646: Train Loss: 0.001360, Train MAPE: 4.851%, Val Loss: 0.008422, Val MAPE: 5.817%\n",
      "Epoch 647: Train Loss: 0.001423, Train MAPE: 4.294%, Val Loss: 0.008208, Val MAPE: 5.165%\n",
      "Epoch 648: Train Loss: 0.001994, Train MAPE: 5.611%, Val Loss: 0.009108, Val MAPE: 6.651%\n",
      "Epoch 649: Train Loss: 0.001684, Train MAPE: 3.890%, Val Loss: 0.008771, Val MAPE: 4.626%\n",
      "Epoch 650: Train Loss: 0.004912, Train MAPE: 4.364%, Val Loss: 0.010246, Val MAPE: 5.016%\n",
      "Epoch 651: Train Loss: 0.003231, Train MAPE: 6.293%, Val Loss: 0.009799, Val MAPE: 7.448%\n",
      "Epoch 652: Train Loss: 0.022745, Train MAPE: 6.107%, Val Loss: 0.029507, Val MAPE: 7.320%\n",
      "Epoch 653: Train Loss: 0.001503, Train MAPE: 3.486%, Val Loss: 0.008013, Val MAPE: 4.482%\n",
      "Epoch 654: Train Loss: 0.001409, Train MAPE: 4.343%, Val Loss: 0.008621, Val MAPE: 4.987%\n",
      "Epoch 655: Train Loss: 0.001356, Train MAPE: 5.178%, Val Loss: 0.007995, Val MAPE: 6.086%\n",
      "Epoch 656: Train Loss: 0.001379, Train MAPE: 4.592%, Val Loss: 0.008022, Val MAPE: 5.248%\n",
      "Epoch 657: Train Loss: 0.001154, Train MAPE: 3.771%, Val Loss: 0.007976, Val MAPE: 4.469%\n",
      "Epoch 658: Train Loss: 0.001275, Train MAPE: 4.669%, Val Loss: 0.007788, Val MAPE: 5.299%\n",
      "Epoch 659: Train Loss: 0.001231, Train MAPE: 4.372%, Val Loss: 0.007800, Val MAPE: 5.200%\n",
      "Epoch 660: Train Loss: 0.001299, Train MAPE: 3.998%, Val Loss: 0.008084, Val MAPE: 4.604%\n",
      "Epoch 661: Train Loss: 0.001146, Train MAPE: 4.531%, Val Loss: 0.008218, Val MAPE: 5.384%\n",
      "Epoch 662: Train Loss: 0.001597, Train MAPE: 4.085%, Val Loss: 0.009467, Val MAPE: 4.770%\n",
      "Epoch 663: Train Loss: 0.001605, Train MAPE: 4.344%, Val Loss: 0.008302, Val MAPE: 5.047%\n",
      "Epoch 664: Train Loss: 0.001235, Train MAPE: 4.490%, Val Loss: 0.008394, Val MAPE: 5.377%\n",
      "Epoch 665: Train Loss: 0.001365, Train MAPE: 3.482%, Val Loss: 0.008398, Val MAPE: 4.108%\n",
      "Epoch 666: Train Loss: 0.001586, Train MAPE: 4.642%, Val Loss: 0.009011, Val MAPE: 5.460%\n",
      "Epoch 667: Train Loss: 0.001027, Train MAPE: 3.416%, Val Loss: 0.008127, Val MAPE: 3.955%\n",
      "Epoch 668: Train Loss: 0.001775, Train MAPE: 5.972%, Val Loss: 0.009152, Val MAPE: 6.899%\n",
      "Epoch 669: Train Loss: 0.001059, Train MAPE: 2.891%, Val Loss: 0.007593, Val MAPE: 3.643%\n",
      "Epoch 670: Train Loss: 0.001442, Train MAPE: 4.323%, Val Loss: 0.008937, Val MAPE: 5.030%\n",
      "Epoch 671: Train Loss: 0.001346, Train MAPE: 4.244%, Val Loss: 0.008465, Val MAPE: 4.797%\n",
      "Epoch 672: Train Loss: 0.001176, Train MAPE: 3.393%, Val Loss: 0.008494, Val MAPE: 4.120%\n",
      "Epoch 673: Train Loss: 0.001397, Train MAPE: 4.185%, Val Loss: 0.009460, Val MAPE: 4.987%\n",
      "Epoch 674: Train Loss: 0.001313, Train MAPE: 3.701%, Val Loss: 0.009186, Val MAPE: 4.378%\n",
      "Epoch 675: Train Loss: 0.002037, Train MAPE: 4.303%, Val Loss: 0.011012, Val MAPE: 5.350%\n",
      "Epoch 676: Train Loss: 0.001357, Train MAPE: 4.408%, Val Loss: 0.009282, Val MAPE: 5.235%\n",
      "Epoch 677: Train Loss: 0.001371, Train MAPE: 4.300%, Val Loss: 0.009240, Val MAPE: 5.147%\n",
      "Epoch 678: Train Loss: 0.001176, Train MAPE: 3.644%, Val Loss: 0.008577, Val MAPE: 4.365%\n",
      "Epoch 679: Train Loss: 0.001484, Train MAPE: 5.002%, Val Loss: 0.009039, Val MAPE: 5.913%\n",
      "Epoch 680: Train Loss: 0.001406, Train MAPE: 4.640%, Val Loss: 0.009327, Val MAPE: 5.476%\n",
      "Epoch 681: Train Loss: 0.001330, Train MAPE: 4.556%, Val Loss: 0.008947, Val MAPE: 5.509%\n",
      "Epoch 682: Train Loss: 0.001752, Train MAPE: 5.491%, Val Loss: 0.009356, Val MAPE: 6.300%\n",
      "Epoch 683: Train Loss: 0.001034, Train MAPE: 3.077%, Val Loss: 0.009063, Val MAPE: 3.936%\n",
      "Epoch 684: Train Loss: 0.001681, Train MAPE: 4.336%, Val Loss: 0.009629, Val MAPE: 4.949%\n",
      "Epoch 685: Train Loss: 0.001382, Train MAPE: 3.635%, Val Loss: 0.009462, Val MAPE: 4.278%\n",
      "Epoch 686: Train Loss: 0.001702, Train MAPE: 4.503%, Val Loss: 0.009151, Val MAPE: 5.595%\n",
      "Epoch 687: Train Loss: 0.001383, Train MAPE: 4.273%, Val Loss: 0.009254, Val MAPE: 4.897%\n",
      "Epoch 688: Train Loss: 0.001600, Train MAPE: 4.189%, Val Loss: 0.009128, Val MAPE: 4.981%\n",
      "Epoch 689: Train Loss: 0.001507, Train MAPE: 3.141%, Val Loss: 0.009007, Val MAPE: 3.672%\n",
      "Epoch 690: Train Loss: 0.001073, Train MAPE: 3.856%, Val Loss: 0.008461, Val MAPE: 4.709%\n",
      "Epoch 691: Train Loss: 0.001426, Train MAPE: 4.827%, Val Loss: 0.009814, Val MAPE: 5.517%\n",
      "Epoch 692: Train Loss: 0.001821, Train MAPE: 4.343%, Val Loss: 0.009591, Val MAPE: 5.230%\n",
      "Epoch 693: Train Loss: 0.001455, Train MAPE: 4.179%, Val Loss: 0.008714, Val MAPE: 5.209%\n",
      "Epoch 694: Train Loss: 0.001731, Train MAPE: 3.903%, Val Loss: 0.009011, Val MAPE: 4.786%\n",
      "Epoch 695: Train Loss: 0.002846, Train MAPE: 3.267%, Val Loss: 0.011147, Val MAPE: 3.920%\n",
      "Epoch 696: Train Loss: 0.001476, Train MAPE: 4.047%, Val Loss: 0.008859, Val MAPE: 4.846%\n",
      "Epoch 697: Train Loss: 0.001230, Train MAPE: 2.987%, Val Loss: 0.008534, Val MAPE: 3.500%\n",
      "Epoch 698: Train Loss: 0.001545, Train MAPE: 3.419%, Val Loss: 0.008804, Val MAPE: 4.807%\n",
      "Epoch 699: Train Loss: 0.001180, Train MAPE: 4.202%, Val Loss: 0.008690, Val MAPE: 5.038%\n",
      "Epoch 700: Train Loss: 0.001351, Train MAPE: 3.754%, Val Loss: 0.008694, Val MAPE: 4.531%\n",
      "Epoch 701: Train Loss: 0.001345, Train MAPE: 4.212%, Val Loss: 0.009310, Val MAPE: 5.104%\n",
      "Epoch 702: Train Loss: 0.001505, Train MAPE: 3.999%, Val Loss: 0.009399, Val MAPE: 4.953%\n",
      "Epoch 703: Train Loss: 0.001106, Train MAPE: 4.308%, Val Loss: 0.008169, Val MAPE: 5.248%\n",
      "Epoch 704: Train Loss: 0.001173, Train MAPE: 3.469%, Val Loss: 0.008285, Val MAPE: 4.250%\n",
      "Epoch 705: Train Loss: 0.001238, Train MAPE: 4.012%, Val Loss: 0.008928, Val MAPE: 4.903%\n",
      "Epoch 706: Train Loss: 0.001472, Train MAPE: 4.256%, Val Loss: 0.008488, Val MAPE: 5.115%\n",
      "Epoch 707: Train Loss: 0.001273, Train MAPE: 3.369%, Val Loss: 0.008680, Val MAPE: 4.142%\n",
      "Epoch 708: Train Loss: 0.001543, Train MAPE: 4.008%, Val Loss: 0.009178, Val MAPE: 5.008%\n",
      "Epoch 709: Train Loss: 0.001563, Train MAPE: 4.250%, Val Loss: 0.008618, Val MAPE: 5.283%\n",
      "Epoch 710: Train Loss: 0.001350, Train MAPE: 4.438%, Val Loss: 0.008912, Val MAPE: 5.301%\n",
      "Epoch 711: Train Loss: 0.001147, Train MAPE: 3.730%, Val Loss: 0.008852, Val MAPE: 4.402%\n",
      "Epoch 712: Train Loss: 0.001323, Train MAPE: 3.532%, Val Loss: 0.008685, Val MAPE: 4.344%\n",
      "Epoch 713: Train Loss: 0.001072, Train MAPE: 3.883%, Val Loss: 0.007913, Val MAPE: 4.572%\n",
      "Epoch 714: Train Loss: 0.001328, Train MAPE: 3.845%, Val Loss: 0.009369, Val MAPE: 4.524%\n",
      "Epoch 715: Train Loss: 0.001667, Train MAPE: 2.488%, Val Loss: 0.009733, Val MAPE: 3.060%\n",
      "Epoch 716: Train Loss: 0.001786, Train MAPE: 5.376%, Val Loss: 0.010300, Val MAPE: 6.392%\n",
      "Epoch 717: Train Loss: 0.001127, Train MAPE: 3.978%, Val Loss: 0.009046, Val MAPE: 4.778%\n",
      "Epoch 718: Train Loss: 0.001699, Train MAPE: 6.303%, Val Loss: 0.009190, Val MAPE: 7.484%\n",
      "Epoch 719: Train Loss: 0.001276, Train MAPE: 3.848%, Val Loss: 0.009334, Val MAPE: 4.957%\n",
      "Epoch 720: Train Loss: 0.001333, Train MAPE: 4.159%, Val Loss: 0.008370, Val MAPE: 5.211%\n",
      "Epoch 721: Train Loss: 0.001565, Train MAPE: 4.230%, Val Loss: 0.009104, Val MAPE: 5.084%\n",
      "Epoch 722: Train Loss: 0.001885, Train MAPE: 5.760%, Val Loss: 0.010535, Val MAPE: 6.996%\n",
      "Epoch 723: Train Loss: 0.001946, Train MAPE: 4.387%, Val Loss: 0.010268, Val MAPE: 5.286%\n",
      "Epoch 724: Train Loss: 0.001814, Train MAPE: 4.571%, Val Loss: 0.009024, Val MAPE: 5.385%\n",
      "Epoch 725: Train Loss: 0.001415, Train MAPE: 3.683%, Val Loss: 0.008840, Val MAPE: 4.495%\n",
      "Epoch 726: Train Loss: 0.001516, Train MAPE: 4.508%, Val Loss: 0.010415, Val MAPE: 5.541%\n",
      "Epoch 727: Train Loss: 0.001597, Train MAPE: 4.378%, Val Loss: 0.009455, Val MAPE: 5.611%\n",
      "Epoch 728: Train Loss: 0.001025, Train MAPE: 3.685%, Val Loss: 0.008421, Val MAPE: 4.726%\n",
      "Epoch 729: Train Loss: 0.001625, Train MAPE: 3.835%, Val Loss: 0.009104, Val MAPE: 4.685%\n",
      "Epoch 730: Train Loss: 0.001429, Train MAPE: 3.094%, Val Loss: 0.008962, Val MAPE: 3.859%\n",
      "Epoch 731: Train Loss: 0.001352, Train MAPE: 4.898%, Val Loss: 0.009036, Val MAPE: 5.793%\n",
      "Epoch 732: Train Loss: 0.001805, Train MAPE: 5.068%, Val Loss: 0.013373, Val MAPE: 5.886%\n",
      "Epoch 733: Train Loss: 0.001302, Train MAPE: 4.465%, Val Loss: 0.008624, Val MAPE: 5.215%\n",
      "Epoch 734: Train Loss: 0.001848, Train MAPE: 5.200%, Val Loss: 0.010111, Val MAPE: 5.982%\n",
      "Epoch 735: Train Loss: 0.001933, Train MAPE: 4.489%, Val Loss: 0.009858, Val MAPE: 5.340%\n",
      "Epoch 736: Train Loss: 0.001093, Train MAPE: 3.633%, Val Loss: 0.008391, Val MAPE: 4.828%\n",
      "Epoch 737: Train Loss: 0.001210, Train MAPE: 4.290%, Val Loss: 0.009604, Val MAPE: 5.204%\n",
      "Epoch 738: Train Loss: 0.001406, Train MAPE: 5.206%, Val Loss: 0.009168, Val MAPE: 6.220%\n",
      "Epoch 739: Train Loss: 0.001447, Train MAPE: 4.190%, Val Loss: 0.008926, Val MAPE: 5.135%\n",
      "Epoch 740: Train Loss: 0.001523, Train MAPE: 4.557%, Val Loss: 0.008955, Val MAPE: 5.503%\n",
      "Epoch 741: Train Loss: 0.001296, Train MAPE: 4.314%, Val Loss: 0.008478, Val MAPE: 5.699%\n",
      "Epoch 742: Train Loss: 0.001564, Train MAPE: 3.831%, Val Loss: 0.008302, Val MAPE: 4.507%\n",
      "Epoch 743: Train Loss: 0.001283, Train MAPE: 3.947%, Val Loss: 0.008942, Val MAPE: 4.858%\n",
      "Epoch 744: Train Loss: 0.001730, Train MAPE: 5.253%, Val Loss: 0.009421, Val MAPE: 6.346%\n",
      "Epoch 745: Train Loss: 0.001352, Train MAPE: 2.745%, Val Loss: 0.009494, Val MAPE: 3.396%\n",
      "Epoch 746: Train Loss: 0.001406, Train MAPE: 3.550%, Val Loss: 0.009087, Val MAPE: 4.278%\n",
      "Epoch 747: Train Loss: 0.001401, Train MAPE: 3.824%, Val Loss: 0.008804, Val MAPE: 4.484%\n",
      "Epoch 748: Train Loss: 0.001241, Train MAPE: 3.948%, Val Loss: 0.008043, Val MAPE: 4.788%\n",
      "Epoch 749: Train Loss: 0.001457, Train MAPE: 5.263%, Val Loss: 0.009951, Val MAPE: 6.262%\n",
      "Epoch 750: Train Loss: 0.005308, Train MAPE: 7.204%, Val Loss: 0.013654, Val MAPE: 8.493%\n",
      "Epoch 751: Train Loss: 0.002357, Train MAPE: 4.248%, Val Loss: 0.012868, Val MAPE: 4.983%\n",
      "Epoch 752: Train Loss: 0.001612, Train MAPE: 4.447%, Val Loss: 0.011583, Val MAPE: 5.397%\n",
      "Epoch 753: Train Loss: 0.001546, Train MAPE: 5.600%, Val Loss: 0.010449, Val MAPE: 6.614%\n",
      "Epoch 754: Train Loss: 0.001130, Train MAPE: 4.222%, Val Loss: 0.010596, Val MAPE: 5.149%\n",
      "Epoch 755: Train Loss: 0.001002, Train MAPE: 4.007%, Val Loss: 0.010818, Val MAPE: 4.809%\n",
      "Epoch 756: Train Loss: 0.001032, Train MAPE: 3.773%, Val Loss: 0.009119, Val MAPE: 4.707%\n",
      "Epoch 757: Train Loss: 0.001295, Train MAPE: 3.960%, Val Loss: 0.009727, Val MAPE: 4.526%\n",
      "Epoch 758: Train Loss: 0.001244, Train MAPE: 3.694%, Val Loss: 0.009521, Val MAPE: 4.814%\n",
      "Epoch 759: Train Loss: 0.001636, Train MAPE: 4.349%, Val Loss: 0.009586, Val MAPE: 5.191%\n",
      "Epoch 760: Train Loss: 0.001663, Train MAPE: 5.283%, Val Loss: 0.010494, Val MAPE: 6.255%\n",
      "Epoch 761: Train Loss: 0.001030, Train MAPE: 3.043%, Val Loss: 0.008543, Val MAPE: 3.799%\n",
      "Epoch 762: Train Loss: 0.001434, Train MAPE: 4.127%, Val Loss: 0.009680, Val MAPE: 4.989%\n",
      "Epoch 763: Train Loss: 0.001393, Train MAPE: 4.558%, Val Loss: 0.010727, Val MAPE: 5.611%\n",
      "Epoch 764: Train Loss: 0.001446, Train MAPE: 4.244%, Val Loss: 0.010936, Val MAPE: 5.174%\n",
      "Epoch 765: Train Loss: 0.001555, Train MAPE: 4.920%, Val Loss: 0.009691, Val MAPE: 6.115%\n",
      "Epoch 766: Train Loss: 0.001371, Train MAPE: 4.526%, Val Loss: 0.009506, Val MAPE: 5.368%\n",
      "Epoch 767: Train Loss: 0.001507, Train MAPE: 3.891%, Val Loss: 0.009095, Val MAPE: 4.742%\n",
      "Epoch 768: Train Loss: 0.001472, Train MAPE: 4.131%, Val Loss: 0.009209, Val MAPE: 5.236%\n",
      "Epoch 769: Train Loss: 0.001438, Train MAPE: 4.549%, Val Loss: 0.010809, Val MAPE: 5.547%\n",
      "Epoch 770: Train Loss: 0.001786, Train MAPE: 4.289%, Val Loss: 0.010089, Val MAPE: 5.599%\n",
      "Epoch 771: Train Loss: 0.001038, Train MAPE: 3.992%, Val Loss: 0.008835, Val MAPE: 4.791%\n",
      "Epoch 772: Train Loss: 0.001300, Train MAPE: 4.677%, Val Loss: 0.009502, Val MAPE: 5.689%\n",
      "Epoch 773: Train Loss: 0.001430, Train MAPE: 4.885%, Val Loss: 0.008725, Val MAPE: 5.840%\n",
      "Epoch 774: Train Loss: 0.013994, Train MAPE: 4.367%, Val Loss: 0.013879, Val MAPE: 4.962%\n",
      "Epoch 775: Train Loss: 0.001317, Train MAPE: 3.478%, Val Loss: 0.009343, Val MAPE: 4.318%\n",
      "Epoch 776: Train Loss: 0.001844, Train MAPE: 4.419%, Val Loss: 0.010124, Val MAPE: 5.253%\n",
      "Epoch 777: Train Loss: 0.001182, Train MAPE: 4.174%, Val Loss: 0.009249, Val MAPE: 5.055%\n",
      "Epoch 778: Train Loss: 0.001135, Train MAPE: 3.740%, Val Loss: 0.008805, Val MAPE: 4.521%\n",
      "Epoch 779: Train Loss: 0.001579, Train MAPE: 4.997%, Val Loss: 0.009168, Val MAPE: 5.984%\n",
      "Epoch 780: Train Loss: 0.001386, Train MAPE: 4.052%, Val Loss: 0.009094, Val MAPE: 4.922%\n",
      "Epoch 781: Train Loss: 0.001450, Train MAPE: 4.180%, Val Loss: 0.008486, Val MAPE: 4.883%\n",
      "Epoch 782: Train Loss: 0.001544, Train MAPE: 4.863%, Val Loss: 0.010520, Val MAPE: 5.891%\n",
      "Epoch 783: Train Loss: 0.001407, Train MAPE: 4.042%, Val Loss: 0.010743, Val MAPE: 4.757%\n",
      "Epoch 784: Train Loss: 0.001764, Train MAPE: 5.151%, Val Loss: 0.010477, Val MAPE: 6.154%\n",
      "Epoch 785: Train Loss: 0.001619, Train MAPE: 4.173%, Val Loss: 0.008841, Val MAPE: 5.179%\n",
      "Epoch 786: Train Loss: 0.001307, Train MAPE: 3.664%, Val Loss: 0.008850, Val MAPE: 4.555%\n",
      "Epoch 787: Train Loss: 0.001077, Train MAPE: 4.027%, Val Loss: 0.009319, Val MAPE: 4.908%\n",
      "Epoch 788: Train Loss: 0.001465, Train MAPE: 4.229%, Val Loss: 0.011852, Val MAPE: 5.034%\n",
      "Epoch 789: Train Loss: 0.001471, Train MAPE: 4.300%, Val Loss: 0.009307, Val MAPE: 5.184%\n",
      "Epoch 790: Train Loss: 0.000944, Train MAPE: 2.925%, Val Loss: 0.008256, Val MAPE: 3.755%\n",
      "Epoch 791: Train Loss: 0.001971, Train MAPE: 4.481%, Val Loss: 0.009931, Val MAPE: 5.310%\n",
      "Epoch 792: Train Loss: 0.001088, Train MAPE: 3.291%, Val Loss: 0.009056, Val MAPE: 4.044%\n",
      "Epoch 793: Train Loss: 0.001976, Train MAPE: 5.205%, Val Loss: 0.010102, Val MAPE: 6.022%\n",
      "Epoch 794: Train Loss: 0.001788, Train MAPE: 3.645%, Val Loss: 0.010508, Val MAPE: 4.296%\n",
      "Epoch 795: Train Loss: 0.001743, Train MAPE: 5.787%, Val Loss: 0.009982, Val MAPE: 7.419%\n",
      "Epoch 796: Train Loss: 0.001708, Train MAPE: 3.226%, Val Loss: 0.010861, Val MAPE: 4.037%\n",
      "Epoch 797: Train Loss: 0.001541, Train MAPE: 3.365%, Val Loss: 0.009863, Val MAPE: 4.182%\n",
      "Epoch 798: Train Loss: 0.000966, Train MAPE: 2.651%, Val Loss: 0.009427, Val MAPE: 3.340%\n",
      "Epoch 799: Train Loss: 0.001523, Train MAPE: 5.528%, Val Loss: 0.010449, Val MAPE: 6.547%\n",
      "Epoch 800: Train Loss: 0.001475, Train MAPE: 5.041%, Val Loss: 0.010700, Val MAPE: 6.260%\n",
      "Epoch 801: Train Loss: 0.001697, Train MAPE: 4.755%, Val Loss: 0.010425, Val MAPE: 5.620%\n",
      "Epoch 802: Train Loss: 0.001011, Train MAPE: 3.492%, Val Loss: 0.008978, Val MAPE: 4.282%\n",
      "Epoch 803: Train Loss: 0.001667, Train MAPE: 4.690%, Val Loss: 0.009661, Val MAPE: 6.276%\n",
      "Epoch 804: Train Loss: 0.001247, Train MAPE: 4.033%, Val Loss: 0.009789, Val MAPE: 5.003%\n",
      "Epoch 805: Train Loss: 0.001520, Train MAPE: 4.868%, Val Loss: 0.009311, Val MAPE: 5.959%\n",
      "Epoch 806: Train Loss: 0.001367, Train MAPE: 4.194%, Val Loss: 0.009140, Val MAPE: 5.215%\n",
      "Epoch 807: Train Loss: 0.001256, Train MAPE: 2.869%, Val Loss: 0.009773, Val MAPE: 3.653%\n",
      "Epoch 808: Train Loss: 0.001736, Train MAPE: 4.018%, Val Loss: 0.009341, Val MAPE: 4.834%\n",
      "Epoch 809: Train Loss: 0.001322, Train MAPE: 4.065%, Val Loss: 0.009006, Val MAPE: 4.878%\n",
      "Epoch 810: Train Loss: 0.002026, Train MAPE: 4.952%, Val Loss: 0.009784, Val MAPE: 5.817%\n",
      "Epoch 811: Train Loss: 0.001003, Train MAPE: 3.874%, Val Loss: 0.008773, Val MAPE: 5.106%\n",
      "Epoch 812: Train Loss: 0.001402, Train MAPE: 4.764%, Val Loss: 0.008549, Val MAPE: 5.908%\n",
      "Epoch 813: Train Loss: 0.001048, Train MAPE: 3.988%, Val Loss: 0.008348, Val MAPE: 4.938%\n",
      "Epoch 814: Train Loss: 0.001154, Train MAPE: 3.840%, Val Loss: 0.009409, Val MAPE: 4.813%\n",
      "Epoch 815: Train Loss: 0.001108, Train MAPE: 3.735%, Val Loss: 0.008268, Val MAPE: 4.492%\n",
      "Epoch 816: Train Loss: 0.001647, Train MAPE: 4.390%, Val Loss: 0.009713, Val MAPE: 5.379%\n",
      "Epoch 817: Train Loss: 0.001271, Train MAPE: 3.636%, Val Loss: 0.010210, Val MAPE: 4.603%\n",
      "Epoch 818: Train Loss: 0.001570, Train MAPE: 4.327%, Val Loss: 0.009668, Val MAPE: 5.613%\n",
      "Epoch 819: Train Loss: 0.001394, Train MAPE: 2.837%, Val Loss: 0.009349, Val MAPE: 3.563%\n",
      "Epoch 820: Train Loss: 0.001339, Train MAPE: 3.794%, Val Loss: 0.010210, Val MAPE: 4.871%\n",
      "Epoch 821: Train Loss: 0.001924, Train MAPE: 4.490%, Val Loss: 0.009027, Val MAPE: 5.395%\n",
      "Epoch 822: Train Loss: 0.001115, Train MAPE: 4.457%, Val Loss: 0.009801, Val MAPE: 5.248%\n",
      "Epoch 823: Train Loss: 0.000947, Train MAPE: 3.726%, Val Loss: 0.008600, Val MAPE: 4.452%\n",
      "Epoch 824: Train Loss: 0.000995, Train MAPE: 4.480%, Val Loss: 0.008492, Val MAPE: 5.484%\n",
      "Epoch 825: Train Loss: 0.001304, Train MAPE: 3.603%, Val Loss: 0.009128, Val MAPE: 4.263%\n",
      "Epoch 826: Train Loss: 0.001540, Train MAPE: 3.306%, Val Loss: 0.009462, Val MAPE: 4.227%\n",
      "Epoch 827: Train Loss: 0.006262, Train MAPE: 6.144%, Val Loss: 0.011954, Val MAPE: 7.121%\n",
      "Epoch 828: Train Loss: 0.007243, Train MAPE: 5.482%, Val Loss: 0.012822, Val MAPE: 7.190%\n",
      "Epoch 829: Train Loss: 0.005283, Train MAPE: 4.968%, Val Loss: 0.013680, Val MAPE: 6.406%\n",
      "Epoch 830: Train Loss: 0.002772, Train MAPE: 4.834%, Val Loss: 0.010932, Val MAPE: 5.672%\n",
      "Epoch 831: Train Loss: 0.001560, Train MAPE: 4.501%, Val Loss: 0.008830, Val MAPE: 5.365%\n",
      "Epoch 832: Train Loss: 0.001768, Train MAPE: 4.814%, Val Loss: 0.009308, Val MAPE: 5.503%\n",
      "Epoch 833: Train Loss: 0.001505, Train MAPE: 4.399%, Val Loss: 0.009252, Val MAPE: 4.979%\n",
      "Epoch 834: Train Loss: 0.001395, Train MAPE: 4.645%, Val Loss: 0.008665, Val MAPE: 5.400%\n",
      "Epoch 835: Train Loss: 0.001347, Train MAPE: 4.520%, Val Loss: 0.008916, Val MAPE: 5.492%\n",
      "Epoch 836: Train Loss: 0.001058, Train MAPE: 3.780%, Val Loss: 0.010009, Val MAPE: 4.435%\n",
      "Epoch 837: Train Loss: 0.001286, Train MAPE: 4.284%, Val Loss: 0.008361, Val MAPE: 4.829%\n",
      "Epoch 838: Train Loss: 0.001222, Train MAPE: 4.424%, Val Loss: 0.007710, Val MAPE: 5.182%\n",
      "Epoch 839: Train Loss: 0.001298, Train MAPE: 4.689%, Val Loss: 0.007835, Val MAPE: 5.429%\n",
      "Epoch 840: Train Loss: 0.001171, Train MAPE: 3.769%, Val Loss: 0.008361, Val MAPE: 4.546%\n",
      "Epoch 841: Train Loss: 0.001046, Train MAPE: 3.142%, Val Loss: 0.007753, Val MAPE: 3.680%\n",
      "Epoch 842: Train Loss: 0.001154, Train MAPE: 4.238%, Val Loss: 0.009662, Val MAPE: 5.119%\n",
      "Epoch 843: Train Loss: 0.001179, Train MAPE: 4.394%, Val Loss: 0.008184, Val MAPE: 5.660%\n",
      "Epoch 844: Train Loss: 0.001034, Train MAPE: 3.559%, Val Loss: 0.008161, Val MAPE: 4.233%\n",
      "Epoch 845: Train Loss: 0.002187, Train MAPE: 6.190%, Val Loss: 0.009092, Val MAPE: 7.165%\n",
      "Epoch 846: Train Loss: 0.001075, Train MAPE: 3.822%, Val Loss: 0.007487, Val MAPE: 4.606%\n",
      "Epoch 847: Train Loss: 0.001107, Train MAPE: 3.990%, Val Loss: 0.008003, Val MAPE: 4.692%\n",
      "Epoch 848: Train Loss: 0.002017, Train MAPE: 5.734%, Val Loss: 0.009425, Val MAPE: 6.615%\n",
      "Epoch 849: Train Loss: 0.001288, Train MAPE: 3.475%, Val Loss: 0.009005, Val MAPE: 4.094%\n",
      "Epoch 850: Train Loss: 0.001499, Train MAPE: 4.822%, Val Loss: 0.022250, Val MAPE: 5.667%\n",
      "Epoch 851: Train Loss: 0.001227, Train MAPE: 4.047%, Val Loss: 0.008297, Val MAPE: 4.800%\n",
      "Epoch 852: Train Loss: 0.001225, Train MAPE: 3.841%, Val Loss: 0.008392, Val MAPE: 4.588%\n",
      "Epoch 853: Train Loss: 0.001725, Train MAPE: 4.543%, Val Loss: 0.010580, Val MAPE: 5.455%\n",
      "Epoch 854: Train Loss: 0.001190, Train MAPE: 3.705%, Val Loss: 0.008841, Val MAPE: 4.792%\n",
      "Epoch 855: Train Loss: 0.001447, Train MAPE: 4.809%, Val Loss: 0.008458, Val MAPE: 5.448%\n",
      "Epoch 856: Train Loss: 0.001130, Train MAPE: 3.957%, Val Loss: 0.007778, Val MAPE: 4.645%\n",
      "Epoch 857: Train Loss: 0.001365, Train MAPE: 3.822%, Val Loss: 0.008261, Val MAPE: 4.481%\n",
      "Epoch 858: Train Loss: 0.001069, Train MAPE: 3.342%, Val Loss: 0.007947, Val MAPE: 3.968%\n",
      "Epoch 859: Train Loss: 0.001399, Train MAPE: 4.962%, Val Loss: 0.008452, Val MAPE: 5.986%\n",
      "Epoch 860: Train Loss: 0.001051, Train MAPE: 4.224%, Val Loss: 0.008473, Val MAPE: 5.021%\n",
      "Epoch 861: Train Loss: 0.001148, Train MAPE: 4.377%, Val Loss: 0.008594, Val MAPE: 5.228%\n",
      "Epoch 862: Train Loss: 0.001556, Train MAPE: 3.485%, Val Loss: 0.009532, Val MAPE: 4.127%\n",
      "Epoch 863: Train Loss: 0.002784, Train MAPE: 4.776%, Val Loss: 0.011514, Val MAPE: 5.523%\n",
      "Epoch 864: Train Loss: 0.001076, Train MAPE: 4.026%, Val Loss: 0.008659, Val MAPE: 4.811%\n",
      "Epoch 865: Train Loss: 0.001574, Train MAPE: 4.953%, Val Loss: 0.009409, Val MAPE: 5.763%\n",
      "Epoch 866: Train Loss: 0.001177, Train MAPE: 4.069%, Val Loss: 0.008510, Val MAPE: 4.858%\n",
      "Epoch 867: Train Loss: 0.001363, Train MAPE: 3.454%, Val Loss: 0.009305, Val MAPE: 4.148%\n",
      "Epoch 868: Train Loss: 0.001180, Train MAPE: 3.615%, Val Loss: 0.009020, Val MAPE: 4.309%\n",
      "Epoch 869: Train Loss: 0.001098, Train MAPE: 3.666%, Val Loss: 0.008510, Val MAPE: 4.349%\n",
      "Epoch 870: Train Loss: 0.001387, Train MAPE: 3.883%, Val Loss: 0.008712, Val MAPE: 4.632%\n",
      "Epoch 871: Train Loss: 0.001539, Train MAPE: 3.328%, Val Loss: 0.008872, Val MAPE: 3.988%\n",
      "Epoch 872: Train Loss: 0.000973, Train MAPE: 3.359%, Val Loss: 0.008319, Val MAPE: 4.109%\n",
      "Epoch 873: Train Loss: 0.001323, Train MAPE: 4.656%, Val Loss: 0.009065, Val MAPE: 5.682%\n",
      "Epoch 874: Train Loss: 0.001287, Train MAPE: 4.060%, Val Loss: 0.008239, Val MAPE: 4.800%\n",
      "Epoch 875: Train Loss: 0.001253, Train MAPE: 4.601%, Val Loss: 0.009097, Val MAPE: 5.424%\n",
      "Epoch 876: Train Loss: 0.001017, Train MAPE: 3.635%, Val Loss: 0.007959, Val MAPE: 4.272%\n",
      "Epoch 877: Train Loss: 0.001071, Train MAPE: 3.826%, Val Loss: 0.008745, Val MAPE: 4.662%\n",
      "Epoch 878: Train Loss: 0.001203, Train MAPE: 3.764%, Val Loss: 0.009423, Val MAPE: 4.769%\n",
      "Epoch 879: Train Loss: 0.001304, Train MAPE: 4.995%, Val Loss: 0.009777, Val MAPE: 6.081%\n",
      "Epoch 880: Train Loss: 0.001353, Train MAPE: 4.554%, Val Loss: 0.008993, Val MAPE: 5.488%\n",
      "Epoch 881: Train Loss: 0.001292, Train MAPE: 4.043%, Val Loss: 0.008678, Val MAPE: 4.895%\n",
      "Epoch 882: Train Loss: 0.001085, Train MAPE: 3.853%, Val Loss: 0.009106, Val MAPE: 4.800%\n",
      "Epoch 883: Train Loss: 0.001001, Train MAPE: 2.791%, Val Loss: 0.008482, Val MAPE: 3.599%\n",
      "Epoch 884: Train Loss: 0.001399, Train MAPE: 3.661%, Val Loss: 0.009391, Val MAPE: 4.395%\n",
      "Epoch 885: Train Loss: 0.000992, Train MAPE: 4.020%, Val Loss: 0.010025, Val MAPE: 4.739%\n",
      "Epoch 886: Train Loss: 0.001283, Train MAPE: 4.100%, Val Loss: 0.008681, Val MAPE: 4.933%\n",
      "Epoch 887: Train Loss: 0.001343, Train MAPE: 4.143%, Val Loss: 0.009305, Val MAPE: 4.960%\n",
      "Epoch 888: Train Loss: 0.001555, Train MAPE: 3.869%, Val Loss: 0.009475, Val MAPE: 4.592%\n",
      "Epoch 889: Train Loss: 0.001180, Train MAPE: 3.797%, Val Loss: 0.009634, Val MAPE: 4.764%\n",
      "Epoch 890: Train Loss: 0.001687, Train MAPE: 4.326%, Val Loss: 0.010249, Val MAPE: 5.141%\n",
      "Epoch 891: Train Loss: 0.001360, Train MAPE: 4.174%, Val Loss: 0.009285, Val MAPE: 5.259%\n",
      "Epoch 892: Train Loss: 0.001427, Train MAPE: 3.130%, Val Loss: 0.008892, Val MAPE: 3.758%\n",
      "Epoch 893: Train Loss: 0.001621, Train MAPE: 3.955%, Val Loss: 0.010832, Val MAPE: 4.666%\n",
      "Epoch 894: Train Loss: 0.001657, Train MAPE: 5.094%, Val Loss: 0.009472, Val MAPE: 6.049%\n",
      "Epoch 895: Train Loss: 0.001279, Train MAPE: 4.048%, Val Loss: 0.009342, Val MAPE: 5.171%\n",
      "Epoch 896: Train Loss: 0.001109, Train MAPE: 4.046%, Val Loss: 0.008668, Val MAPE: 4.739%\n",
      "Epoch 897: Train Loss: 0.001372, Train MAPE: 3.263%, Val Loss: 0.008627, Val MAPE: 3.957%\n",
      "Epoch 898: Train Loss: 0.001301, Train MAPE: 4.838%, Val Loss: 0.008660, Val MAPE: 5.762%\n",
      "Epoch 899: Train Loss: 0.001086, Train MAPE: 3.407%, Val Loss: 0.008427, Val MAPE: 4.106%\n",
      "Epoch 900: Train Loss: 0.001240, Train MAPE: 3.997%, Val Loss: 0.009144, Val MAPE: 4.818%\n",
      "Epoch 901: Train Loss: 0.001246, Train MAPE: 4.818%, Val Loss: 0.009054, Val MAPE: 5.864%\n",
      "Epoch 902: Train Loss: 0.001186, Train MAPE: 3.341%, Val Loss: 0.009133, Val MAPE: 4.214%\n",
      "Epoch 903: Train Loss: 0.001125, Train MAPE: 3.086%, Val Loss: 0.008539, Val MAPE: 3.888%\n",
      "Epoch 904: Train Loss: 0.001157, Train MAPE: 3.845%, Val Loss: 0.008873, Val MAPE: 4.818%\n",
      "Epoch 905: Train Loss: 0.001403, Train MAPE: 4.301%, Val Loss: 0.008503, Val MAPE: 5.541%\n",
      "Epoch 906: Train Loss: 0.001544, Train MAPE: 4.561%, Val Loss: 0.015342, Val MAPE: 5.539%\n",
      "Epoch 907: Train Loss: 0.001788, Train MAPE: 5.066%, Val Loss: 0.010133, Val MAPE: 6.231%\n",
      "Epoch 908: Train Loss: 0.000967, Train MAPE: 3.640%, Val Loss: 0.008668, Val MAPE: 4.507%\n",
      "Epoch 909: Train Loss: 0.001417, Train MAPE: 5.019%, Val Loss: 0.009371, Val MAPE: 6.004%\n",
      "Epoch 910: Train Loss: 0.001607, Train MAPE: 4.384%, Val Loss: 0.010539, Val MAPE: 5.171%\n",
      "Epoch 911: Train Loss: 0.001364, Train MAPE: 4.573%, Val Loss: 0.009469, Val MAPE: 5.300%\n",
      "Epoch 912: Train Loss: 0.001270, Train MAPE: 3.747%, Val Loss: 0.008714, Val MAPE: 4.438%\n",
      "Epoch 913: Train Loss: 0.001251, Train MAPE: 4.679%, Val Loss: 0.009179, Val MAPE: 5.516%\n",
      "Epoch 914: Train Loss: 0.001486, Train MAPE: 4.139%, Val Loss: 0.009389, Val MAPE: 4.920%\n",
      "Epoch 915: Train Loss: 0.001318, Train MAPE: 5.133%, Val Loss: 0.009120, Val MAPE: 6.085%\n",
      "Epoch 916: Train Loss: 0.001426, Train MAPE: 4.405%, Val Loss: 0.008831, Val MAPE: 5.143%\n",
      "Epoch 917: Train Loss: 0.001185, Train MAPE: 4.324%, Val Loss: 0.008922, Val MAPE: 5.175%\n",
      "Epoch 918: Train Loss: 0.001223, Train MAPE: 3.755%, Val Loss: 0.009194, Val MAPE: 4.523%\n",
      "Epoch 919: Train Loss: 0.001296, Train MAPE: 3.835%, Val Loss: 0.009961, Val MAPE: 4.574%\n",
      "Epoch 920: Train Loss: 0.001374, Train MAPE: 4.248%, Val Loss: 0.008810, Val MAPE: 5.014%\n",
      "Epoch 921: Train Loss: 0.001435, Train MAPE: 4.591%, Val Loss: 0.009883, Val MAPE: 5.570%\n",
      "Epoch 922: Train Loss: 0.001070, Train MAPE: 3.817%, Val Loss: 0.009430, Val MAPE: 4.718%\n",
      "Epoch 923: Train Loss: 0.001702, Train MAPE: 5.050%, Val Loss: 0.009153, Val MAPE: 5.795%\n",
      "Epoch 924: Train Loss: 0.001250, Train MAPE: 3.374%, Val Loss: 0.009663, Val MAPE: 4.011%\n",
      "Epoch 925: Train Loss: 0.001348, Train MAPE: 4.407%, Val Loss: 0.009185, Val MAPE: 5.052%\n",
      "Epoch 926: Train Loss: 0.001084, Train MAPE: 3.907%, Val Loss: 0.008776, Val MAPE: 4.836%\n",
      "Epoch 927: Train Loss: 0.001611, Train MAPE: 4.794%, Val Loss: 0.009110, Val MAPE: 5.561%\n",
      "Epoch 928: Train Loss: 0.001354, Train MAPE: 4.220%, Val Loss: 0.008886, Val MAPE: 4.985%\n",
      "Epoch 929: Train Loss: 0.001269, Train MAPE: 3.979%, Val Loss: 0.008865, Val MAPE: 4.715%\n",
      "Epoch 930: Train Loss: 0.001043, Train MAPE: 3.665%, Val Loss: 0.010080, Val MAPE: 4.394%\n",
      "Epoch 931: Train Loss: 0.001381, Train MAPE: 4.361%, Val Loss: 0.009487, Val MAPE: 5.221%\n",
      "Epoch 932: Train Loss: 0.001132, Train MAPE: 3.623%, Val Loss: 0.009364, Val MAPE: 4.290%\n",
      "Epoch 933: Train Loss: 0.000974, Train MAPE: 3.336%, Val Loss: 0.008948, Val MAPE: 4.161%\n",
      "Epoch 934: Train Loss: 0.001329, Train MAPE: 4.044%, Val Loss: 0.009087, Val MAPE: 4.956%\n",
      "Epoch 935: Train Loss: 0.001534, Train MAPE: 4.529%, Val Loss: 0.009314, Val MAPE: 5.250%\n",
      "Epoch 936: Train Loss: 0.001304, Train MAPE: 4.918%, Val Loss: 0.009737, Val MAPE: 5.799%\n",
      "Epoch 937: Train Loss: 0.001349, Train MAPE: 3.658%, Val Loss: 0.010649, Val MAPE: 4.563%\n",
      "Epoch 938: Train Loss: 0.001134, Train MAPE: 3.381%, Val Loss: 0.008261, Val MAPE: 4.119%\n",
      "Epoch 939: Train Loss: 0.001842, Train MAPE: 4.602%, Val Loss: 0.009292, Val MAPE: 5.333%\n",
      "Epoch 940: Train Loss: 0.001522, Train MAPE: 4.201%, Val Loss: 0.009700, Val MAPE: 4.883%\n",
      "Epoch 941: Train Loss: 0.001448, Train MAPE: 2.979%, Val Loss: 0.008892, Val MAPE: 3.651%\n",
      "Epoch 942: Train Loss: 0.001189, Train MAPE: 4.137%, Val Loss: 0.008142, Val MAPE: 4.863%\n",
      "Epoch 943: Train Loss: 0.001487, Train MAPE: 4.127%, Val Loss: 0.009818, Val MAPE: 4.845%\n",
      "Epoch 944: Train Loss: 0.001217, Train MAPE: 3.330%, Val Loss: 0.008836, Val MAPE: 3.993%\n",
      "Epoch 945: Train Loss: 0.001110, Train MAPE: 3.296%, Val Loss: 0.008867, Val MAPE: 4.317%\n",
      "Epoch 946: Train Loss: 0.001791, Train MAPE: 4.708%, Val Loss: 0.010153, Val MAPE: 5.601%\n",
      "Epoch 947: Train Loss: 0.001715, Train MAPE: 4.386%, Val Loss: 0.010881, Val MAPE: 5.237%\n",
      "Epoch 948: Train Loss: 0.001299, Train MAPE: 4.019%, Val Loss: 0.009557, Val MAPE: 5.065%\n",
      "Epoch 949: Train Loss: 0.001152, Train MAPE: 4.188%, Val Loss: 0.009278, Val MAPE: 5.016%\n",
      "Epoch 950: Train Loss: 0.001021, Train MAPE: 3.244%, Val Loss: 0.010077, Val MAPE: 3.961%\n",
      "Epoch 951: Train Loss: 0.001296, Train MAPE: 4.097%, Val Loss: 0.009911, Val MAPE: 4.903%\n",
      "Epoch 952: Train Loss: 0.001120, Train MAPE: 3.393%, Val Loss: 0.009479, Val MAPE: 4.133%\n",
      "Epoch 953: Train Loss: 0.001088, Train MAPE: 3.429%, Val Loss: 0.008918, Val MAPE: 4.112%\n",
      "Epoch 954: Train Loss: 0.001645, Train MAPE: 3.978%, Val Loss: 0.009627, Val MAPE: 4.840%\n",
      "Epoch 955: Train Loss: 0.001477, Train MAPE: 4.258%, Val Loss: 0.009627, Val MAPE: 5.103%\n",
      "Epoch 956: Train Loss: 0.001416, Train MAPE: 4.230%, Val Loss: 0.010351, Val MAPE: 5.124%\n",
      "Epoch 957: Train Loss: 0.001317, Train MAPE: 3.507%, Val Loss: 0.009600, Val MAPE: 4.190%\n",
      "Epoch 958: Train Loss: 0.001600, Train MAPE: 5.091%, Val Loss: 0.010297, Val MAPE: 6.109%\n",
      "Epoch 959: Train Loss: 0.001275, Train MAPE: 4.104%, Val Loss: 0.009645, Val MAPE: 5.077%\n",
      "Epoch 960: Train Loss: 0.001485, Train MAPE: 4.626%, Val Loss: 0.009785, Val MAPE: 5.654%\n",
      "Epoch 961: Train Loss: 0.001583, Train MAPE: 4.841%, Val Loss: 0.010508, Val MAPE: 5.854%\n",
      "Epoch 962: Train Loss: 0.001434, Train MAPE: 4.890%, Val Loss: 0.009525, Val MAPE: 5.799%\n",
      "Epoch 963: Train Loss: 0.001213, Train MAPE: 3.682%, Val Loss: 0.009320, Val MAPE: 4.535%\n",
      "Epoch 964: Train Loss: 0.000987, Train MAPE: 3.651%, Val Loss: 0.010278, Val MAPE: 5.139%\n",
      "Epoch 965: Train Loss: 0.001458, Train MAPE: 3.498%, Val Loss: 0.009773, Val MAPE: 4.806%\n",
      "Epoch 966: Train Loss: 0.001293, Train MAPE: 3.796%, Val Loss: 0.009649, Val MAPE: 4.647%\n",
      "Epoch 967: Train Loss: 0.001179, Train MAPE: 3.534%, Val Loss: 0.009400, Val MAPE: 4.306%\n",
      "Epoch 968: Train Loss: 0.001038, Train MAPE: 4.147%, Val Loss: 0.009118, Val MAPE: 4.951%\n",
      "Epoch 969: Train Loss: 0.001128, Train MAPE: 4.655%, Val Loss: 0.009551, Val MAPE: 5.424%\n",
      "Epoch 970: Train Loss: 0.001512, Train MAPE: 3.928%, Val Loss: 0.010238, Val MAPE: 4.668%\n",
      "Epoch 971: Train Loss: 0.001251, Train MAPE: 3.666%, Val Loss: 0.010676, Val MAPE: 4.457%\n",
      "Epoch 972: Train Loss: 0.001256, Train MAPE: 4.379%, Val Loss: 0.009884, Val MAPE: 5.223%\n",
      "Epoch 973: Train Loss: 0.001238, Train MAPE: 3.774%, Val Loss: 0.008917, Val MAPE: 4.424%\n",
      "Epoch 974: Train Loss: 0.001369, Train MAPE: 3.398%, Val Loss: 0.010360, Val MAPE: 4.179%\n",
      "Epoch 975: Train Loss: 0.001352, Train MAPE: 4.510%, Val Loss: 0.009885, Val MAPE: 5.506%\n",
      "Epoch 976: Train Loss: 0.001212, Train MAPE: 4.080%, Val Loss: 0.009528, Val MAPE: 4.877%\n",
      "Epoch 977: Train Loss: 0.000952, Train MAPE: 3.776%, Val Loss: 0.009615, Val MAPE: 4.494%\n",
      "Epoch 978: Train Loss: 0.001086, Train MAPE: 3.978%, Val Loss: 0.008791, Val MAPE: 4.830%\n",
      "Epoch 979: Train Loss: 0.001075, Train MAPE: 3.636%, Val Loss: 0.009697, Val MAPE: 4.701%\n",
      "Epoch 980: Train Loss: 0.001844, Train MAPE: 3.366%, Val Loss: 0.009924, Val MAPE: 3.998%\n",
      "Epoch 981: Train Loss: 0.001376, Train MAPE: 4.368%, Val Loss: 0.009110, Val MAPE: 5.204%\n",
      "Epoch 982: Train Loss: 0.001162, Train MAPE: 3.702%, Val Loss: 0.009074, Val MAPE: 4.483%\n",
      "Epoch 983: Train Loss: 0.001634, Train MAPE: 3.448%, Val Loss: 0.009972, Val MAPE: 4.251%\n",
      "Epoch 984: Train Loss: 0.003424, Train MAPE: 4.523%, Val Loss: 0.012947, Val MAPE: 5.230%\n",
      "Epoch 985: Train Loss: 0.002151, Train MAPE: 3.215%, Val Loss: 0.011272, Val MAPE: 4.160%\n",
      "Epoch 986: Train Loss: 0.001402, Train MAPE: 5.041%, Val Loss: 0.009429, Val MAPE: 5.905%\n",
      "Epoch 987: Train Loss: 0.000923, Train MAPE: 3.538%, Val Loss: 0.008856, Val MAPE: 4.338%\n",
      "Epoch 988: Train Loss: 0.001057, Train MAPE: 3.653%, Val Loss: 0.009126, Val MAPE: 4.313%\n",
      "Epoch 989: Train Loss: 0.001085, Train MAPE: 4.119%, Val Loss: 0.009357, Val MAPE: 4.984%\n",
      "Epoch 990: Train Loss: 0.001100, Train MAPE: 3.924%, Val Loss: 0.009369, Val MAPE: 4.721%\n",
      "Epoch 991: Train Loss: 0.001212, Train MAPE: 4.547%, Val Loss: 0.009335, Val MAPE: 5.360%\n",
      "Epoch 992: Train Loss: 0.001145, Train MAPE: 3.969%, Val Loss: 0.019507, Val MAPE: 4.709%\n",
      "Epoch 993: Train Loss: 0.000890, Train MAPE: 3.985%, Val Loss: 0.008741, Val MAPE: 4.939%\n",
      "Epoch 994: Train Loss: 0.000936, Train MAPE: 3.294%, Val Loss: 0.009208, Val MAPE: 3.944%\n",
      "Epoch 995: Train Loss: 0.001196, Train MAPE: 3.834%, Val Loss: 0.009472, Val MAPE: 4.643%\n",
      "Epoch 996: Train Loss: 0.001244, Train MAPE: 4.463%, Val Loss: 0.009677, Val MAPE: 5.343%\n",
      "Epoch 997: Train Loss: 0.001569, Train MAPE: 3.859%, Val Loss: 0.011491, Val MAPE: 4.574%\n",
      "Epoch 998: Train Loss: 0.001192, Train MAPE: 3.725%, Val Loss: 0.009016, Val MAPE: 4.329%\n",
      "Epoch 999: Train Loss: 0.000887, Train MAPE: 3.382%, Val Loss: 0.008653, Val MAPE: 4.064%\n",
      "Epoch 1000: Train Loss: 0.001031, Train MAPE: 4.127%, Val Loss: 0.008917, Val MAPE: 4.883%\n",
      "\n",
      "Lowest Train Loss: 2.224720 at Epoch 462\n",
      "Lowest Val Loss: 2.697208 at Epoch 462\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "\n",
    "# Initialize variables to store the lowest losses and corresponding epochs\n",
    "lowest_train_loss = float('inf')\n",
    "lowest_val_loss = float('inf')\n",
    "best_train_epoch = -1\n",
    "best_val_epoch = -1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train(model, train_loader, optimizer, criterion, device)\n",
    "    \n",
    "    train_loss = validate(model, train_loader, criterion, device)\n",
    "    val_loss = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    train_mape = validate_mape(model, train_loader, device, use_scaling=use_scaling)\n",
    "    val_mape = validate_mape(model, val_loader, device, use_scaling=use_scaling)\n",
    "\n",
    "    print(f\"Epoch {(epoch+1):3d}: Train Loss: {train_loss:.6f}, Train MAPE: {train_mape:.3f}%, Val Loss: {val_loss:.6f}, Val MAPE: {val_mape:.3f}%\")\n",
    "\n",
    "    #scheduler.step()\n",
    "    \n",
    "    if train_mape < lowest_train_loss:\n",
    "        lowest_train_loss = train_mape\n",
    "        best_train_epoch = epoch + 1\n",
    "\n",
    "    if val_mape < lowest_val_loss:\n",
    "        lowest_val_loss = val_mape\n",
    "        best_val_epoch = epoch + 1\n",
    "\n",
    "print(f\"\\nLowest Train Loss: {lowest_train_loss:.6f} at Epoch {best_train_epoch}\")\n",
    "print(f\"Lowest Val Loss: {lowest_val_loss:.6f} at Epoch {best_val_epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953a0d1d-5db4-417d-8fed-c44ca4256892",
   "metadata": {},
   "source": [
    "### Sample from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850f5ca1-bfd3-4716-9088-996e14a68c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_model(model, input, sequence_length, fixed_features_len, device):\n",
    "    model.eval()\n",
    "\n",
    "    input = input[:fixed_features_len, :]\n",
    "    T, C = input.shape\n",
    "    input = input.view(1, T, C)\n",
    "    outputs = torch.zeros(sequence_length, 2, device=device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i in range(0, sequence_length):\n",
    "            output = model(input)\n",
    "            predictions = output[:, -1, :].view(1, 1, 2)\n",
    "            outputs[i, :] = predictions\n",
    "            \n",
    "            if i == sequence_length:\n",
    "                break\n",
    "\n",
    "            input = torch.cat((input, predictions), dim=1)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea04c086-2a30-4939-8b0f-8f2a453338e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mape(true_values, predictions, epsilon=1e-8):\n",
    "    mape = torch.mean(torch.abs((true_values - predictions) / (true_values + epsilon))) * 100\n",
    "    return mape.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "5a32d364-08a1-4e3c-8b47-d93f79db3b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoregressive_error(model, loader, n_samples, sequence_length, fixed_features_len, use_scaling):\n",
    "\n",
    "    mapes = []\n",
    "    \n",
    "    for idx in range(n_samples):\n",
    "        input = loader.dataset[idx][0]\n",
    "        \n",
    "        outputs = sample_from_model(model, input, sequence_length, fixed_features_len, device)\n",
    "        targets = loader.dataset[idx][1][fixed_features_len-1:, :]\n",
    "\n",
    "        T, C = outputs.shape\n",
    "        outputs = outputs.view(1, T, C)\n",
    "        targets = targets.view(1, T, C)\n",
    "        \n",
    "        if use_scaling:\n",
    "            outputs = torch.tensor(reverse_tranform_output(outputs, 0, label_scaler))\n",
    "            targets = torch.tensor(reverse_tranform_output(targets, 0, label_scaler))\n",
    "        \n",
    "        mapes.append(calculate_mape(targets, outputs))\n",
    "\n",
    "    return np.mean(mapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "c5d8f5f5-b935-48d8-9bd4-8e864fe16bd0",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 1, 2]' is invalid for input of size 54",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[185], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m fixed_features_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(fixed_features)\n\u001b[0;32m      4\u001b[0m loader \u001b[38;5;241m=\u001b[39m train_loader\n\u001b[1;32m----> 6\u001b[0m \u001b[43mautoregressive_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfixed_features_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_scaling\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[184], line 8\u001b[0m, in \u001b[0;36mautoregressive_error\u001b[1;34m(model, loader, n_samples, sequence_length, fixed_features_len, use_scaling)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_samples):\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mdataset[idx][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m----> 8\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43msample_from_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfixed_features_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     targets \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mdataset[idx][\u001b[38;5;241m1\u001b[39m][fixed_features_len\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:, :]\n\u001b[0;32m     11\u001b[0m     T, C \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mshape\n",
      "Cell \u001b[1;32mIn[182], line 13\u001b[0m, in \u001b[0;36msample_from_model\u001b[1;34m(model, input, sequence_length, fixed_features_len, device)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, sequence_length):\n\u001b[0;32m     12\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     outputs[i, :] \u001b[38;5;241m=\u001b[39m predictions\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m sequence_length:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[1, 1, 2]' is invalid for input of size 54"
     ]
    }
   ],
   "source": [
    "n_samples = 100\n",
    "sequence_length = 27\n",
    "fixed_features_len = len(fixed_features)\n",
    "loader = train_loader\n",
    "\n",
    "autoregressive_error(model, loader, n_samples, sequence_length, fixed_features_len, use_scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98c960f-1bbe-44da-8d37-f26fae79bd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "sequence_length = 27\n",
    "fixed_features_len = len(fixed_features)\n",
    "max_features = fixed_features_len + len(labels) - 2\n",
    "\n",
    "total_mapes = []\n",
    "\n",
    "for j in range(sequence_length):\n",
    "    fixed_labels_len = j\n",
    "    \n",
    "    mapes = []\n",
    "    \n",
    "    for idx in range(n_samples):\n",
    "        input = val_loader.dataset[idx][0][fixed_labels_len]\n",
    "        output = sample_from_model(model, input, sequence_length, fixed_features_len, fixed_labels_len, max_features, device)\n",
    "        \n",
    "        target = convert(val_loader.dataset[idx][1].reshape(-1))\n",
    "        output = convert(output.reshape(-1))\n",
    "        \n",
    "        mapes.append(calculate_mape(target, output))\n",
    "    \n",
    "        if (idx+1) % 1000 == 0:\n",
    "            print(f\"sequence {idx/1000} done\")\n",
    "\n",
    "    total_mapes.append(np.mean(mapes))\n",
    "\n",
    "print(total_mapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4dc4e9-5dfe-4946-8e0d-5955bb9d7af8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
