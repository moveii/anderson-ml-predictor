{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8719881-b067-4348-b595-a5353298c294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca748c2b-4249-41c3-8efa-a86ca9872139",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ac45631-459d-4fa3-9f8b-211e80ca1226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b32cbd-59d6-4dc6-9f60-b550e4de4eab",
   "metadata": {},
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f7459a0f-4a62-48c6-9539-27276524679d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImpurityDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataframe, fixed_features, labels, feature_scaler=None, label_scaler=None, device=None):\n",
    "        assert len(labels) % 2 == 0\n",
    "        \n",
    "        self.fixed_features = fixed_features\n",
    "        self.labels = labels\n",
    "        self.n_samples = len(dataframe)\n",
    "        \n",
    "        self.output_length = 2\n",
    "        self.input_length = 2\n",
    "        self.sequence_length = len(fixed_features) + (len(labels) // 2) - 1\n",
    "        \n",
    "        df_features = dataframe[fixed_features]\n",
    "        df_labels = dataframe[labels]\n",
    "\n",
    "        if feature_scaler is not None and label_scaler is not None:\n",
    "            xs = feature_scaler.transform(df_features.values)\n",
    "            ys = label_scaler.transform(df_labels.values)\n",
    "        else:\n",
    "            xs = df_features.values\n",
    "            ys = df_labels.values\n",
    "\n",
    "        feature_data = np.zeros((self.n_samples, self.sequence_length, self.input_length))\n",
    "        label_data = np.zeros((self.n_samples, self.sequence_length, self.output_length))\n",
    "        \n",
    "        for i in range(self.n_samples):\n",
    "            xi = xs[i]\n",
    "            xj = ys[i][:-self.output_length]\n",
    "            \n",
    "            features = np.zeros((self.sequence_length, self.input_length))\n",
    "\n",
    "            features[:len(xi), 0] = xi\n",
    "            features[len(xi):, :] = xj.reshape(-1, self.input_length)\n",
    "            \n",
    "            y = np.zeros((self.sequence_length, self.input_length))\n",
    "            y[len(xi)-1:] = ys[i].reshape(-1, self.output_length)\n",
    "            \n",
    "            feature_data[i, :, :] = features\n",
    "            label_data[i, :, :] = y\n",
    "\n",
    "        self.feature_data = torch.tensor(feature_data, dtype=torch.float).to(device)\n",
    "        self.label_data = torch.tensor(label_data, dtype=torch.float).to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.feature_data[idx], self.label_data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "e4745dbe-e866-40ee-8014-320fb6de09fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scalers(dataframe, fixed_features, labels, test_size, random_state=None):\n",
    "    train_df, _ = train_test_split(dataframe, test_size=test_size, random_state=random_state)\n",
    "    df_features = train_df[fixed_features]\n",
    "    df_labels = train_df[labels]\n",
    "    \n",
    "    feature_scaler = StandardScaler()\n",
    "    feature_scaler.fit(df_features.values)\n",
    "    \n",
    "    label_scaler = StandardScaler()\n",
    "    label_scaler.fit(df_labels.values)\n",
    "\n",
    "    return feature_scaler, label_scaler      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "d1bb7fbd-7c0a-495d-bc24-772b235a90e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../data/20230825_144318_10k_EVDoubExp-TExp-wmax5-sparse-hyb_with_perturbation.csv'\n",
    "\n",
    "#fixed_features = ['beta', 'U', 'Eimp', 'E1', 'E2', 'E3', 'V1', 'V2', 'V3']\n",
    "fixed_features = ['beta', 'E1', 'E2', 'E3', 'V1', 'V2', 'V3']\n",
    "labels = ['ReSf1', 'ImSf1', 'ReSf3', 'ImSf3', 'ReSf5', 'ImSf5', 'ReSf7', 'ImSf7', 'ReSf9', 'ImSf9', 'ReSf11', 'ImSf11', 'ReSf13', 'ImSf13', 'ReSf15', 'ImSf15', 'ReSf17', 'ImSf17', 'ReSf19', 'ImSf19', 'ReSf21', 'ImSf21', 'ReSf23', 'ImSf23', 'ReSf25', 'ImSf25', 'ReSf29', 'ImSf29', 'ReSf33', 'ImSf33', 'ReSf37', 'ImSf37', 'ReSf43', 'ImSf43', 'ReSf49', 'ImSf49', 'ReSf57', 'ImSf57', 'ReSf69', 'ImSf69', 'ReSf83', 'ImSf83', 'ReSf101', 'ImSf101', 'ReSf127', 'ImSf127', 'ReSf165', 'ImSf165', 'ReSf237', 'ImSf237', 'ReSf399', 'ImSf399', 'ReSf1207', 'ImSf1207']\n",
    "\n",
    "df = pd.read_csv(file_path, skiprows=4) # we skip the first four lines, because they are just metadata\n",
    "df = df[fixed_features + labels]\n",
    "\n",
    "# remove one special row, looks very weird; ReSf1 = 2.377167465976437e-06\n",
    "df = df[df['ReSf1'] >= 1e-05]\n",
    "\n",
    "validation_size = 0.1 # 90% training, 10% for validation\n",
    "\n",
    "use_scaling = True\n",
    "\n",
    "if use_scaling:\n",
    "    feature_scaler, label_scaler = compute_scalers(df, fixed_features, labels, validation_size, seed) # make sure we use the same seed, otherwise the two splits differ!\n",
    "    dataset = ImpurityDataset(df, fixed_features, labels, feature_scaler, label_scaler, device=device)\n",
    "else:\n",
    "    dataset = ImpurityDataset(df, fixed_features, labels, device=device)\n",
    "\n",
    "indices = list(range(len(dataset)))\n",
    "train_indices, val_indices = train_test_split(indices, test_size=validation_size, random_state=seed)  # make sure we use the same seed, otherwise the two splits differ!\n",
    "\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "98f8b9d2-454a-41dd-bd6e-232135263d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.__getitem__(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "71ac3b91-cf54-40e2-b18b-682d9df0ee08",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(df['ReSf1'].abs())[:10];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23694fa-f39d-48b1-9a50-d815762d86ea",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "2581363e-4b96-4ef8-bf92-5f633aca6dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncodingL(nn.Module):\n",
    "    def __init__(self, T, C, dropout):\n",
    "        super(PositionalEncodingL, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.positional_embedding = nn.Parameter(torch.zeros(T, C))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        position_encoded = self.positional_embedding[:T, :].unsqueeze(0).expand(B, -1, -1)\n",
    "        x = x + position_encoded\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "c648bc1c-a7b4-4b82-bf63-e06dd0967f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    unmasked_input: int\n",
    "    input_dim: int\n",
    "    output_dim: int\n",
    "    sequence_length: int\n",
    "    \n",
    "    d_model: int\n",
    "    nhead: int\n",
    "    num_layers: int\n",
    "    dim_feedforward: int\n",
    "    \n",
    "    dropout: float\n",
    "    activation: str\n",
    "    bias: bool\n",
    "\n",
    "class AutoregressiveTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, config, device):\n",
    "        super(AutoregressiveTransformer, self).__init__()\n",
    "\n",
    "        self.config = config\n",
    "        self.input_projection = nn.Linear(config.input_dim, config.d_model)\n",
    "        self.positional_encoding = PositionalEncodingL(config.sequence_length, config.d_model, config.dropout)\n",
    "        \n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=config.d_model, \n",
    "            nhead=config.nhead, \n",
    "            dim_feedforward=config.dim_feedforward, \n",
    "            dropout=config.dropout,\n",
    "            activation=config.activation, \n",
    "            batch_first=True, \n",
    "            norm_first=True, \n",
    "            bias=config.bias\n",
    "        )\n",
    "        \n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=config.num_layers)\n",
    "        self.output_layer = nn.Linear(config.d_model, config.output_dim)\n",
    "        self.att_mask = {}\n",
    "        \n",
    "    def forward(self, x):        \n",
    "        x = self.input_projection(x)        \n",
    "        x = self.positional_encoding(x)\n",
    "\n",
    "        seq_len = x.size(1)\n",
    "        if seq_len not in self.att_mask:\n",
    "            self.att_mask[seq_len] = self.generate_mask(seq_len, device)\n",
    "        \n",
    "        output = self.transformer_decoder(x, x, tgt_mask=self.att_mask[seq_len])\n",
    "        output = self.output_layer(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def generate_mask(self, sequence_length, device):\n",
    "        mask = torch.full((sequence_length, sequence_length), float('-inf'), device=device)\n",
    "        mask[:, :self.config.unmasked_input] = 0 \n",
    "                \n",
    "        for i in range(self.config.unmasked_input, sequence_length):\n",
    "            mask[i:, i] = torch.tensor([0] * (sequence_length - i), device=device)\n",
    "            \n",
    "        return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b0842f-41f6-420c-b896-4b19f67805c3",
   "metadata": {},
   "source": [
    "### Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "id": "5edc082b-3d45-4afb-95b6-4d2edaa703e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269.442 k parameters\n"
     ]
    }
   ],
   "source": [
    "config = ModelConfig(\n",
    "    unmasked_input = len(fixed_features),\n",
    "    input_dim = 2,\n",
    "    output_dim = 2,\n",
    "    sequence_length = len(fixed_features) + (len(labels) // 2) - 1,\n",
    "    \n",
    "    d_model = 64,\n",
    "    nhead = 4,\n",
    "    num_layers = 4,\n",
    "    dim_feedforward = 64 * 4,\n",
    "    \n",
    "    dropout = 0.1,\n",
    "    activation = 'gelu',\n",
    "    bias = True\n",
    ")\n",
    "\n",
    "model = AutoregressiveTransformer(config, device).to(device)\n",
    "print(sum(p.numel() for p in model.parameters())/1e3, 'k parameters')\n",
    "criterion = nn.MSELoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "id": "dec32597-9f94-42fc-a936-be7eb75d331e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, masked_output, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    \n",
    "    for inputs, targets in train_loader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        outputs[:, :masked_output, :] = torch.zeros((masked_output, outputs.size(2)), device=device)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "\n",
    "    return np.average(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "id": "7d324c9b-2a4f-4b65-abcd-7ead8d07e4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, masked_output, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for inputs, targets in val_loader:\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            outputs[:, :masked_output, :] = torch.zeros((masked_output, outputs.size(2)), device=device)\n",
    "            loss = criterion(outputs, targets)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "    return np.average(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "id": "f72c8c95-6a90-4792-a74e-302534c87e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_mape(model, masked_output, loader, device, use_scaling=use_scaling, epsilon=1e-8):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for inputs, targets in loader:\n",
    "            ogtgt = targets\n",
    "            outputs = model(inputs)\n",
    "            outputs[:, :masked_output, :] = torch.zeros((masked_output, outputs.size(2)), device=device)\n",
    "            \n",
    "            if use_scaling:\n",
    "                outputs = torch.tensor(reverse_tranform_output(outputs, masked_output, label_scaler))\n",
    "                targets = torch.tensor(reverse_tranform_output(targets, masked_output, label_scaler))\n",
    "            \n",
    "            ape = torch.abs((targets - outputs) / (targets + epsilon))\n",
    "            mape = torch.mean(ape) * 100\n",
    "            \n",
    "            losses.append(mape.item())\n",
    "\n",
    "    return np.average(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "id": "aa4537b0-7c15-4d74-85bf-65d053516262",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_tranform_output(data, masked_output, scaler):\n",
    "    data = data[:, masked_output:, :]\n",
    "    B, T, C = data.shape\n",
    "    data = data.view(B, T*C)\n",
    "    data = data.cpu().numpy()\n",
    "    return scaler.inverse_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "id": "711ddc3c-3eac-4ae7-9007-46b69f5c6ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109.59513825007848"
      ]
     },
     "execution_count": 711,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_output = len(fixed_features) - 1\n",
    "\n",
    "validate_mape(model, masked_output, train_loader, device, use_scaling=use_scaling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec0284b-7904-4dbc-b42d-69b6d367b8d6",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "id": "b9067e6b-ea16-42e9-989b-ef6896481deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1: Train Loss: 0.022386, Train MAPE: 25.680%, Val Loss: 0.021374, Val MAPE: 29.629%, REG 119.019%\n",
      "Epoch   2: Train Loss: 0.019915, Train MAPE: 24.843%, Val Loss: 0.018657, Val MAPE: 29.360%\n",
      "Epoch   3: Train Loss: 0.018500, Train MAPE: 22.755%, Val Loss: 0.017257, Val MAPE: 27.746%\n",
      "Epoch   4: Train Loss: 0.017511, Train MAPE: 20.284%, Val Loss: 0.016330, Val MAPE: 25.132%\n",
      "Epoch   5: Train Loss: 0.016462, Train MAPE: 19.947%, Val Loss: 0.016525, Val MAPE: 23.890%\n",
      "Epoch   6: Train Loss: 0.014944, Train MAPE: 19.647%, Val Loss: 0.014376, Val MAPE: 23.476%, REG 127.537%\n",
      "Epoch   7: Train Loss: 0.014167, Train MAPE: 18.709%, Val Loss: 0.014221, Val MAPE: 22.695%\n",
      "Epoch   8: Train Loss: 0.012831, Train MAPE: 18.766%, Val Loss: 0.013227, Val MAPE: 22.313%\n",
      "Epoch   9: Train Loss: 0.011581, Train MAPE: 18.846%, Val Loss: 0.012882, Val MAPE: 22.600%\n",
      "Epoch  10: Train Loss: 0.009797, Train MAPE: 17.338%, Val Loss: 0.010271, Val MAPE: 21.175%\n",
      "Epoch  11: Train Loss: 0.008593, Train MAPE: 16.990%, Val Loss: 0.016251, Val MAPE: 20.109%, REG 128.232%\n",
      "Epoch  12: Train Loss: 0.006728, Train MAPE: 14.992%, Val Loss: 0.007984, Val MAPE: 17.361%\n",
      "Epoch  13: Train Loss: 0.005106, Train MAPE: 13.147%, Val Loss: 0.005961, Val MAPE: 15.241%\n",
      "Epoch  14: Train Loss: 0.004591, Train MAPE: 11.331%, Val Loss: 0.005205, Val MAPE: 12.479%\n",
      "Epoch  15: Train Loss: 0.004022, Train MAPE: 9.916%, Val Loss: 0.004867, Val MAPE: 11.541%\n",
      "Epoch  16: Train Loss: 0.003634, Train MAPE: 9.091%, Val Loss: 0.004369, Val MAPE: 10.409%, REG 105.312%\n",
      "Epoch  17: Train Loss: 0.003506, Train MAPE: 8.521%, Val Loss: 0.004234, Val MAPE: 10.187%\n",
      "Epoch  18: Train Loss: 0.003048, Train MAPE: 7.680%, Val Loss: 0.003859, Val MAPE: 9.296%\n",
      "Epoch  19: Train Loss: 0.003089, Train MAPE: 7.263%, Val Loss: 0.003985, Val MAPE: 8.662%\n",
      "Epoch  20: Train Loss: 0.002713, Train MAPE: 7.189%, Val Loss: 0.003683, Val MAPE: 8.943%\n",
      "Epoch  21: Train Loss: 0.002813, Train MAPE: 6.559%, Val Loss: 0.003872, Val MAPE: 8.050%, REG 88.926%\n",
      "Epoch  22: Train Loss: 0.002242, Train MAPE: 6.388%, Val Loss: 0.003407, Val MAPE: 8.086%\n",
      "Epoch  23: Train Loss: 0.002362, Train MAPE: 5.905%, Val Loss: 0.003419, Val MAPE: 7.330%\n",
      "Epoch  24: Train Loss: 0.002817, Train MAPE: 5.963%, Val Loss: 0.006044, Val MAPE: 7.255%\n",
      "Epoch  25: Train Loss: 0.002180, Train MAPE: 5.207%, Val Loss: 0.002992, Val MAPE: 7.540%\n",
      "Epoch  26: Train Loss: 0.002097, Train MAPE: 5.130%, Val Loss: 0.003943, Val MAPE: 6.280%, REG 84.081%\n",
      "Epoch  27: Train Loss: 0.001962, Train MAPE: 4.790%, Val Loss: 0.003577, Val MAPE: 5.956%\n",
      "Epoch  28: Train Loss: 0.001701, Train MAPE: 4.932%, Val Loss: 0.002797, Val MAPE: 5.903%\n",
      "Epoch  29: Train Loss: 0.001861, Train MAPE: 4.894%, Val Loss: 0.003031, Val MAPE: 5.962%\n",
      "Epoch  30: Train Loss: 0.001636, Train MAPE: 4.465%, Val Loss: 0.002381, Val MAPE: 5.623%\n",
      "Epoch  31: Train Loss: 0.001717, Train MAPE: 4.598%, Val Loss: 0.002736, Val MAPE: 5.403%, REG 91.800%\n",
      "Epoch  32: Train Loss: 0.001906, Train MAPE: 4.606%, Val Loss: 0.002629, Val MAPE: 5.416%\n",
      "Epoch  33: Train Loss: 0.001674, Train MAPE: 4.985%, Val Loss: 0.002747, Val MAPE: 6.063%\n",
      "Epoch  34: Train Loss: 0.001838, Train MAPE: 4.420%, Val Loss: 0.006033, Val MAPE: 5.957%\n",
      "Epoch  35: Train Loss: 0.001404, Train MAPE: 4.683%, Val Loss: 0.002122, Val MAPE: 5.601%\n",
      "Epoch  36: Train Loss: 0.001590, Train MAPE: 3.784%, Val Loss: 0.002550, Val MAPE: 4.526%, REG 101.957%\n",
      "Epoch  37: Train Loss: 0.001737, Train MAPE: 5.133%, Val Loss: 0.003205, Val MAPE: 6.366%\n",
      "Epoch  38: Train Loss: 0.001582, Train MAPE: 4.031%, Val Loss: 0.002423, Val MAPE: 4.810%\n",
      "Epoch  39: Train Loss: 0.001651, Train MAPE: 4.266%, Val Loss: 0.002681, Val MAPE: 4.986%\n",
      "Epoch  40: Train Loss: 0.001455, Train MAPE: 4.369%, Val Loss: 0.002328, Val MAPE: 5.467%\n",
      "Epoch  41: Train Loss: 0.001540, Train MAPE: 4.181%, Val Loss: 0.002445, Val MAPE: 5.126%, REG 110.313%\n",
      "Epoch  42: Train Loss: 0.001362, Train MAPE: 4.099%, Val Loss: 0.002079, Val MAPE: 5.112%\n",
      "Epoch  43: Train Loss: 0.001213, Train MAPE: 4.923%, Val Loss: 0.001710, Val MAPE: 5.935%\n",
      "Epoch  44: Train Loss: 0.001211, Train MAPE: 3.538%, Val Loss: 0.001672, Val MAPE: 4.067%\n",
      "Epoch  45: Train Loss: 0.001228, Train MAPE: 3.782%, Val Loss: 0.001757, Val MAPE: 4.595%\n",
      "Epoch  46: Train Loss: 0.001302, Train MAPE: 3.902%, Val Loss: 0.001907, Val MAPE: 4.711%, REG 114.121%\n",
      "Epoch  47: Train Loss: 0.001241, Train MAPE: 4.034%, Val Loss: 0.001815, Val MAPE: 4.876%\n",
      "Epoch  48: Train Loss: 0.001331, Train MAPE: 3.337%, Val Loss: 0.001663, Val MAPE: 4.163%\n",
      "Epoch  49: Train Loss: 0.001498, Train MAPE: 3.551%, Val Loss: 0.001944, Val MAPE: 4.366%\n",
      "Epoch  50: Train Loss: 0.001688, Train MAPE: 3.853%, Val Loss: 0.002026, Val MAPE: 4.713%\n",
      "Epoch  51: Train Loss: 0.001114, Train MAPE: 2.990%, Val Loss: 0.001471, Val MAPE: 3.542%, REG 115.906%\n",
      "Epoch  52: Train Loss: 0.001164, Train MAPE: 3.684%, Val Loss: 0.001413, Val MAPE: 4.282%\n",
      "Epoch  53: Train Loss: 0.001247, Train MAPE: 3.337%, Val Loss: 0.001899, Val MAPE: 4.137%\n",
      "Epoch  54: Train Loss: 0.001537, Train MAPE: 4.015%, Val Loss: 0.002425, Val MAPE: 4.925%\n",
      "Epoch  55: Train Loss: 0.001145, Train MAPE: 3.519%, Val Loss: 0.001589, Val MAPE: 4.344%\n",
      "Epoch  56: Train Loss: 0.001617, Train MAPE: 3.738%, Val Loss: 0.001750, Val MAPE: 4.251%, REG 114.258%\n",
      "Epoch  57: Train Loss: 0.001396, Train MAPE: 3.668%, Val Loss: 0.002160, Val MAPE: 5.180%\n",
      "Epoch  58: Train Loss: 0.001331, Train MAPE: 3.279%, Val Loss: 0.001804, Val MAPE: 4.925%\n",
      "Epoch  59: Train Loss: 0.001943, Train MAPE: 3.177%, Val Loss: 0.002516, Val MAPE: 3.716%\n",
      "Epoch  60: Train Loss: 0.001054, Train MAPE: 3.131%, Val Loss: 0.001217, Val MAPE: 3.677%\n",
      "Epoch  61: Train Loss: 0.001214, Train MAPE: 3.434%, Val Loss: 0.001480, Val MAPE: 4.185%, REG 123.008%\n",
      "Epoch  62: Train Loss: 0.001037, Train MAPE: 3.914%, Val Loss: 0.001360, Val MAPE: 4.808%\n",
      "Epoch  63: Train Loss: 0.001181, Train MAPE: 3.864%, Val Loss: 0.001458, Val MAPE: 4.781%\n",
      "Epoch  64: Train Loss: 0.001035, Train MAPE: 3.817%, Val Loss: 0.001273, Val MAPE: 4.696%\n",
      "Epoch  65: Train Loss: 0.001777, Train MAPE: 3.219%, Val Loss: 0.002160, Val MAPE: 4.030%\n",
      "Epoch  66: Train Loss: 0.001244, Train MAPE: 3.824%, Val Loss: 0.001504, Val MAPE: 4.899%, REG 124.642%\n",
      "Epoch  67: Train Loss: 0.001406, Train MAPE: 3.201%, Val Loss: 0.001755, Val MAPE: 3.918%\n",
      "Epoch  68: Train Loss: 0.001384, Train MAPE: 3.006%, Val Loss: 0.001621, Val MAPE: 3.477%\n",
      "Epoch  69: Train Loss: 0.001392, Train MAPE: 3.287%, Val Loss: 0.001829, Val MAPE: 4.079%\n",
      "Epoch  70: Train Loss: 0.001311, Train MAPE: 3.442%, Val Loss: 0.002074, Val MAPE: 4.343%\n",
      "Epoch  71: Train Loss: 0.001453, Train MAPE: 3.272%, Val Loss: 0.001760, Val MAPE: 3.963%, REG 131.092%\n",
      "Epoch  72: Train Loss: 0.001198, Train MAPE: 3.291%, Val Loss: 0.001749, Val MAPE: 3.935%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "masked_output = len(fixed_features) - 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train(model, masked_output, train_loader, optimizer, criterion, device)\n",
    "    \n",
    "    train_loss = validate(model, masked_output, train_loader, criterion, device)\n",
    "    val_loss = validate(model, masked_output, val_loader, criterion, device)\n",
    "    \n",
    "    train_mape = validate_mape(model, masked_output, train_loader, device, use_scaling=use_scaling)\n",
    "    val_mape = validate_mape(model, masked_output, val_loader, device, use_scaling=use_scaling)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        n_samples = 100\n",
    "        sequence_length = 27\n",
    "        fixed_features_len = len(fixed_features)\n",
    "        loader = val_loader\n",
    "        \n",
    "        reg_error = autoregressive_error(model, loader, n_samples, sequence_length, fixed_features_len, use_scaling)\n",
    "        print(f\"Epoch {(epoch+1):3d}: Train Loss: {train_loss:.6f}, Train MAPE: {train_mape:.3f}%, Val Loss: {val_loss:.6f}, Val MAPE: {val_mape:.3f}%, REG {reg_error:.3f}%\")\n",
    "    else:\n",
    "        print(f\"Epoch {(epoch+1):3d}: Train Loss: {train_loss:.6f}, Train MAPE: {train_mape:.3f}%, Val Loss: {val_loss:.6f}, Val MAPE: {val_mape:.3f}%\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953a0d1d-5db4-417d-8fed-c44ca4256892",
   "metadata": {},
   "source": [
    "### Sample from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "id": "850f5ca1-bfd3-4716-9088-996e14a68c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_model(model, input, sequence_length, fixed_features_len, device):\n",
    "    model.eval()\n",
    "\n",
    "    input = input[:fixed_features_len, :]\n",
    "    T, C = input.shape\n",
    "    input = input.view(1, T, C)\n",
    "    outputs = torch.zeros(sequence_length, 2, device=device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i in range(0, sequence_length):\n",
    "            output = model(input)\n",
    "            predictions = output[:, -1, :].view(1, 1, 2)\n",
    "            outputs[i, :] = predictions\n",
    "            \n",
    "            if i == sequence_length:\n",
    "                break\n",
    "\n",
    "            input = torch.cat((input, predictions), dim=1)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "id": "ea04c086-2a30-4939-8b0f-8f2a453338e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mape(true_values, predictions, epsilon=1e-8):\n",
    "    mape = torch.mean(torch.abs((true_values - predictions) / (true_values + epsilon))) * 100\n",
    "    return mape.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "id": "5a32d364-08a1-4e3c-8b47-d93f79db3b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoregressive_error(model, loader, n_samples, sequence_length, fixed_features_len, use_scaling):\n",
    "\n",
    "    mapes = []\n",
    "    \n",
    "    for idx in range(n_samples):\n",
    "        input = loader.dataset[idx][0]\n",
    "        \n",
    "        outputs = sample_from_model(model, input, sequence_length, fixed_features_len, device)\n",
    "        targets = loader.dataset[idx][1][fixed_features_len-1:, :]\n",
    "\n",
    "        T, C = outputs.shape\n",
    "        outputs = outputs.view(1, T, C)\n",
    "        targets = targets.view(1, T, C)\n",
    "        \n",
    "        if use_scaling:\n",
    "            outputs = torch.tensor(reverse_tranform_output(outputs, 0, label_scaler))\n",
    "            targets = torch.tensor(reverse_tranform_output(targets, 0, label_scaler))\n",
    "        \n",
    "        mapes.append(calculate_mape(targets, outputs))\n",
    "\n",
    "    return np.mean(mapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "id": "c5d8f5f5-b935-48d8-9bd4-8e864fe16bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.46531008720397"
      ]
     },
     "execution_count": 713,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples = 100\n",
    "sequence_length = 27\n",
    "fixed_features_len = len(fixed_features)\n",
    "loader = train_loader\n",
    "\n",
    "autoregressive_error(model, loader, n_samples, sequence_length, fixed_features_len, use_scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98c960f-1bbe-44da-8d37-f26fae79bd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "sequence_length = 27\n",
    "fixed_features_len = len(fixed_features)\n",
    "max_features = fixed_features_len + len(labels) - 2\n",
    "\n",
    "total_mapes = []\n",
    "\n",
    "for j in range(sequence_length):\n",
    "    fixed_labels_len = j\n",
    "    \n",
    "    mapes = []\n",
    "    \n",
    "    for idx in range(n_samples):\n",
    "        input = val_loader.dataset[idx][0][fixed_labels_len]\n",
    "        output = sample_from_model(model, input, sequence_length, fixed_features_len, fixed_labels_len, max_features, device)\n",
    "        \n",
    "        target = convert(val_loader.dataset[idx][1].reshape(-1))\n",
    "        output = convert(output.reshape(-1))\n",
    "        \n",
    "        mapes.append(calculate_mape(target, output))\n",
    "    \n",
    "        if (idx+1) % 1000 == 0:\n",
    "            print(f\"sequence {idx/1000} done\")\n",
    "\n",
    "    total_mapes.append(np.mean(mapes))\n",
    "\n",
    "print(total_mapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4dc4e9-5dfe-4946-8e0d-5955bb9d7af8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
