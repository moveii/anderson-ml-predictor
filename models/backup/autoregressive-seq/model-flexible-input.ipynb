{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8719881-b067-4348-b595-a5353298c294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, QuantileTransformer\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca748c2b-4249-41c3-8efa-a86ca9872139",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ac45631-459d-4fa3-9f8b-211e80ca1226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b32cbd-59d6-4dc6-9f60-b550e4de4eab",
   "metadata": {},
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7459a0f-4a62-48c6-9539-27276524679d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImpurityDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataframe, fixed_features, labels, feature_scaler=None, label_scaler=None, device=None):\n",
    "        assert len(labels) % 2 == 0\n",
    "        \n",
    "        self.fixed_features = fixed_features\n",
    "        self.labels = labels\n",
    "        self.n_samples = len(dataframe)\n",
    "        \n",
    "        self.output_length = 2\n",
    "        self.input_length = len(fixed_features) + len(labels) - self.output_length\n",
    "        self.sequence_length = len(labels) // self.output_length\n",
    "\n",
    "        df_features = dataframe[fixed_features]\n",
    "        df_labels = dataframe[labels]\n",
    "\n",
    "        if feature_scaler is not None and label_scaler is not None:\n",
    "            xs = feature_scaler.transform(df_features.values)\n",
    "            ys = label_scaler.transform(df_labels.values)\n",
    "        else:\n",
    "            xs = df_features.values\n",
    "            ys = df_labels.values\n",
    "\n",
    "        feature_data = np.zeros((self.n_samples, self.sequence_length, self.input_length))\n",
    "        label_data = np.zeros((self.n_samples, self.sequence_length, self.output_length))\n",
    "        \n",
    "        for i in range(self.n_samples):\n",
    "            for j in range(self.sequence_length):\n",
    "                xi = xs[i]\n",
    "                xj = ys[i][0:j*2]\n",
    "                y = ys[i][j*2:j*2+2]\n",
    "        \n",
    "                features = np.concatenate([xi, xj], axis=0)\n",
    "                pad_width = self.input_length - len(features)\n",
    "                feature_data[i, j, :] = np.pad(features, (0, pad_width))\n",
    "                label_data[i, j, :] = y\n",
    "\n",
    "        self.feature_data = torch.tensor(feature_data, dtype=torch.float).to(device)\n",
    "        self.label_data = torch.tensor(label_data, dtype=torch.float).to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.feature_data[idx], self.label_data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4745dbe-e866-40ee-8014-320fb6de09fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scalers(dataframe, fixed_features, labels, test_size=0.1, random_state=None):\n",
    "    train_df, _ = train_test_split(dataframe, test_size=test_size, random_state=random_state)\n",
    "    df_features = train_df[fixed_features]\n",
    "    df_labels = train_df[labels]\n",
    "    \n",
    "    feature_scaler = StandardScaler()\n",
    "    feature_scaler.fit(df_features.values)\n",
    "    \n",
    "    label_scaler = StandardScaler()\n",
    "    label_scaler.fit(df_labels.values) \n",
    "\n",
    "    return feature_scaler, label_scaler      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d1bb7fbd-7c0a-495d-bc24-772b235a90e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../data/20230825_144318_10k_EVDoubExp-TExp-wmax5-sparse-hyb_with_perturbation.csv'\n",
    "\n",
    "#fixed_features = ['beta', 'U', 'Eimp', 'E1', 'E2', 'E3', 'V1', 'V2', 'V3']\n",
    "fixed_features = ['beta', 'E1', 'E2', 'E3', 'V1', 'V2', 'V3']\n",
    "#fixed_features = ['beta', 'E1', 'E2', 'E3', 'V1', 'V2', 'V3', 'ReFso1', 'ImFso1', 'ReFso3', 'ImFso3', 'ReFso5', 'ImFso5', 'ReFso7', 'ImFso7', 'ReFso9', 'ImFso9', 'ReFso11', 'ImFso11', 'ReFso13', 'ImFso13', 'ReFso15', 'ImFso15', 'ReFso17', 'ImFso17', 'ReFso19', 'ImFso19', 'ReFso21', 'ImFso21', 'ReFso23', 'ImFso23', 'ReFso25', 'ImFso25', 'ReFso29', 'ImFso29', 'ReFso33', 'ImFso33', 'ReFso37', 'ImFso37', 'ReFso43', 'ImFso43', 'ReFso49', 'ImFso49', 'ReFso57', 'ImFso57', 'ReFso69', 'ImFso69', 'ReFso83', 'ImFso83', 'ReFso101', 'ImFso101', 'ReFso127', 'ImFso127', 'ReFso165', 'ImFso165', 'ReFso237', 'ImFso237', 'ReFso399', 'ImFso399', 'ReFso1207', 'ImFso1207']\n",
    "labels = ['ReSf1', 'ImSf1', 'ReSf3', 'ImSf3', 'ReSf5', 'ImSf5', 'ReSf7', 'ImSf7', 'ReSf9', 'ImSf9', 'ReSf11', 'ImSf11', 'ReSf13', 'ImSf13', 'ReSf15', 'ImSf15', 'ReSf17', 'ImSf17', 'ReSf19', 'ImSf19', 'ReSf21', 'ImSf21', 'ReSf23', 'ImSf23', 'ReSf25', 'ImSf25', 'ReSf29', 'ImSf29', 'ReSf33', 'ImSf33', 'ReSf37', 'ImSf37', 'ReSf43', 'ImSf43', 'ReSf49', 'ImSf49', 'ReSf57', 'ImSf57', 'ReSf69', 'ImSf69', 'ReSf83', 'ImSf83', 'ReSf101', 'ImSf101', 'ReSf127', 'ImSf127', 'ReSf165', 'ImSf165', 'ReSf237', 'ImSf237', 'ReSf399', 'ImSf399', 'ReSf1207', 'ImSf1207']\n",
    "#labels = ['ReSf1207', 'ImSf1207', 'ReSf399', 'ImSf399', 'ReSf237', 'ImSf237', 'ReSf165', 'ImSf165', 'ReSf127', 'ImSf127', 'ReSf101', 'ImSf101', 'ReSf83', 'ImSf83', 'ReSf69', 'ImSf69', 'ReSf57', 'ImSf57', 'ReSf49', 'ImSf49', 'ReSf43', 'ImSf43', 'ReSf37', 'ImSf37', 'ReSf33', 'ImSf33', 'ReSf29', 'ImSf29', 'ReSf25', 'ImSf25', 'ReSf23', 'ImSf23', 'ReSf21', 'ImSf21', 'ReSf19', 'ImSf19', 'ReSf17', 'ImSf17', 'ReSf15', 'ImSf15', 'ReSf13', 'ImSf13', 'ReSf11', 'ImSf11', 'ReSf9', 'ImSf9', 'ReSf7', 'ImSf7', 'ReSf5', 'ImSf5', 'ReSf3', 'ImSf3', 'ReSf1', 'ImSf1']\n",
    "\n",
    "df = pd.read_csv(file_path, skiprows=4) # we skip the first four lines, because they are just metadata\n",
    "df = df[fixed_features + labels]\n",
    "\n",
    "# remove one special row, looks very weird; ReSf1 = 2.377167465976437e-06\n",
    "df = df[df['ReSf1'] >= 1e-05]\n",
    "\n",
    "validation_size = 0.1 # 90% training, 10% for validation\n",
    "\n",
    "use_scaling = True\n",
    "\n",
    "if use_scaling:\n",
    "    feature_scaler, label_scaler = compute_scalers(df, fixed_features, labels, validation_size, seed) # make sure we use the same seed, otherwise the two splits differ!\n",
    "    dataset = ImpurityDataset(df, fixed_features, labels, feature_scaler, label_scaler, device)         \n",
    "else:\n",
    "    dataset = ImpurityDataset(df, fixed_features, labels, device=device)\n",
    "\n",
    "indices = list(range(len(dataset)))\n",
    "train_indices, val_indices = train_test_split(indices, test_size=validation_size, random_state=seed)  # make sure we use the same seed, otherwise the two splits differ!\n",
    "\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "98f8b9d2-454a-41dd-bd6e-232135263d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.7253, -0.2833, -0.1903,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 1.7253, -0.2833, -0.1903,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 1.7253, -0.2833, -0.1903,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 1.7253, -0.2833, -0.1903,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 1.7253, -0.2833, -0.1903,  ..., -1.9985,  0.0000,  0.0000],\n",
       "         [ 1.7253, -0.2833, -0.1903,  ..., -1.9985,  0.0287, -1.9653]],\n",
       "        device='cuda:0'),\n",
       " tensor([[ 1.4672e-01,  6.0201e-01],\n",
       "         [ 6.0624e-02,  8.5195e-01],\n",
       "         [ 2.4445e-02,  9.2148e-01],\n",
       "         [ 4.7469e-03,  8.9540e-01],\n",
       "         [-7.2976e-03,  8.0284e-01],\n",
       "         [-1.5029e-02,  6.6537e-01],\n",
       "         [-2.0027e-02,  5.0002e-01],\n",
       "         [-2.3156e-02,  3.1974e-01],\n",
       "         [-2.4946e-02,  1.3394e-01],\n",
       "         [-2.5747e-02, -5.0702e-02],\n",
       "         [-2.5808e-02, -2.2963e-01],\n",
       "         [-2.5312e-02, -3.9989e-01],\n",
       "         [-2.4399e-02, -5.5967e-01],\n",
       "         [-2.1736e-02, -8.4473e-01],\n",
       "         [-1.8445e-02, -1.0836e+00],\n",
       "         [-1.4916e-02, -1.2800e+00],\n",
       "         [-9.6705e-03, -1.5073e+00],\n",
       "         [-4.8277e-03, -1.6711e+00],\n",
       "         [ 7.6287e-04, -1.8191e+00],\n",
       "         [ 7.3237e-03, -1.9456e+00],\n",
       "         [ 1.2772e-02, -2.0144e+00],\n",
       "         [ 1.7432e-02, -2.0457e+00],\n",
       "         [ 2.1473e-02, -2.0485e+00],\n",
       "         [ 2.4586e-02, -2.0307e+00],\n",
       "         [ 2.7078e-02, -1.9985e+00],\n",
       "         [ 2.8663e-02, -1.9653e+00],\n",
       "         [ 2.9450e-02, -1.9431e+00]], device='cuda:0'))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23694fa-f39d-48b1-9a50-d815762d86ea",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2581363e-4b96-4ef8-bf92-5f633aca6dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncodingL(nn.Module):\n",
    "    def __init__(self, T, C, dropout):\n",
    "        super(PositionalEncodingL, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.positional_embedding = nn.Parameter(torch.zeros(T, C))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        position_encoded = self.positional_embedding[:T, :].unsqueeze(0).expand(B, -1, -1)\n",
    "        x = x + position_encoded\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c648bc1c-a7b4-4b82-bf63-e06dd0967f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    input_dim: int\n",
    "    output_dim: int\n",
    "    sequence_length: int\n",
    "    \n",
    "    d_model: int\n",
    "    nhead: int\n",
    "    num_layers: int\n",
    "    dim_feedforward: int\n",
    "    \n",
    "    dropout: float\n",
    "    activation: str\n",
    "    bias: bool\n",
    "\n",
    "class AutoregressiveTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, config, device):\n",
    "        super(AutoregressiveTransformer, self).__init__()\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        self.input_projection = nn.Linear(config.input_dim, config.d_model)\n",
    "        self.positional_encoding = PositionalEncodingL(config.sequence_length, config.d_model, config.dropout)\n",
    "        \n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=config.d_model, \n",
    "            nhead=config.nhead, \n",
    "            dim_feedforward=config.dim_feedforward, \n",
    "            dropout=config.dropout,\n",
    "            activation=config.activation, \n",
    "            batch_first=True, \n",
    "            norm_first=True, \n",
    "            bias=config.bias\n",
    "        )\n",
    "        \n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=config.num_layers)\n",
    "        self.sl1 = nn.Linear(config.d_model, config.d_model)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.output_layer = nn.Linear(config.d_model, config.output_dim)\n",
    "\n",
    "        self.att_mask = {}\n",
    "        \n",
    "    def forward(self, x):        \n",
    "        x = self.input_projection(x)        \n",
    "        x = self.positional_encoding(x)\n",
    "\n",
    "        seq_len = x.size(1)\n",
    "        if seq_len not in self.att_mask:\n",
    "            self.att_mask[seq_len] = self.generate_mask(seq_len, device)\n",
    "        \n",
    "        output = self.transformer_decoder(x, x, tgt_mask=self.att_mask[seq_len])\n",
    "        output = self.sl1(output)\n",
    "        output = self.gelu(output)\n",
    "        output = self.output_layer(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def generate_mask(self, sequence_length, device):\n",
    "        mask = torch.zeros((sequence_length, sequence_length), device=device)\n",
    "        mask = torch.triu(mask.fill_(float('-inf')), diagonal=1)\n",
    "            \n",
    "        return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b0842f-41f6-420c-b896-4b19f67805c3",
   "metadata": {},
   "source": [
    "### Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5edc082b-3d45-4afb-95b6-4d2edaa703e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1086.21 k parameters\n"
     ]
    }
   ],
   "source": [
    "config = ModelConfig(\n",
    "    input_dim = len(fixed_features) + len(labels) - 2,\n",
    "    output_dim = 2,\n",
    "    sequence_length = 27,\n",
    "    \n",
    "    d_model = 128,\n",
    "    nhead = 4,\n",
    "    num_layers = 4,\n",
    "    dim_feedforward = 128 * 4,\n",
    "    \n",
    "    dropout = 0.2,\n",
    "    activation = 'gelu',\n",
    "    bias = True\n",
    ")\n",
    "\n",
    "model = AutoregressiveTransformer(config, device).to(device)\n",
    "print(sum(p.numel() for p in model.parameters())/1e3, 'k parameters')\n",
    "criterion = nn.MSELoss().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dec32597-9f94-42fc-a936-be7eb75d331e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for inputs, targets in train_loader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7d324c9b-2a4f-4b65-abcd-7ead8d07e4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for inputs, targets in val_loader:\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec0284b-7904-4dbc-b42d-69b6d367b8d6",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b9067e6b-ea16-42e9-989b-ef6896481deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1: Train Loss: 0.034187, Train MAPE: 31.669%, Val Loss: 0.036533, Val MAPE: 39.508%, Val REG: 70.820%\n",
      "Epoch   2: Train Loss: 0.022160, Train MAPE: 26.583%, Val Loss: 0.026411, Val MAPE: 33.711%\n",
      "Epoch   3: Train Loss: 0.014780, Train MAPE: 20.774%, Val Loss: 0.021899, Val MAPE: 25.530%\n",
      "Epoch   4: Train Loss: 0.010044, Train MAPE: 17.877%, Val Loss: 0.011229, Val MAPE: 21.870%\n",
      "Epoch   5: Train Loss: 0.010025, Train MAPE: 13.797%, Val Loss: 0.012031, Val MAPE: 16.572%\n",
      "Epoch   6: Train Loss: 0.007237, Train MAPE: 13.066%, Val Loss: 0.008097, Val MAPE: 15.814%, Val REG: 52.870%\n",
      "Epoch   7: Train Loss: 0.006459, Train MAPE: 10.795%, Val Loss: 0.007004, Val MAPE: 13.067%\n",
      "Epoch   8: Train Loss: 0.006304, Train MAPE: 9.928%, Val Loss: 0.006920, Val MAPE: 11.909%\n",
      "Epoch   9: Train Loss: 0.004215, Train MAPE: 8.378%, Val Loss: 0.004761, Val MAPE: 10.091%\n",
      "Epoch  10: Train Loss: 0.004700, Train MAPE: 7.788%, Val Loss: 0.006586, Val MAPE: 9.585%\n",
      "Epoch  11: Train Loss: 0.003615, Train MAPE: 7.127%, Val Loss: 0.004040, Val MAPE: 9.622%, Val REG: 51.895%\n",
      "Epoch  12: Train Loss: 0.003345, Train MAPE: 6.892%, Val Loss: 0.003837, Val MAPE: 8.336%\n",
      "Epoch  13: Train Loss: 0.003073, Train MAPE: 5.799%, Val Loss: 0.003408, Val MAPE: 7.154%\n",
      "Epoch  14: Train Loss: 0.002588, Train MAPE: 5.882%, Val Loss: 0.002901, Val MAPE: 7.503%\n",
      "Epoch  15: Train Loss: 0.002849, Train MAPE: 4.795%, Val Loss: 0.003182, Val MAPE: 5.989%\n",
      "Epoch  16: Train Loss: 0.003078, Train MAPE: 4.840%, Val Loss: 0.003353, Val MAPE: 6.880%, Val REG: 64.822%\n",
      "Epoch  17: Train Loss: 0.003336, Train MAPE: 5.352%, Val Loss: 0.003579, Val MAPE: 6.596%\n",
      "Epoch  18: Train Loss: 0.003211, Train MAPE: 4.507%, Val Loss: 0.003706, Val MAPE: 5.470%\n",
      "Epoch  19: Train Loss: 0.002537, Train MAPE: 5.248%, Val Loss: 0.003251, Val MAPE: 6.563%\n",
      "Epoch  20: Train Loss: 0.002615, Train MAPE: 4.909%, Val Loss: 0.003268, Val MAPE: 6.199%\n",
      "Epoch  21: Train Loss: 0.002195, Train MAPE: 4.448%, Val Loss: 0.002635, Val MAPE: 5.576%, Val REG: 64.925%\n",
      "Epoch  22: Train Loss: 0.002158, Train MAPE: 4.264%, Val Loss: 0.002460, Val MAPE: 5.500%\n",
      "Epoch  23: Train Loss: 0.002233, Train MAPE: 4.228%, Val Loss: 0.002554, Val MAPE: 5.377%\n",
      "Epoch  24: Train Loss: 0.002410, Train MAPE: 4.282%, Val Loss: 0.003117, Val MAPE: 5.062%\n",
      "Epoch  25: Train Loss: 0.002272, Train MAPE: 4.514%, Val Loss: 0.002604, Val MAPE: 5.623%\n",
      "Epoch  26: Train Loss: 0.002140, Train MAPE: 4.274%, Val Loss: 0.002610, Val MAPE: 5.469%, Val REG: 66.286%\n",
      "Epoch  27: Train Loss: 0.002091, Train MAPE: 3.409%, Val Loss: 0.002445, Val MAPE: 4.129%\n",
      "Epoch  28: Train Loss: 0.001595, Train MAPE: 3.160%, Val Loss: 0.001945, Val MAPE: 3.890%\n",
      "Epoch  29: Train Loss: 0.001883, Train MAPE: 4.433%, Val Loss: 0.002080, Val MAPE: 4.880%\n",
      "Epoch  30: Train Loss: 0.001366, Train MAPE: 3.105%, Val Loss: 0.002203, Val MAPE: 3.747%\n",
      "Epoch  31: Train Loss: 0.002864, Train MAPE: 4.361%, Val Loss: 0.003117, Val MAPE: 5.098%, Val REG: 65.537%\n",
      "Epoch  32: Train Loss: 0.001313, Train MAPE: 3.416%, Val Loss: 0.001555, Val MAPE: 4.249%\n",
      "Epoch  33: Train Loss: 0.001835, Train MAPE: 4.446%, Val Loss: 0.002186, Val MAPE: 5.514%\n",
      "Epoch  34: Train Loss: 0.001869, Train MAPE: 3.317%, Val Loss: 0.002135, Val MAPE: 4.072%\n",
      "Epoch  35: Train Loss: 0.001492, Train MAPE: 4.014%, Val Loss: 0.001623, Val MAPE: 5.136%\n",
      "Epoch  36: Train Loss: 0.001664, Train MAPE: 3.398%, Val Loss: 0.001817, Val MAPE: 4.294%, Val REG: 68.761%\n",
      "Epoch  37: Train Loss: 0.001448, Train MAPE: 3.064%, Val Loss: 0.001617, Val MAPE: 3.613%\n",
      "Epoch  38: Train Loss: 0.001722, Train MAPE: 4.070%, Val Loss: 0.001938, Val MAPE: 4.717%\n",
      "Epoch  39: Train Loss: 0.001871, Train MAPE: 3.517%, Val Loss: 0.002146, Val MAPE: 4.183%\n",
      "Epoch  40: Train Loss: 0.001930, Train MAPE: 3.116%, Val Loss: 0.002191, Val MAPE: 3.540%\n",
      "Epoch  41: Train Loss: 0.001797, Train MAPE: 2.935%, Val Loss: 0.001998, Val MAPE: 3.397%, Val REG: 67.197%\n",
      "Epoch  42: Train Loss: 0.001341, Train MAPE: 3.819%, Val Loss: 0.001525, Val MAPE: 4.825%\n",
      "Epoch  43: Train Loss: 0.001110, Train MAPE: 2.500%, Val Loss: 0.001303, Val MAPE: 2.928%\n",
      "Epoch  44: Train Loss: 0.001243, Train MAPE: 2.912%, Val Loss: 0.001561, Val MAPE: 3.577%\n",
      "Epoch  45: Train Loss: 0.001243, Train MAPE: 2.608%, Val Loss: 0.001440, Val MAPE: 2.981%\n",
      "Epoch  46: Train Loss: 0.000854, Train MAPE: 2.324%, Val Loss: 0.000971, Val MAPE: 2.678%, Val REG: 62.546%\n",
      "Epoch  47: Train Loss: 0.001385, Train MAPE: 2.474%, Val Loss: 0.001549, Val MAPE: 2.894%\n",
      "Epoch  48: Train Loss: 0.001257, Train MAPE: 2.436%, Val Loss: 0.001424, Val MAPE: 2.820%\n",
      "Epoch  49: Train Loss: 0.001848, Train MAPE: 2.382%, Val Loss: 0.001947, Val MAPE: 2.758%\n",
      "Epoch  50: Train Loss: 0.001218, Train MAPE: 3.067%, Val Loss: 0.001369, Val MAPE: 3.831%\n",
      "Epoch  51: Train Loss: 0.001251, Train MAPE: 2.472%, Val Loss: 0.001389, Val MAPE: 2.743%, Val REG: 62.162%\n",
      "Epoch  52: Train Loss: 0.001295, Train MAPE: 2.365%, Val Loss: 0.001542, Val MAPE: 2.817%\n",
      "Epoch  53: Train Loss: 0.001287, Train MAPE: 2.841%, Val Loss: 0.001521, Val MAPE: 3.407%\n",
      "Epoch  54: Train Loss: 0.000833, Train MAPE: 2.454%, Val Loss: 0.000951, Val MAPE: 2.743%\n",
      "Epoch  55: Train Loss: 0.000939, Train MAPE: 2.618%, Val Loss: 0.001037, Val MAPE: 2.949%\n",
      "Epoch  56: Train Loss: 0.001390, Train MAPE: 2.639%, Val Loss: 0.001564, Val MAPE: 3.115%, Val REG: 57.959%\n",
      "Epoch  57: Train Loss: 0.001238, Train MAPE: 2.401%, Val Loss: 0.001460, Val MAPE: 2.796%\n",
      "Epoch  58: Train Loss: 0.001142, Train MAPE: 2.221%, Val Loss: 0.001212, Val MAPE: 2.463%\n",
      "Epoch  59: Train Loss: 0.001049, Train MAPE: 2.492%, Val Loss: 0.001144, Val MAPE: 2.812%\n",
      "Epoch  60: Train Loss: 0.001084, Train MAPE: 2.424%, Val Loss: 0.001193, Val MAPE: 2.585%\n",
      "Epoch  61: Train Loss: 0.000939, Train MAPE: 2.431%, Val Loss: 0.001071, Val MAPE: 2.943%, Val REG: 56.464%\n",
      "Epoch  62: Train Loss: 0.001104, Train MAPE: 3.441%, Val Loss: 0.001279, Val MAPE: 4.063%\n",
      "Epoch  63: Train Loss: 0.001181, Train MAPE: 2.234%, Val Loss: 0.001277, Val MAPE: 2.422%\n",
      "Epoch  64: Train Loss: 0.001396, Train MAPE: 2.282%, Val Loss: 0.001509, Val MAPE: 2.439%\n",
      "Epoch  65: Train Loss: 0.001382, Train MAPE: 2.381%, Val Loss: 0.001496, Val MAPE: 2.618%\n",
      "Epoch  66: Train Loss: 0.001182, Train MAPE: 2.190%, Val Loss: 0.001370, Val MAPE: 2.424%, Val REG: 57.069%\n",
      "Epoch  67: Train Loss: 0.001129, Train MAPE: 2.167%, Val Loss: 0.001163, Val MAPE: 2.486%\n",
      "Epoch  68: Train Loss: 0.001191, Train MAPE: 2.097%, Val Loss: 0.001266, Val MAPE: 2.294%\n",
      "Epoch  69: Train Loss: 0.001313, Train MAPE: 2.263%, Val Loss: 0.001374, Val MAPE: 2.551%\n",
      "Epoch  70: Train Loss: 0.000867, Train MAPE: 1.769%, Val Loss: 0.001003, Val MAPE: 1.945%\n",
      "Epoch  71: Train Loss: 0.001296, Train MAPE: 2.392%, Val Loss: 0.001351, Val MAPE: 2.585%, Val REG: 56.536%\n",
      "Epoch  72: Train Loss: 0.000852, Train MAPE: 2.585%, Val Loss: 0.000989, Val MAPE: 2.921%\n",
      "Epoch  73: Train Loss: 0.001135, Train MAPE: 2.099%, Val Loss: 0.001224, Val MAPE: 2.329%\n",
      "Epoch  74: Train Loss: 0.000834, Train MAPE: 2.223%, Val Loss: 0.000919, Val MAPE: 2.481%\n",
      "Epoch  75: Train Loss: 0.000887, Train MAPE: 1.911%, Val Loss: 0.000959, Val MAPE: 2.082%\n",
      "Epoch  76: Train Loss: 0.001003, Train MAPE: 1.995%, Val Loss: 0.001109, Val MAPE: 2.224%, Val REG: 52.850%\n",
      "Epoch  77: Train Loss: 0.001250, Train MAPE: 2.275%, Val Loss: 0.001433, Val MAPE: 2.462%\n",
      "Epoch  78: Train Loss: 0.001270, Train MAPE: 2.276%, Val Loss: 0.001358, Val MAPE: 2.640%\n",
      "Epoch  79: Train Loss: 0.001049, Train MAPE: 2.938%, Val Loss: 0.001113, Val MAPE: 3.348%\n",
      "Epoch  80: Train Loss: 0.001266, Train MAPE: 2.419%, Val Loss: 0.001528, Val MAPE: 2.940%\n",
      "Epoch  81: Train Loss: 0.001623, Train MAPE: 2.525%, Val Loss: 0.001806, Val MAPE: 2.862%, Val REG: 52.287%\n",
      "Epoch  82: Train Loss: 0.001202, Train MAPE: 2.177%, Val Loss: 0.001360, Val MAPE: 2.611%\n",
      "Epoch  83: Train Loss: 0.001819, Train MAPE: 2.046%, Val Loss: 0.001885, Val MAPE: 2.279%\n",
      "Epoch  84: Train Loss: 0.000957, Train MAPE: 2.024%, Val Loss: 0.001101, Val MAPE: 2.252%\n",
      "Epoch  85: Train Loss: 0.001941, Train MAPE: 2.252%, Val Loss: 0.002030, Val MAPE: 2.487%\n",
      "Epoch  86: Train Loss: 0.001072, Train MAPE: 2.056%, Val Loss: 0.001254, Val MAPE: 2.207%, Val REG: 53.956%\n",
      "Epoch  87: Train Loss: 0.001699, Train MAPE: 2.569%, Val Loss: 0.001887, Val MAPE: 2.932%\n",
      "Epoch  88: Train Loss: 0.000932, Train MAPE: 2.638%, Val Loss: 0.001034, Val MAPE: 3.245%\n",
      "Epoch  89: Train Loss: 0.001331, Train MAPE: 2.472%, Val Loss: 0.001508, Val MAPE: 2.739%\n",
      "Epoch  90: Train Loss: 0.001003, Train MAPE: 1.744%, Val Loss: 0.001104, Val MAPE: 1.968%\n",
      "Epoch  91: Train Loss: 0.001769, Train MAPE: 2.219%, Val Loss: 0.001856, Val MAPE: 2.521%, Val REG: 52.441%\n",
      "Epoch  92: Train Loss: 0.001691, Train MAPE: 2.090%, Val Loss: 0.001878, Val MAPE: 2.346%\n",
      "Epoch  93: Train Loss: 0.000778, Train MAPE: 2.140%, Val Loss: 0.000956, Val MAPE: 2.463%\n",
      "Epoch  94: Train Loss: 0.001611, Train MAPE: 1.988%, Val Loss: 0.001653, Val MAPE: 2.174%\n",
      "Epoch  95: Train Loss: 0.001236, Train MAPE: 2.046%, Val Loss: 0.001302, Val MAPE: 2.286%\n",
      "Epoch  96: Train Loss: 0.001303, Train MAPE: 2.156%, Val Loss: 0.001395, Val MAPE: 2.421%, Val REG: 54.050%\n",
      "Epoch  97: Train Loss: 0.000860, Train MAPE: 3.052%, Val Loss: 0.000951, Val MAPE: 3.364%\n",
      "Epoch  98: Train Loss: 0.001138, Train MAPE: 2.278%, Val Loss: 0.001247, Val MAPE: 2.554%\n",
      "Epoch  99: Train Loss: 0.000897, Train MAPE: 1.842%, Val Loss: 0.000925, Val MAPE: 2.108%\n",
      "Epoch 100: Train Loss: 0.001325, Train MAPE: 2.156%, Val Loss: 0.001461, Val MAPE: 2.407%\n",
      "Epoch 101: Train Loss: 0.001119, Train MAPE: 2.424%, Val Loss: 0.001228, Val MAPE: 2.796%, Val REG: 52.821%\n",
      "Epoch 102: Train Loss: 0.000867, Train MAPE: 1.985%, Val Loss: 0.000917, Val MAPE: 2.259%\n",
      "Epoch 103: Train Loss: 0.000792, Train MAPE: 2.139%, Val Loss: 0.000876, Val MAPE: 2.416%\n",
      "Epoch 104: Train Loss: 0.000896, Train MAPE: 1.851%, Val Loss: 0.001048, Val MAPE: 2.048%\n",
      "Epoch 105: Train Loss: 0.001116, Train MAPE: 2.007%, Val Loss: 0.001161, Val MAPE: 2.187%\n",
      "Epoch 106: Train Loss: 0.000870, Train MAPE: 1.897%, Val Loss: 0.000958, Val MAPE: 2.173%, Val REG: 53.710%\n",
      "Epoch 107: Train Loss: 0.001093, Train MAPE: 2.162%, Val Loss: 0.001192, Val MAPE: 2.441%\n",
      "Epoch 108: Train Loss: 0.000882, Train MAPE: 2.441%, Val Loss: 0.000952, Val MAPE: 2.866%\n",
      "Epoch 109: Train Loss: 0.001027, Train MAPE: 2.082%, Val Loss: 0.001091, Val MAPE: 2.378%\n",
      "Epoch 110: Train Loss: 0.000738, Train MAPE: 1.643%, Val Loss: 0.000802, Val MAPE: 1.862%\n",
      "Epoch 111: Train Loss: 0.000791, Train MAPE: 1.741%, Val Loss: 0.001016, Val MAPE: 1.960%, Val REG: 52.972%\n",
      "Epoch 112: Train Loss: 0.001621, Train MAPE: 2.099%, Val Loss: 0.001735, Val MAPE: 2.293%\n",
      "Epoch 113: Train Loss: 0.001615, Train MAPE: 2.465%, Val Loss: 0.001611, Val MAPE: 2.684%\n",
      "Epoch 114: Train Loss: 0.001402, Train MAPE: 2.268%, Val Loss: 0.001471, Val MAPE: 2.433%\n",
      "Epoch 115: Train Loss: 0.001239, Train MAPE: 2.183%, Val Loss: 0.001395, Val MAPE: 2.406%\n",
      "Epoch 116: Train Loss: 0.001149, Train MAPE: 2.024%, Val Loss: 0.001232, Val MAPE: 2.185%, Val REG: 52.442%\n",
      "Epoch 117: Train Loss: 0.001139, Train MAPE: 1.671%, Val Loss: 0.001465, Val MAPE: 1.932%\n",
      "Epoch 118: Train Loss: 0.001063, Train MAPE: 2.185%, Val Loss: 0.001200, Val MAPE: 2.426%\n",
      "Epoch 119: Train Loss: 0.001448, Train MAPE: 1.912%, Val Loss: 0.001532, Val MAPE: 2.246%\n",
      "Epoch 120: Train Loss: 0.001168, Train MAPE: 2.253%, Val Loss: 0.001250, Val MAPE: 2.607%\n",
      "Epoch 121: Train Loss: 0.001428, Train MAPE: 2.020%, Val Loss: 0.001443, Val MAPE: 2.194%, Val REG: 51.380%\n",
      "Epoch 122: Train Loss: 0.001344, Train MAPE: 1.936%, Val Loss: 0.001423, Val MAPE: 2.161%\n",
      "Epoch 123: Train Loss: 0.000861, Train MAPE: 1.815%, Val Loss: 0.000948, Val MAPE: 2.067%\n",
      "Epoch 124: Train Loss: 0.001214, Train MAPE: 1.934%, Val Loss: 0.001276, Val MAPE: 2.202%\n",
      "Epoch 125: Train Loss: 0.000906, Train MAPE: 1.660%, Val Loss: 0.000994, Val MAPE: 1.837%\n",
      "Epoch 126: Train Loss: 0.001326, Train MAPE: 1.927%, Val Loss: 0.001350, Val MAPE: 2.224%, Val REG: 50.897%\n",
      "Epoch 127: Train Loss: 0.001391, Train MAPE: 1.897%, Val Loss: 0.001504, Val MAPE: 2.175%\n",
      "Epoch 128: Train Loss: 0.001067, Train MAPE: 2.458%, Val Loss: 0.001121, Val MAPE: 2.762%\n",
      "Epoch 129: Train Loss: 0.001042, Train MAPE: 1.895%, Val Loss: 0.001164, Val MAPE: 2.171%\n",
      "Epoch 130: Train Loss: 0.001071, Train MAPE: 1.746%, Val Loss: 0.001159, Val MAPE: 1.962%\n",
      "Epoch 131: Train Loss: 0.001206, Train MAPE: 2.053%, Val Loss: 0.001270, Val MAPE: 2.266%, Val REG: 53.107%\n",
      "Epoch 132: Train Loss: 0.001464, Train MAPE: 2.025%, Val Loss: 0.001534, Val MAPE: 2.316%\n",
      "Epoch 133: Train Loss: 0.001133, Train MAPE: 2.273%, Val Loss: 0.001161, Val MAPE: 2.668%\n",
      "Epoch 134: Train Loss: 0.001212, Train MAPE: 1.853%, Val Loss: 0.001273, Val MAPE: 2.015%\n",
      "Epoch 135: Train Loss: 0.001658, Train MAPE: 1.994%, Val Loss: 0.001709, Val MAPE: 2.141%\n",
      "Epoch 136: Train Loss: 0.000959, Train MAPE: 2.066%, Val Loss: 0.001187, Val MAPE: 2.373%, Val REG: 50.807%\n",
      "Epoch 137: Train Loss: 0.000892, Train MAPE: 1.860%, Val Loss: 0.001146, Val MAPE: 2.016%\n",
      "Epoch 138: Train Loss: 0.001455, Train MAPE: 1.999%, Val Loss: 0.001593, Val MAPE: 2.352%\n",
      "Epoch 139: Train Loss: 0.001320, Train MAPE: 2.055%, Val Loss: 0.001572, Val MAPE: 2.250%\n",
      "Epoch 140: Train Loss: 0.001167, Train MAPE: 1.836%, Val Loss: 0.001152, Val MAPE: 1.995%\n",
      "Epoch 141: Train Loss: 0.000905, Train MAPE: 1.785%, Val Loss: 0.000942, Val MAPE: 2.047%, Val REG: 51.427%\n",
      "Epoch 142: Train Loss: 0.001272, Train MAPE: 1.831%, Val Loss: 0.001485, Val MAPE: 2.278%\n",
      "Epoch 143: Train Loss: 0.001511, Train MAPE: 2.311%, Val Loss: 0.001573, Val MAPE: 2.609%\n",
      "Epoch 144: Train Loss: 0.000867, Train MAPE: 1.708%, Val Loss: 0.000967, Val MAPE: 1.888%\n",
      "Epoch 145: Train Loss: 0.001378, Train MAPE: 2.159%, Val Loss: 0.001504, Val MAPE: 2.383%\n",
      "Epoch 146: Train Loss: 0.001114, Train MAPE: 1.900%, Val Loss: 0.001141, Val MAPE: 2.064%, Val REG: 51.910%\n",
      "Epoch 147: Train Loss: 0.001812, Train MAPE: 2.405%, Val Loss: 0.001896, Val MAPE: 2.635%\n",
      "Epoch 148: Train Loss: 0.002638, Train MAPE: 2.499%, Val Loss: 0.004730, Val MAPE: 2.795%\n",
      "Epoch 149: Train Loss: 0.001498, Train MAPE: 1.943%, Val Loss: 0.001571, Val MAPE: 2.112%\n",
      "Epoch 150: Train Loss: 0.001031, Train MAPE: 1.716%, Val Loss: 0.001052, Val MAPE: 1.921%\n",
      "Epoch 151: Train Loss: 0.000998, Train MAPE: 2.277%, Val Loss: 0.001054, Val MAPE: 2.464%, Val REG: 52.753%\n",
      "Epoch 152: Train Loss: 0.001345, Train MAPE: 2.041%, Val Loss: 0.001506, Val MAPE: 2.302%\n",
      "Epoch 153: Train Loss: 0.002049, Train MAPE: 2.714%, Val Loss: 0.002037, Val MAPE: 2.916%\n",
      "Epoch 154: Train Loss: 0.001595, Train MAPE: 2.079%, Val Loss: 0.001643, Val MAPE: 2.265%\n",
      "Epoch 155: Train Loss: 0.001395, Train MAPE: 2.012%, Val Loss: 0.001427, Val MAPE: 2.240%\n",
      "Epoch 156: Train Loss: 0.001252, Train MAPE: 2.664%, Val Loss: 0.001384, Val MAPE: 3.236%, Val REG: 51.097%\n",
      "Epoch 157: Train Loss: 0.001401, Train MAPE: 1.973%, Val Loss: 0.001465, Val MAPE: 2.175%\n",
      "Epoch 158: Train Loss: 0.001456, Train MAPE: 2.002%, Val Loss: 0.001547, Val MAPE: 2.242%\n",
      "Epoch 159: Train Loss: 0.001268, Train MAPE: 2.042%, Val Loss: 0.001285, Val MAPE: 2.214%\n",
      "Epoch 160: Train Loss: 0.001566, Train MAPE: 2.235%, Val Loss: 0.001682, Val MAPE: 2.418%\n",
      "Epoch 161: Train Loss: 0.001941, Train MAPE: 1.981%, Val Loss: 0.001991, Val MAPE: 2.184%, Val REG: 50.554%\n",
      "Epoch 162: Train Loss: 0.001379, Train MAPE: 2.142%, Val Loss: 0.001474, Val MAPE: 2.643%\n",
      "Epoch 163: Train Loss: 0.001126, Train MAPE: 1.758%, Val Loss: 0.001173, Val MAPE: 2.019%\n",
      "Epoch 164: Train Loss: 0.001105, Train MAPE: 2.099%, Val Loss: 0.001126, Val MAPE: 2.321%\n",
      "Epoch 165: Train Loss: 0.001417, Train MAPE: 1.971%, Val Loss: 0.001470, Val MAPE: 2.194%\n",
      "Epoch 166: Train Loss: 0.001091, Train MAPE: 1.819%, Val Loss: 0.001180, Val MAPE: 2.018%, Val REG: 51.105%\n",
      "Epoch 167: Train Loss: 0.001450, Train MAPE: 2.094%, Val Loss: 0.001569, Val MAPE: 2.324%\n",
      "Epoch 168: Train Loss: 0.001703, Train MAPE: 1.961%, Val Loss: 0.001792, Val MAPE: 2.187%\n",
      "Epoch 169: Train Loss: 0.001363, Train MAPE: 1.857%, Val Loss: 0.001382, Val MAPE: 2.023%\n",
      "Epoch 170: Train Loss: 0.000952, Train MAPE: 1.504%, Val Loss: 0.000968, Val MAPE: 1.709%\n",
      "Epoch 171: Train Loss: 0.001319, Train MAPE: 2.453%, Val Loss: 0.001333, Val MAPE: 2.737%, Val REG: 54.231%\n",
      "Epoch 172: Train Loss: 0.001167, Train MAPE: 1.820%, Val Loss: 0.001196, Val MAPE: 1.996%\n",
      "Epoch 173: Train Loss: 0.001188, Train MAPE: 1.840%, Val Loss: 0.001292, Val MAPE: 2.116%\n",
      "Epoch 174: Train Loss: 0.001528, Train MAPE: 1.773%, Val Loss: 0.001552, Val MAPE: 1.978%\n",
      "Epoch 175: Train Loss: 0.001318, Train MAPE: 1.756%, Val Loss: 0.001568, Val MAPE: 1.951%\n",
      "Epoch 176: Train Loss: 0.001414, Train MAPE: 1.705%, Val Loss: 0.001999, Val MAPE: 1.978%, Val REG: 50.972%\n",
      "Epoch 177: Train Loss: 0.001724, Train MAPE: 2.299%, Val Loss: 0.001853, Val MAPE: 2.504%\n",
      "Epoch 178: Train Loss: 0.001051, Train MAPE: 2.199%, Val Loss: 0.001223, Val MAPE: 2.516%\n",
      "Epoch 179: Train Loss: 0.000977, Train MAPE: 1.642%, Val Loss: 0.001014, Val MAPE: 1.842%\n",
      "Epoch 180: Train Loss: 0.000634, Train MAPE: 1.428%, Val Loss: 0.000722, Val MAPE: 1.622%\n",
      "Epoch 181: Train Loss: 0.001727, Train MAPE: 2.018%, Val Loss: 0.001775, Val MAPE: 2.221%, Val REG: 51.603%\n",
      "Epoch 182: Train Loss: 0.001153, Train MAPE: 1.902%, Val Loss: 0.001276, Val MAPE: 2.142%\n",
      "Epoch 183: Train Loss: 0.001183, Train MAPE: 1.820%, Val Loss: 0.001367, Val MAPE: 2.031%\n",
      "Epoch 184: Train Loss: 0.001655, Train MAPE: 2.106%, Val Loss: 0.001755, Val MAPE: 2.264%\n",
      "Epoch 185: Train Loss: 0.001470, Train MAPE: 2.019%, Val Loss: 0.001520, Val MAPE: 2.248%\n",
      "Epoch 186: Train Loss: 0.001326, Train MAPE: 1.911%, Val Loss: 0.001360, Val MAPE: 2.136%, Val REG: 52.520%\n",
      "Epoch 187: Train Loss: 0.000837, Train MAPE: 1.689%, Val Loss: 0.000913, Val MAPE: 1.870%\n",
      "Epoch 188: Train Loss: 0.001125, Train MAPE: 2.111%, Val Loss: 0.001296, Val MAPE: 2.333%\n",
      "Epoch 189: Train Loss: 0.001529, Train MAPE: 2.362%, Val Loss: 0.001560, Val MAPE: 2.770%\n",
      "Epoch 190: Train Loss: 0.001460, Train MAPE: 2.459%, Val Loss: 0.001552, Val MAPE: 2.666%\n",
      "Epoch 191: Train Loss: 0.001518, Train MAPE: 2.611%, Val Loss: 0.001984, Val MAPE: 2.871%, Val REG: 53.651%\n",
      "Epoch 192: Train Loss: 0.001097, Train MAPE: 1.942%, Val Loss: 0.001162, Val MAPE: 2.066%\n",
      "Epoch 193: Train Loss: 0.001498, Train MAPE: 1.718%, Val Loss: 0.001713, Val MAPE: 1.914%\n",
      "Epoch 194: Train Loss: 0.001071, Train MAPE: 1.647%, Val Loss: 0.001068, Val MAPE: 1.828%\n",
      "Epoch 195: Train Loss: 0.001725, Train MAPE: 2.092%, Val Loss: 0.001818, Val MAPE: 2.332%\n",
      "Epoch 196: Train Loss: 0.001556, Train MAPE: 2.531%, Val Loss: 0.001546, Val MAPE: 2.840%, Val REG: 53.972%\n",
      "Epoch 197: Train Loss: 0.001176, Train MAPE: 1.981%, Val Loss: 0.001202, Val MAPE: 2.161%\n",
      "Epoch 198: Train Loss: 0.001935, Train MAPE: 2.081%, Val Loss: 0.001975, Val MAPE: 2.262%\n",
      "Epoch 199: Train Loss: 0.001251, Train MAPE: 1.974%, Val Loss: 0.001265, Val MAPE: 2.144%\n",
      "Epoch 200: Train Loss: 0.001060, Train MAPE: 1.862%, Val Loss: 0.001060, Val MAPE: 1.976%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train(model, train_loader, optimizer, criterion, device)\n",
    "    train_loss = validate(model, train_loader, criterion, device)\n",
    "    val_loss = validate(model, val_loader, criterion, device)\n",
    "    train_mape = validate_mape(model, train_loader, device, use_scaling=use_scaling)\n",
    "    val_mape = validate_mape(model, val_loader, device, use_scaling=use_scaling)\n",
    "\n",
    "    if epoch%5 == 0:\n",
    "        val_reg = autoregressive_error(100, 27, 0, len(fixed_features), len(fixed_features) + len(labels) - 2, val_loader, use_scaling)\n",
    "        print(f\"Epoch {(epoch+1):3d}: Train Loss: {train_loss:.6f}, Train MAPE: {train_mape:.3f}%, Val Loss: {val_loss:.6f}, Val MAPE: {val_mape:.3f}%, Val REG: {val_reg:.3f}%\")\n",
    "        if val_reg <= 4:\n",
    "            break\n",
    "    else:\n",
    "        print(f\"Epoch {(epoch+1):3d}: Train Loss: {train_loss:.6f}, Train MAPE: {train_mape:.3f}%, Val Loss: {val_loss:.6f}, Val MAPE: {val_mape:.3f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c375fecd-b79b-4a84-b5de-78ea7ed0d24b",
   "metadata": {},
   "source": [
    "### Manual model validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f72c8c95-6a90-4792-a74e-302534c87e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_mape(model, loader, device, use_scaling=use_scaling, epsilon=1e-8):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for inputs, targets in loader:\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "\n",
    "            if use_scaling:\n",
    "                outputs = torch.tensor(reverse_tranform_output(outputs, label_scaler))\n",
    "                targets = torch.tensor(reverse_tranform_output(targets, label_scaler))\n",
    "            \n",
    "            ape = torch.abs((targets - outputs) / (targets + epsilon))\n",
    "            mape = torch.mean(ape) * 100\n",
    "                    \n",
    "            total_loss += mape.item()\n",
    "    \n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "aa4537b0-7c15-4d74-85bf-65d053516262",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_tranform_output(data, scaler):\n",
    "    B, T, C = data.shape\n",
    "    data = data.view(B, T*C)\n",
    "    data = data.cpu().numpy()\n",
    "    return scaler.inverse_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "45952c7e-e2d3-4aff-8810-7fd7693e104d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9841176383197308"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_mape(model, val_loader, device, use_scaling=use_scaling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953a0d1d-5db4-417d-8fed-c44ca4256892",
   "metadata": {},
   "source": [
    "### Sample from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "850f5ca1-bfd3-4716-9088-996e14a68c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_model(model, input, sequence_length, fixed_feature_len, fixed_labels_len, max_features, device):\n",
    "    model.eval()\n",
    "\n",
    "    fixed_features = input[:fixed_feature_len]\n",
    "    fixed_labels = input[fixed_feature_len:fixed_labels_len*2 + fixed_feature_len]\n",
    "    \n",
    "    assert len(fixed_labels) % 2 == 0\n",
    "    provided_sequences = len(fixed_labels) // 2\n",
    "    \n",
    "    initial_input = torch.zeros(1, provided_sequences+1, max_features)\n",
    "    initial_input[0, 0, :fixed_feature_len] = fixed_features\n",
    "\n",
    "    outputs = torch.zeros(sequence_length, 2, device = device)\n",
    "    \n",
    "    for i in range(1, provided_sequences + 1):\n",
    "        labels = fixed_labels[(i-1)*2:2*i]\n",
    "        initial_input[0, i, :2*i + fixed_feature_len] = initial_input[0, i-1, :2*i + fixed_feature_len]\n",
    "        initial_input[0, i, 2*(i-1) + fixed_feature_len:2*i + fixed_feature_len] = labels\n",
    "        outputs[i-1, :] = labels\n",
    "\n",
    "    current_input = initial_input.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i in range(fixed_labels_len + 1, sequence_length + 1):\n",
    "\n",
    "            output = model(current_input)\n",
    "            predictions = output[0, -1, :]\n",
    "            \n",
    "            outputs[i-1, :] = predictions\n",
    "            \n",
    "            if i == sequence_length:\n",
    "                break\n",
    "            \n",
    "            position_to_insert = i * 2 + fixed_feature_len\n",
    "            temp = torch.zeros(1, 1, max_features, device=device)\n",
    "            temp[0, 0, :position_to_insert-2] = current_input[0, i-1, :position_to_insert-2]\n",
    "            temp[0, 0, position_to_insert-2:position_to_insert] = predictions\n",
    "            current_input = torch.cat((current_input, temp), dim=1)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ea04c086-2a30-4939-8b0f-8f2a453338e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mape(true_values, predictions, start, epsilon=1e-8):\n",
    "    true_values = true_values[:, start:]\n",
    "    predictions = predictions[:, start:]\n",
    "    \n",
    "    mape = torch.mean(torch.abs((true_values - predictions) / (true_values + epsilon)))* 100\n",
    "    return mape.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5a32d364-08a1-4e3c-8b47-d93f79db3b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoregressive_error(n_samples, sequence_length, fixed_labels_len, fixed_features_len, max_features, loader, use_scaling):\n",
    "    mapes = []\n",
    "    \n",
    "    for idx in range(n_samples):\n",
    "        input = loader.dataset[idx][0][fixed_labels_len]\n",
    "        \n",
    "        outputs = sample_from_model(model, input, sequence_length, fixed_features_len, fixed_labels_len, max_features, device)\n",
    "        targets = loader.dataset[idx][1]\n",
    "    \n",
    "        T, C = outputs.shape\n",
    "        outputs = outputs.view(1, T, C)\n",
    "        targets = targets.view(1, T, C)\n",
    "        \n",
    "        if use_scaling:\n",
    "            outputs = torch.tensor(reverse_tranform_output(outputs, label_scaler))\n",
    "            targets = torch.tensor(reverse_tranform_output(targets, label_scaler))\n",
    "        \n",
    "        mapes.append(calculate_mape(targets, outputs, fixed_labels_len))\n",
    "\n",
    "    return np.mean(mapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "74ca0b4c-cdb5-4dde-ac86-0c3744f1ebb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.019063957929611"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples = 100\n",
    "sequence_length = 27\n",
    "fixed_labels_len = 15\n",
    "fixed_features_len = len(fixed_features)\n",
    "max_features = fixed_features_len + len(labels) - 2\n",
    "loader = val_loader\n",
    "\n",
    "autoregressive_error(n_samples, sequence_length, fixed_labels_len, fixed_features_len, max_features, loader, use_scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f98c960f-1bbe-44da-8d37-f26fae79bd6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[45.941008078500296]\n",
      "[45.941008078500296, 39.60480784218556]\n",
      "[45.941008078500296, 39.60480784218556, 31.571197920881765]\n",
      "[45.941008078500296, 39.60480784218556, 31.571197920881765, 26.786610456299734]\n",
      "[45.941008078500296, 39.60480784218556, 31.571197920881765, 26.786610456299734, 23.945855725938884]\n",
      "[45.941008078500296, 39.60480784218556, 31.571197920881765, 26.786610456299734, 23.945855725938884, 21.818009250360717]\n",
      "[45.941008078500296, 39.60480784218556, 31.571197920881765, 26.786610456299734, 23.945855725938884, 21.818009250360717, 19.76058903054692]\n",
      "[45.941008078500296, 39.60480784218556, 31.571197920881765, 26.786610456299734, 23.945855725938884, 21.818009250360717, 19.76058903054692, 17.23577772023932]\n",
      "[45.941008078500296, 39.60480784218556, 31.571197920881765, 26.786610456299734, 23.945855725938884, 21.818009250360717, 19.76058903054692, 17.23577772023932, 15.24018933175795]\n",
      "[45.941008078500296, 39.60480784218556, 31.571197920881765, 26.786610456299734, 23.945855725938884, 21.818009250360717, 19.76058903054692, 17.23577772023932, 15.24018933175795, 13.59838957350019]\n",
      "[45.941008078500296, 39.60480784218556, 31.571197920881765, 26.786610456299734, 23.945855725938884, 21.818009250360717, 19.76058903054692, 17.23577772023932, 15.24018933175795, 13.59838957350019, 12.10429064884512]\n",
      "[45.941008078500296, 39.60480784218556, 31.571197920881765, 26.786610456299734, 23.945855725938884, 21.818009250360717, 19.76058903054692, 17.23577772023932, 15.24018933175795, 13.59838957350019, 12.10429064884512, 10.37251573603158]\n",
      "[45.941008078500296, 39.60480784218556, 31.571197920881765, 26.786610456299734, 23.945855725938884, 21.818009250360717, 19.76058903054692, 17.23577772023932, 15.24018933175795, 13.59838957350019, 12.10429064884512, 10.37251573603158, 9.053465744799052]\n",
      "[45.941008078500296, 39.60480784218556, 31.571197920881765, 26.786610456299734, 23.945855725938884, 21.818009250360717, 19.76058903054692, 17.23577772023932, 15.24018933175795, 13.59838957350019, 12.10429064884512, 10.37251573603158, 9.053465744799052, 7.750980697317383]\n",
      "[45.941008078500296, 39.60480784218556, 31.571197920881765, 26.786610456299734, 23.945855725938884, 21.818009250360717, 19.76058903054692, 17.23577772023932, 15.24018933175795, 13.59838957350019, 12.10429064884512, 10.37251573603158, 9.053465744799052, 7.750980697317383, 6.4278397342748566]\n",
      "[45.941008078500296, 39.60480784218556, 31.571197920881765, 26.786610456299734, 23.945855725938884, 21.818009250360717, 19.76058903054692, 17.23577772023932, 15.24018933175795, 13.59838957350019, 12.10429064884512, 10.37251573603158, 9.053465744799052, 7.750980697317383, 6.4278397342748566, 5.209496619865688]\n",
      "[45.941008078500296, 39.60480784218556, 31.571197920881765, 26.786610456299734, 23.945855725938884, 21.818009250360717, 19.76058903054692, 17.23577772023932, 15.24018933175795, 13.59838957350019, 12.10429064884512, 10.37251573603158, 9.053465744799052, 7.750980697317383, 6.4278397342748566, 5.209496619865688, 4.181144711312274]\n",
      "[45.941008078500296, 39.60480784218556, 31.571197920881765, 26.786610456299734, 23.945855725938884, 21.818009250360717, 19.76058903054692, 17.23577772023932, 15.24018933175795, 13.59838957350019, 12.10429064884512, 10.37251573603158, 9.053465744799052, 7.750980697317383, 6.4278397342748566, 5.209496619865688, 4.181144711312274, 3.2995101904995003]\n",
      "[45.941008078500296, 39.60480784218556, 31.571197920881765, 26.786610456299734, 23.945855725938884, 21.818009250360717, 19.76058903054692, 17.23577772023932, 15.24018933175795, 13.59838957350019, 12.10429064884512, 10.37251573603158, 9.053465744799052, 7.750980697317383, 6.4278397342748566, 5.209496619865688, 4.181144711312274, 3.2995101904995003, 2.6223421826119155]\n",
      "[45.941008078500296, 39.60480784218556, 31.571197920881765, 26.786610456299734, 23.945855725938884, 21.818009250360717, 19.76058903054692, 17.23577772023932, 15.24018933175795, 13.59838957350019, 12.10429064884512, 10.37251573603158, 9.053465744799052, 7.750980697317383, 6.4278397342748566, 5.209496619865688, 4.181144711312274, 3.2995101904995003, 2.6223421826119155, 2.0462720025053445]\n",
      "[45.941008078500296, 39.60480784218556, 31.571197920881765, 26.786610456299734, 23.945855725938884, 21.818009250360717, 19.76058903054692, 17.23577772023932, 15.24018933175795, 13.59838957350019, 12.10429064884512, 10.37251573603158, 9.053465744799052, 7.750980697317383, 6.4278397342748566, 5.209496619865688, 4.181144711312274, 3.2995101904995003, 2.6223421826119155, 2.0462720025053445, 1.5349839515984058]\n",
      "[45.941008078500296, 39.60480784218556, 31.571197920881765, 26.786610456299734, 23.945855725938884, 21.818009250360717, 19.76058903054692, 17.23577772023932, 15.24018933175795, 13.59838957350019, 12.10429064884512, 10.37251573603158, 9.053465744799052, 7.750980697317383, 6.4278397342748566, 5.209496619865688, 4.181144711312274, 3.2995101904995003, 2.6223421826119155, 2.0462720025053445, 1.5349839515984058, 1.1221540441522955]\n",
      "[45.941008078500296, 39.60480784218556, 31.571197920881765, 26.786610456299734, 23.945855725938884, 21.818009250360717, 19.76058903054692, 17.23577772023932, 15.24018933175795, 13.59838957350019, 12.10429064884512, 10.37251573603158, 9.053465744799052, 7.750980697317383, 6.4278397342748566, 5.209496619865688, 4.181144711312274, 3.2995101904995003, 2.6223421826119155, 2.0462720025053445, 1.5349839515984058, 1.1221540441522955, 0.8016437287938967]\n",
      "[45.941008078500296, 39.60480784218556, 31.571197920881765, 26.786610456299734, 23.945855725938884, 21.818009250360717, 19.76058903054692, 17.23577772023932, 15.24018933175795, 13.59838957350019, 12.10429064884512, 10.37251573603158, 9.053465744799052, 7.750980697317383, 6.4278397342748566, 5.209496619865688, 4.181144711312274, 3.2995101904995003, 2.6223421826119155, 2.0462720025053445, 1.5349839515984058, 1.1221540441522955, 0.8016437287938967, 0.5521614287513302]\n",
      "[45.941008078500296, 39.60480784218556, 31.571197920881765, 26.786610456299734, 23.945855725938884, 21.818009250360717, 19.76058903054692, 17.23577772023932, 15.24018933175795, 13.59838957350019, 12.10429064884512, 10.37251573603158, 9.053465744799052, 7.750980697317383, 6.4278397342748566, 5.209496619865688, 4.181144711312274, 3.2995101904995003, 2.6223421826119155, 2.0462720025053445, 1.5349839515984058, 1.1221540441522955, 0.8016437287938967, 0.5521614287513302, 0.3616550479166704]\n",
      "[45.941008078500296, 39.60480784218556, 31.571197920881765, 26.786610456299734, 23.945855725938884, 21.818009250360717, 19.76058903054692, 17.23577772023932, 15.24018933175795, 13.59838957350019, 12.10429064884512, 10.37251573603158, 9.053465744799052, 7.750980697317383, 6.4278397342748566, 5.209496619865688, 4.181144711312274, 3.2995101904995003, 2.6223421826119155, 2.0462720025053445, 1.5349839515984058, 1.1221540441522955, 0.8016437287938967, 0.5521614287513302, 0.3616550479166704, 0.22588701627893706]\n",
      "[45.941008078500296, 39.60480784218556, 31.571197920881765, 26.786610456299734, 23.945855725938884, 21.818009250360717, 19.76058903054692, 17.23577772023932, 15.24018933175795, 13.59838957350019, 12.10429064884512, 10.37251573603158, 9.053465744799052, 7.750980697317383, 6.4278397342748566, 5.209496619865688, 4.181144711312274, 3.2995101904995003, 2.6223421826119155, 2.0462720025053445, 1.5349839515984058, 1.1221540441522955, 0.8016437287938967, 0.5521614287513302, 0.3616550479166704, 0.22588701627893706, 0.11899051693140696]\n",
      "final output\n",
      "[45.941008078500296, 39.60480784218556, 31.571197920881765, 26.786610456299734, 23.945855725938884, 21.818009250360717, 19.76058903054692, 17.23577772023932, 15.24018933175795, 13.59838957350019, 12.10429064884512, 10.37251573603158, 9.053465744799052, 7.750980697317383, 6.4278397342748566, 5.209496619865688, 4.181144711312274, 3.2995101904995003, 2.6223421826119155, 2.0462720025053445, 1.5349839515984058, 1.1221540441522955, 0.8016437287938967, 0.5521614287513302, 0.3616550479166704, 0.22588701627893706, 0.11899051693140696]\n"
     ]
    }
   ],
   "source": [
    "n_samples = len(val_loader.dataset)\n",
    "sequence_length = 27\n",
    "fixed_features_len = len(fixed_features)\n",
    "max_features = fixed_features_len + len(labels) - 2\n",
    "loader = val_loader\n",
    "\n",
    "total_mapes = []\n",
    "\n",
    "for j in range(sequence_length):\n",
    "    mape = autoregressive_error(n_samples, sequence_length, j, fixed_features_len, max_features, loader, use_scaling)\n",
    "    total_mapes.append(mape)\n",
    "    print(total_mapes)\n",
    "\n",
    "print(\"final output\")\n",
    "print(total_mapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215d5479-c02f-4c59-a9c6-ed8bbae43fbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
